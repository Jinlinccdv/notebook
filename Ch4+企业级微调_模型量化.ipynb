{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe98a9e5-4646-47d0-9445-19bf22ef3a54",
   "metadata": {},
   "source": [
    "# <center>Ch4 企业级微调_模型量化</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f975f8",
   "metadata": {},
   "source": [
    "# 背景\n",
    "\n",
    "### 1. 大模型规模爆炸性增长\n",
    "自从Transformer架构出现后，语言模型的规模呈指数级增长。从BERT的3.4亿参数，到GPT-3的1750亿参数，再到GPT-4和Claude等超大规模模型，参数量可能已达数万亿级别。这种增长带来性能提升的同时，也造成了巨大的资源消耗。    \n",
    "大模型量化和蒸馏就是把一个\"大而强但很慢很贵\"的AI模型变成\"小而快但依然够用\"的技术。   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa7529",
   "metadata": {},
   "source": [
    "### 2. 计算资源与成本压力\n",
    "训练和部署大模型需要昂贵的硬件设施：\n",
    "- GPU/TPU集群成本高昂\n",
    "- 云计算资源费用持续累积\n",
    "- 电力消耗巨大\n",
    "- 维护大型计算集群的人力成本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89599474",
   "metadata": {},
   "source": [
    "### 3. 终端设备的局限性\n",
    "消费级设备（手机、笔记本、IoT设备等）无法直接运行大型AI模型：\n",
    "- 存储空间有限\n",
    "- 计算能力不足\n",
    "- 电池续航问题\n",
    "- 发热问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c4f69",
   "metadata": {},
   "source": [
    "### 4. 实时性需求\n",
    "许多应用场景对响应速度有严格要求：\n",
    "- 在线翻译\n",
    "- 语音助手\n",
    "- 自动驾驶\n",
    "- 实时内容生成与推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312347d6",
   "metadata": {},
   "source": [
    "### 5. 隐私与安全考量\n",
    "将数据发送到云端处理存在隐私风险：\n",
    "- 个人敏感信息可能泄露\n",
    "- 企业数据安全隐患\n",
    "- 合规性要求（如GDPR、CCPA等）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56421d8",
   "metadata": {},
   "source": [
    " ## 大模型量化与蒸馏的必要性\n",
    "\n",
    "基于上述背景，大模型量化与蒸馏成为解决这些问题的关键技术：\n",
    "\n",
    "1. **弥合理论与实践的鸿沟**：大模型虽然强大，但实际应用受限，量化与蒸馏技术使其能力得以在现实环境中落地\n",
    "\n",
    "2. **降低AI民主化门槛**：通过量化与蒸馏，使更多组织和个人能够负担得起AI技术应用，推动技术普及\n",
    "\n",
    "3. **实现效能与性能的平衡**：在计算资源有限的情况下寻求最优的性能-资源平衡点\n",
    "\n",
    "4. **满足特定场景需求**：针对性地为不同应用场景定制轻量级模型，如医疗、法律、金融等垂直领域\n",
    "\n",
    "大模型量化与蒸馏技术的出现是AI领域从\"追求极致性能\"向\"追求实际可用性\"转变的重要标志，也是大模型时代技术成熟和商业化的必经之路。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2ab9a",
   "metadata": {},
   "source": [
    "\n",
    "## 什么是大模型量化技术？\n",
    "\n",
    "大模型量化技术是将大模型（特别是大型语言模型、视觉模型等）中的高精度浮点数（通常为FP32或FP16）转换为低精度数值表示（如INT8、INT4等）的过程，旨在减少模型的计算和存储需求，同时尽可能保持模型性能。就像把高清照片转成压缩照片，虽然会损失一些细节，但可以节省很多空间。   \n",
    "\n",
    "举例：原来模型中的权重值如0.7352可能被转换成整数7，配合一个缩放因子0.1，在使用时再还原为0.7。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddf184",
   "metadata": {},
   "source": [
    "## 为什么需要量化？\n",
    " \n",
    " 想象一下，手机内存只有8GB，但一个原始模型需要12GB，这时你就无法使用。量化后，模型可能只需要3GB，就能在手机上运行了。     \n",
    "\n",
    "量化的主要优势包括：\n",
    "\n",
    "1. **模型体积减小**：INT8比FP32节省75%存储空间，INT4可节省87.5%\n",
    "2. **推理速度提升**：低精度运算通常更快，许多硬件有专门的低精度加速\n",
    "3. **内存占用降低**：减少运行时内存需求，使大模型能在受限设备上运行\n",
    "4. **能耗降低**：低精度计算通常能耗更低，对移动设备和边缘设备尤为重要\n",
    "5. **吞吐量提升**：可以在同样硬件上处理更多并发请求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00276f86",
   "metadata": {},
   "source": [
    "## 适用场景\n",
    "\n",
    "静态量化特别适用于以下场景：\n",
    "\n",
    "1. **资源受限环境**：边缘设备、移动设备、消费级GPU，手机、平板等内存有限的设备等\n",
    "2. **高吞吐量需求**：需要处理大量并发请求的服务\n",
    "3. **延迟敏感应用**：需要快速响应的实时系统\n",
    "4. **批量推理场景**：预测任务批量处理\n",
    "5. **部署环境固定**：应用场景和数据分布相对稳定\n",
    "\n",
    "## 与其他模型压缩技术的对比\n",
    "\n",
    "| 技术 | 原理 | 优势 | 劣势 | 与量化的互补性 |\n",
    "|------|------|------|------|--------------|\n",
    "| **量化** | 降低数值精度 | 直接实现、性能提升明显、适用大多数模型 | 精度损失、可能需要微调 | - |\n",
    "| **知识蒸馏** | 小模型学习大模型行为 | 模型体积和结构简化、适应性强 | 需要重新训练、蒸馏过程复杂 | 可先蒸馏后量化，双重压缩 |\n",
    "| **剪枝** | 移除不重要连接/神经元 | 减少计算路径、保留关键结构 | 需专门硬件/软件支持才能发挥优势、训练复杂 | 常与量化结合使用 |\n",
    "| **参数共享** | 多个权重使用同一参数 | 直接减少独立参数数量 | 可能显著降低模型表达能力 | 能与量化协同工作 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859dc9e9",
   "metadata": {},
   "source": [
    "## 量化技术的优缺点总结\n",
    "\n",
    "### 优点\n",
    "- 实现相对简单，大多数框架直接支持\n",
    "- 不改变模型结构，易于集成到现有系统\n",
    "- 显著减小模型体积（4-8倍）\n",
    "- 硬件加速支持广泛（特别是INT8）\n",
    "- 部署友好，无需专门训练\n",
    "\n",
    "### 缺点\n",
    "- 精度损失，尤其在高压缩比（如INT4）下\n",
    "- 对某些任务（如长文本生成）影响较大\n",
    "- 可能需要特定优化才能保持性能\n",
    "- 不同于蒸馏，不减少模型结构复杂度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f8d23",
   "metadata": {},
   "source": [
    "更多原理信息请查看 **《Ch8 大模型量化与蒸馏技术详解》**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0eb5c",
   "metadata": {},
   "source": [
    "# 1. 量化技术与框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3573c",
   "metadata": {},
   "source": [
    "量化可以在模型生命周期的不同阶段进行，每个位置有其特点和适用场景。下面详细分析各种量化位置的区别及适用场景。\n",
    "\n",
    "## 1.1 主要量化位置对比\n",
    "\n",
    "### 1. 训练后量化(PTQ)\n",
    "**位置**: 完成训练后，部署前\n",
    "\n",
    "```json\n",
    "预训练 → 微调 → [量化] → 部署\n",
    "```\n",
    "\n",
    "### 2. 量化感知训练(QAT)\n",
    "**位置**: 训练/微调过程中\n",
    "\n",
    "```json\n",
    "预训练 → [量化感知微调] → 转换 → 部署\n",
    "```\n",
    "\n",
    "### 3. 量化后微调(QFT)\n",
    "**位置**: 先量化，再微调\n",
    "\n",
    "```json\n",
    "预训练 → 微调 → 量化 → [微调] → 部署\n",
    "```\n",
    "\n",
    "### 4. 训练阶段量化(TTQ/QT)\n",
    "**位置**: 完整训练过程中\n",
    "\n",
    "```json\n",
    "[低精度训练] → 导出量化模型 → 部署\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46da49",
   "metadata": {},
   "source": [
    "\n",
    "## 2 详细比较与适用场景\n",
    "\n",
    "| 方法                | 实施方式                                                                                     | 优势                                                                                     | 劣势                                                                                     | 适用场景                                                                                     | 实际应用例子                                                                                     |\n",
    "|---------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **1. 训练后量化(PTQ)** | - 在完全训练好的模型上直接应用量化<br>- 使用少量校准数据确定量化参数<br>- 不需要重新训练或微调               | - 实施简单快速<br>- 计算资源需求低<br>- 无需训练数据或仅需少量校准数据<br>- 可以快速尝试不同量化配置 | - 精度损失可能较大<br>- 对某些模型结构效果不佳<br>- 控制精度损失的能力有限                       | - 资源有限环境<br>- 时间紧迫场景<br>- 探索阶段<br>- 模型结构适合<br>- 通用模型部署                 | - 将通用大模型(如BERT/GPT)快速量化部署到线上服务<br>- 将视觉模型量化到移动设备<br>- 快速评估模型在低精度下的表现 |\n",
    "| **2. 量化感知训练(QAT)** | - 在训练或微调过程中模拟量化操作<br>- 使用伪量化节点和直通估计器<br>- 模型学习补偿量化误差                   | - 精度损失最小<br>- 模型能适应量化带来的精度影响<br>- 可以追求极限压缩比(如INT4/INT2)             | - 需要完整训练/微调流程<br>- 计算资源需求高<br>- 训练时间长<br>- 实现复杂度高                    | - 性能极其敏感<br>- 极限压缩需求<br>- 资源充足<br>- 复杂模型<br>- 特定领域优化                     | - 将语音识别模型量化到4位精度而保持准确率<br>- 医疗诊断模型的高精度量化<br>- 自动驾驶感知模型的关键组件量化 |\n",
    "| **3. 量化后微调(QFT)** | - 先对模型进行量化<br>- 再使用少量数据微调已量化的模型<br>- 针对量化后的精度损失进行恢复                   | - 比QAT资源需求低<br>- 比PTQ精度更高<br>- 可以针对特定任务优化<br>- 训练时间相对较短              | - 仍需要训练资源<br>- 需要适当的微调数据<br>- 实现稍复杂                                      | - 平衡场景<br>- 特定任务适应<br>- 有限数据场景<br>- 增量改善<br>- 部署后优化                       | - 将量化后的大语言模型针对客服场景微调<br>- 量化视觉模型后，针对新环境条件微调<br>- 解决量化后模型在特定输入上的性能下降 |\n",
    "| **4. 训练阶段量化(TTQ/QT)** | - 从头开始就使用低精度训练<br>- 权重和激活值都使用低精度表示<br>- 设计特殊训练策略适应低精度                 | - 训练和推理一致<br>- 训练过程也能节省资源<br>- 理论上量化误差最小<br>- 适合从头训练的场景           | - 实现技术要求高<br>- 训练不稳定风险<br>- 需要专门优化的训练流程<br>- 框架支持可能不完善          | - 从头训练<br>- 极端资源限制<br>- 专用硬件<br>- 研究场景<br>- 一致性要求                         | - 在量化友好的专用硬件上训练模型<br>- 边缘AI系统的完整低精度训练流程<br>- 为资源受限环境设计的模型架构 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5ccc0",
   "metadata": {},
   "source": [
    "## 3.量化相关名词概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e47ee",
   "metadata": {},
   "source": [
    "### **1. 量化算法 vs 量化精度**\n",
    "\n",
    "**量化算法**（决定\"怎么量化\"）：\n",
    "- **GPTQ**：基于二阶信息的量化算法\n",
    "- **AWQ**：激活感知权重量化算法\n",
    "- **SmoothQuant**：平滑量化算法\n",
    "- **AQLM**：加法量化算法\n",
    "\n",
    "**量化精度**（决定\"量化到几位\"）：\n",
    "- **INT4**：4位整数精度\n",
    "- **INT8**：8位整数精度  \n",
    "- **FP16**：16位浮点精度\n",
    "\n",
    "**组合关系**：\n",
    "- GPTQ + INT4 = 用GPTQ算法量化到4位\n",
    "- AWQ + INT4 = 用AWQ算法量化到4位\n",
    "- 同一个算法可以量化到不同精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6777aff",
   "metadata": {},
   "source": [
    "### **2. 量化方法 vs 量化框架**\n",
    "\n",
    "**量化方法**：具体的算法和技术\n",
    "- **GPTQ**：一种量化算法\n",
    "- **AWQ**：一种量化算法  \n",
    "- **GGML/GGUF**：一种量化格式标准\n",
    "\n",
    "**量化框架**：实现这些方法的工具库\n",
    "- **GPTQModel**：实现GPTQ算法的框架\n",
    "- **AutoGPTQ**：实现GPTQ算法的框架\n",
    "- **llama.cpp**：实现GGML/GGUF格式的框架\n",
    "- **bitsandbytes**：实现多种量化方法的框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab01f2",
   "metadata": {},
   "source": [
    "## 4.量化技术体系的完整架构\n",
    "\n",
    "### **1. 框架与算法的关系**\n",
    "\n",
    "| 框架 | 支持的算法 | 支持的精度 | 特点 |\n",
    "|------|------------|------------|------|\n",
    "| **bitsandbytes** | NF4、FP4、INT8 | 4位、8位 | 算法+精度打包，无需选择 |\n",
    "| **GPTQModel** | GPTQ | INT4、INT8 | 单算法，支持多精度 (AutoGPTQ的官方继任者，解决了原版本的维护问题)|\n",
    "| **AutoAWQ** | AWQ | INT4 | 单算法，专门优化 |\n",
    "| **llama.cpp** | GGML/GGUF | 各种Q等级 | 专门的格式标准 |\n",
    "| **Transformers** | 集成多种 | 各种组合 | 平台化，支持多种后端 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd795d",
   "metadata": {},
   "source": [
    "### **2. HuggingFace上的量化格式解释**\n",
    "\n",
    "| 文件格式 | 含义 | 精度 | 特点 |\n",
    "|----------|------|------|------|\n",
    "| `.gguf` | GGUF格式 | INT4/INT8 | CPU推理优化，llama.cpp使用 |\n",
    "| `-int4` | INT4量化 | 4位整数 | 体积最小，精度损失较大 |\n",
    "| `-int8` | INT8量化 | 8位整数 | 平衡体积和精度 |\n",
    "| `-gptq` | GPTQ算法 | 主要INT4 | GPU推理优化 |\n",
    "| `-awq` | AWQ算法 | 主要INT4 | GPU推理优化，精度更好 |\n",
    "| `-fp16` | 半精度浮点 | 16位浮点 | 原始精度，体积较大 |\n",
    "\n",
    "- **W4A16**：权重4位，激活16位\n",
    "- **W8A8**：权重8位，激活8位  \n",
    "- **W4A8**：权重4位，激活8位\n",
    "- **W3A16**：权重3位，激活16位\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a36ed",
   "metadata": {},
   "source": [
    "### **3. 推理框架 vs 量化框架**\n",
    "\n",
    "**推理框架**：\n",
    "- **llama.cpp**：CPU推理框架\n",
    "- **vLLM**：GPU推理框架\n",
    "- **TensorRT-LLM**：NVIDIA GPU推理框架\n",
    "\n",
    "**量化框架**：\n",
    "- **GPTQModel**：实现GPTQ算法\n",
    "- **AutoAWQ**：实现AWQ算法\n",
    "- **bitsandbytes**：实现多种量化算法\n",
    "\n",
    "**概念层次**：\n",
    "1. **量化算法**：GPTQ、AWQ、SmoothQuant等（决定怎么量化）\n",
    "2. **量化精度**：INT4、INT8、FP16等（决定量化到几位）\n",
    "3. **量化框架**：GPTQModel、AutoAWQ等（实现算法的工具）\n",
    "4. **推理框架**：vLLM、llama.cpp等（运行量化模型的平台）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844ed08",
   "metadata": {},
   "source": [
    "\n",
    "### **4. 如何选择的系统性建议**\n",
    "\n",
    "**选择决策树：**\n",
    "\n",
    "```json\n",
    "1. 部署环境？\n",
    "   ├── CPU → GGUF (llama.cpp)\n",
    "   ├── GPU → 继续下一步\n",
    "   \n",
    "2. 使用场景？\n",
    "   ├── 生产环境 → GPTQ (GPTQModel)\n",
    "   ├── 长上下文 → EXL2 (ExLlamaV2)  \n",
    "   ├── 实验研究 → bitsandbytes\n",
    "   \n",
    "3. 精度要求？\n",
    "   ├── 最高精度 → W8A8 或 W4A16\n",
    "   ├── 平衡性能 → W4A16\n",
    "   ├── 最小体积 → W4A8 或 W3A16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feec4cb",
   "metadata": {},
   "source": [
    "## 5.不同硬件设备的适配\n",
    "\n",
    "**CPU优化方案：**\n",
    "- **GGUF + llama.cpp**：专门为CPU推理优化\n",
    "- **支持的精度**：Q2_K到Q8_0各种等级\n",
    "- **优势**：CPU推理速度快，内存使用低\n",
    "\n",
    "**GPU优化方案：**\n",
    "- **GPTQ + GPTQModel**：GPU加速量化和推理\n",
    "- **AWQ + AutoAWQ**：GPU推理优化\n",
    "- **EXL2 + ExLlamaV2**：支持4位KV缓存，长上下文优化\n",
    "\n",
    "**基于实际基准测试数据：**\n",
    "\n",
    "| 设备类型 | 推荐方案 | 精度 | 理由 |\n",
    "|----------|----------|------|------|\n",
    "| **CPU (特别是Mac)** | GGUF + llama.cpp | Q4_K_M | CPU推理最优化 |\n",
    "| **GPU (NVIDIA)** | GPTQ + GPTQModel | W4A16 | GPU加速，生产就绪 |\n",
    "| **GPU (长上下文)** | EXL2 + ExLlamaV2 | W4A16 | 支持4位KV缓存 |\n",
    "| **资源受限** | bitsandbytes | NF4 | 动态量化，内存友好 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a49b7d",
   "metadata": {},
   "source": [
    "### **1.补充说明**\n",
    "\n",
    "**llama.cpp/GGUF并不是只能用于CPU！**\n",
    "\n",
    "**实际支持的设备：**\n",
    "- **CPU**：主要优化目标，支持所有主流CPU架构\n",
    "- **GPU**：支持NVIDIA GPU（CUDA）、AMD GPU（ROCm）、Intel GPU\n",
    "- **Apple Silicon**：支持Mac M1/M2/M3的GPU加速（Metal）\n",
    "- **其他加速器**：支持OpenCL等\n",
    "\n",
    "**GPTQ和AWQ并不是只能用于GPU！**\n",
    "\n",
    "**根据vLLM的官方硬件支持表：**\n",
    "- **AWQ**：支持GPU（NVIDIA）、Intel GPU、**x86 CPU**\n",
    "- **GPTQ**：支持GPU（NVIDIA）、Intel GPU、**x86 CPU**\n",
    "\n",
    "### **2 各技术的实际设备支持**\n",
    "\n",
    "| 技术 | CPU支持 | GPU支持 | 主要优化目标 | 性能特点 |\n",
    "|------|---------|---------|-------------|----------|\n",
    "| **GGUF + llama.cpp** | ✅ 主要优化 | ✅ 支持加速 | CPU推理 | CPU上最快，GPU可加速 |\n",
    "| **GPTQ + GPTQModel** | ✅ 支持 | ✅ 主要优化 | GPU推理 | GPU上最快，CPU较慢 |\n",
    "| **AWQ + AutoAWQ** | ✅ 支持 | ✅ 主要优化 | GPU推理 | GPU上最快，CPU较慢 |\n",
    "| **bitsandbytes** | ❌ 不支持 | ✅ 仅GPU | GPU推理 | 仅GPU，动态量化 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b837b",
   "metadata": {},
   "source": [
    "\n",
    "### **3 性能差异说明**\n",
    "\n",
    "**GGUF在不同设备上的表现：**\n",
    "- **CPU**：专门优化，性能最佳\n",
    "- **GPU**：可以加速，但不如专门的GPU量化方法\n",
    "\n",
    "**GPTQ/AWQ在不同设备上的表现：**\n",
    "- **GPU**：专门优化，性能最佳\n",
    "- **CPU**：可以运行，但性能较差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5499aa8",
   "metadata": {},
   "source": [
    "\n",
    "### **4 基于设备的推荐**\n",
    "\n",
    "**如果主要使用CPU：**\n",
    "- **首选**：GGUF + llama.cpp\n",
    "- **备选**：GPTQ（性能较差）\n",
    "- **不推荐**：AWQ、bitsandbytes\n",
    "\n",
    "**如果主要使用GPU：**\n",
    "- **首选**：GPTQ + GPTQModel 或 AWQ + AutoAWQ\n",
    "- **备选**：GGUF + llama.cpp（可以GPU加速）\n",
    "- **特殊用途**：bitsandbytes（动态量化）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74fa73",
   "metadata": {},
   "source": [
    "\n",
    "### **4.边界和限制**\n",
    "\n",
    "**技术边界：**\n",
    "- **bitsandbytes**：真正只支持GPU\n",
    "- **其他技术**：都支持CPU和GPU，只是优化程度不同\n",
    "\n",
    "**性能边界：**\n",
    "- **CPU优化**：GGUF > GPTQ > AWQ\n",
    "- **GPU优化**：GPTQ/AWQ > GGUF > bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af28dea",
   "metadata": {},
   "source": [
    "## 6.如何选择量化技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd46c4",
   "metadata": {},
   "source": [
    "### **1 主要推理框架**\n",
    "\n",
    "| 推理框架 | 核心特点 | 主要优势 | 适用场景 |\n",
    "|---------|----------|----------|----------|\n",
    "| **vLLM** | 高吞吐量推理引擎 | PagedAttention、批处理优化 | 生产环境、高并发 |\n",
    "| **SGLang** | 新一代推理引擎 | 性能优于vLLM，纯Python实现 | 高性能需求、易定制 |\n",
    "| **llama.cpp** | 跨平台推理引擎 | CPU优化、跨平台支持 | 边缘设备、CPU推理 |\n",
    "| **TensorRT-LLM** | NVIDIA官方引擎 | 极致性能优化 | NVIDIA GPU、高性能 |\n",
    "| **Text-Generation-Inference (TGI)** | HuggingFace官方 | 易用性、生态完整 | 快速部署、原型开发 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dc72a",
   "metadata": {},
   "source": [
    "\n",
    "### **2 完整兼容性矩阵**\n",
    "\n",
    "| 量化技术 | vLLM | SGLang | llama.cpp | TensorRT-LLM | TGI |\n",
    "|----------|------|--------|-----------|--------------|-----|\n",
    "| **GPTQ** | ✅ | ✅ | ❌ | ✅ | ✅ |\n",
    "| **AWQ** | ✅ | ✅ | ❌ | ✅ | ❌ |\n",
    "| **GGUF** | ❌ | ❌ | ✅ | ❌ | ❌ |\n",
    "| **EXL2** | ❌ | ❌ | ❌ | ❌ | ❌ |\n",
    "| **bitsandbytes** | ❌ | ❌ | ❌ | ❌ | ✅ |\n",
    "| **FP8** | ✅ | ✅ | ❌ | ✅ | ❌ |\n",
    "| **torchao** | ❌ | ✅ | ❌ | ❌ | ❌ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9968b",
   "metadata": {},
   "source": [
    "\n",
    "### **3 各框架的量化支持详情**\n",
    "\n",
    "**vLLM支持的量化：**\n",
    "- GPTQ (4bit/8bit)\n",
    "- AWQ (4bit)\n",
    "- FP8 (通过CUTLASS)\n",
    "- SqueezeLLM\n",
    "- 不支持：GGUF、EXL2、bitsandbytes\n",
    "\n",
    "**SGLang支持的量化：**\n",
    "- GPTQ (4bit/8bit)\n",
    "- AWQ (4bit)\n",
    "- FP8 (通过CUTLASS)\n",
    "- torchao (INT4/INT8/FP8) - 新增功能\n",
    "- 不支持：GGUF、EXL2、bitsandbytes\n",
    "\n",
    "**llama.cpp支持的量化：**\n",
    "- GGUF格式的所有量化类型\n",
    "- 不支持：GPTQ、AWQ、EXL2、bitsandbytes\n",
    "\n",
    "**TensorRT-LLM支持的量化：**\n",
    "- GPTQ (4bit/8bit)\n",
    "- AWQ (4bit)\n",
    "- FP8\n",
    "- INT8/INT4权重量化\n",
    "- 不支持：GGUF、EXL2、bitsandbytes\n",
    "\n",
    "**TGI支持的量化：**\n",
    "- GPTQ (4bit/8bit)\n",
    "- bitsandbytes (4bit/8bit)\n",
    "- 不支持：AWQ、GGUF、EXL2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc2b92",
   "metadata": {},
   "source": [
    "\n",
    "### **5 推荐的技术栈组合**\n",
    "\n",
    "**生产环境推荐：**\n",
    "1. **GPTQ + vLLM** - 最成熟稳定\n",
    "2. **AWQ + SGLang** - 最高性能\n",
    "3. **GGUF + llama.cpp** - CPU场景\n",
    "\n",
    "**开发测试推荐：**\n",
    "1. **GPTQ + TGI** - 快速原型\n",
    "2. **bitsandbytes + TGI** - 简单易用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73998d66",
   "metadata": {},
   "source": [
    "# 2. 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6927e9a",
   "metadata": {},
   "source": [
    "GPTQ为什么需要激活数据\n",
    "\n",
    "**你要\"量化\"一道菜的调料**\n",
    "\n",
    "### **不使用激活数据的方式（错误方法）**\n",
    "```json\n",
    "厨师说：\"我要减少调料用量来节省成本\"\n",
    "看了看调料瓶：盐、糖、胡椒粉、味精都是白色粉末\n",
    "决定：每种调料都减少一半用量\n",
    "```\n",
    "\n",
    "**结果**：菜变得很难吃，因为：\n",
    "- 盐很重要，减少一半菜就没味道了\n",
    "- 胡椒粉不太重要，减少一半影响不大\n",
    "- 但厨师不知道哪个重要，所以全部减半\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec533489",
   "metadata": {},
   "source": [
    "\n",
    "### **使用激活数据的方式（GPTQ方法）**\n",
    "```json\n",
    "厨师说：\"我要先试试每种调料对菜味道的影响\"\n",
    "实验过程：\n",
    "1. 做一道标准菜（这就是\"激活数据\"）\n",
    "2. 分别减少每种调料，尝尝味道变化\n",
    "3. 发现：盐减少10%，菜就很淡；胡椒粉减少50%，几乎没影响\n",
    "4. 决定：盐只减少5%，胡椒粉减少60%，总体还是节省了成本\n",
    "```\n",
    "\n",
    "**结果**：菜还是很好吃，因为重要的调料保持了足够的量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53b338",
   "metadata": {},
   "source": [
    "\n",
    "## 回到神经网络\n",
    "\n",
    "### **权重就像调料**\n",
    "- 神经网络有几十亿个权重参数\n",
    "- 就像菜谱中有很多种调料\n",
    "- 每个权重对最终结果的重要性不同\n",
    "\n",
    "### **激活数据就像\"试菜\"**\n",
    "- 把真实数据输入网络，看看每个权重的作用\n",
    "- 就像厨师试菜来判断调料的重要性\n",
    "- 通过这个过程知道哪些权重重要，哪些不重要\n",
    "\n",
    "### **量化就像\"减少调料\"**\n",
    "- 量化是把32位的权重变成4位（减少精度来节省内存）\n",
    "- 就像减少调料用量来节省成本\n",
    "- 如果随便减少，效果会很差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2dd3d",
   "metadata": {},
   "source": [
    "\n",
    "## 为什么必须要激活数据？\n",
    "\n",
    "**因为你不知道哪些权重重要！**\n",
    "\n",
    "### **详细解释**\n",
    "```json\n",
    "情况1：没有激活数据\n",
    "- 看到权重A = 0.5，权重B = 0.5\n",
    "- 不知道哪个重要，所以都量化成同样精度\n",
    "- 结果：如果A很重要但被量化得很粗糙，网络性能就坏了\n",
    "\n",
    "情况2：有激活数据\n",
    "- 用真实数据测试发现：权重A影响很大，权重B影响很小\n",
    "- 决定：权重A用高精度量化，权重B用低精度量化\n",
    "- 结果：在相同的存储空间下，网络性能保持得很好\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff507a",
   "metadata": {},
   "source": [
    "\n",
    "## 数字举例\n",
    "\n",
    "假设你有两个权重：\n",
    "\n",
    "**原始权重**：\n",
    "- 权重A = 0.12345（很重要）\n",
    "- 权重B = 0.67890（不重要）\n",
    "\n",
    "**不用激活数据的量化**：\n",
    "- 权重A量化为 0.12（损失了0.00345）\n",
    "- 权重B量化为 0.68（损失了0.00890）\n",
    "- 但是A的损失影响很大！\n",
    "\n",
    "**用激活数据的量化**：\n",
    "- 发现A重要，B不重要\n",
    "- 权重A量化为 0.123（只损失0.00045）\n",
    "- 权重B量化为 0.7（损失了0.01890，但影响小）\n",
    "- 总体效果更好！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdb325",
   "metadata": {},
   "source": [
    "## 为什么是\"必须\"？\n",
    "\n",
    "**因为神经网络太复杂了！**\n",
    "\n",
    "- 人类无法直接看出哪个权重重要\n",
    "- 有几十亿个权重，不可能一个个分析\n",
    "- 只能通过运行真实数据来\"测试\"每个权重的作用\n",
    "- 这就像厨师必须尝菜才知道调料的重要性一样\n",
    "\n",
    "## **总结**\n",
    "\n",
    "**GPTQ需要激活数据的原因**：\n",
    "1. **识别重要性**：知道哪些权重重要，哪些不重要\n",
    "2. **差异化处理**：重要的权重用高精度，不重要的用低精度\n",
    "3. **优化效果**：在相同存储空间下获得更好的性能\n",
    "\n",
    "**没有激活数据就像**：\n",
    "- 闭着眼睛做菜\n",
    "- 不知道调料的重要性\n",
    "- 结果肯定不好\n",
    "\n",
    "**有激活数据就像**：\n",
    "- 边尝边调整\n",
    "- 知道每种调料的作用\n",
    "- 能做出好菜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8044214",
   "metadata": {},
   "source": [
    "\n",
    "## 如何选择对应激活数据\n",
    "\n",
    "### **1.1 任意数据的可行性**\n",
    "\n",
    "**技术上完全可行：**\n",
    "- 用英文小说、新闻、维基百科等任意文本\n",
    "- 用其他领域的数据（如医学、法律等）\n",
    "- 甚至用随机生成的文本\n",
    "\n",
    "**为什么可行：**\n",
    "- 量化算法只需要激活值的统计信息\n",
    "- 任何文本都能激活模型的神经元\n",
    "- 从纯技术角度，有激活就能量化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f2c6d",
   "metadata": {},
   "source": [
    "\n",
    "### **1.2 不同数据选择的实际影响**\n",
    "\n",
    "**影响程度排序（非实验验证）：**\n",
    "\n",
    "| 数据类型 | 量化效果 | 性能保持 | 推荐程度 |\n",
    "|---------|----------|----------|----------|\n",
    "| **目标领域数据** | 最佳 | 90-95% | ✅✅✅ |\n",
    "| **相关领域数据** | 良好 | 85-90% | ✅✅ |\n",
    "| **通用高质量数据** | 一般 | 80-85% | ✅ |\n",
    "| **无关领域数据** | 较差 | 70-80% | ⚠️ |\n",
    "| **低质量/随机数据** | 差 | 60-70% | ❌ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85bfeb",
   "metadata": {},
   "source": [
    "\n",
    "## 具体的数据选择分析\n",
    "\n",
    "### **2.1 使用训练数据**\n",
    "\n",
    "**优势：**\n",
    "- **激活模式匹配**：校准时的激活分布与实际使用时高度一致\n",
    "- **特征覆盖全面**：涵盖了模型学到的所有重要模式\n",
    "- **量化精度最高**：统计信息最准确\n",
    "\n",
    "```python\n",
    "# 最优选择：GRPO数据\n",
    "grpo_prompts = [\n",
    "    \"请分析当前股市的投资机会\",\n",
    "    \"如何评估一只股票的价值？\",\n",
    "    \"什么是价值投资策略？\"\n",
    "]\n",
    "\n",
    "# 次优选择：SFT数据\n",
    "sft_qa_pairs = [\n",
    "    \"用户：请解释股票投资的基本原理\\n助手：股票投资的基本原理包括...\",\n",
    "    \"用户：如何进行风险控制？\\n助手：风险控制的方法有...\"\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acf1abb",
   "metadata": {},
   "source": [
    "\n",
    "### **2.2 使用专业领域数据**\n",
    "\n",
    "**金融领域专业数据示例：**\n",
    "```python\n",
    "# 专业文章片段\n",
    "finance_articles = [\n",
    "    \"央行货币政策的调整对股市的影响主要体现在...\",\n",
    "    \"价值投资理论认为股票的内在价值由企业的基本面决定...\",\n",
    "    \"技术分析通过研究价格走势和成交量来预测...\"\n",
    "]\n",
    "\n",
    "# 专业问答\n",
    "finance_qa = [\n",
    "    \"Q: 什么是市盈率？A: 市盈率是股票价格与每股收益的比值...\",\n",
    "    \"Q: 如何分析财务报表？A: 财务报表分析包括...\"\n",
    "]\n",
    "```\n",
    "\n",
    "**效果分析：**\n",
    "- **激活相关性**：能激活金融相关的神经元\n",
    "- **词汇覆盖**：覆盖专业术语和概念\n",
    "- **性能保持**：通常能保持85-90%的性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cbdac",
   "metadata": {},
   "source": [
    "\n",
    "### **2.3 使用通用数据**\n",
    "\n",
    "**通用数据示例：**\n",
    "```python\n",
    "# C4数据集（常用选择）\n",
    "c4_samples = [\n",
    "    \"The quick brown fox jumps over the lazy dog...\",\n",
    "    \"Machine learning is a subset of artificial intelligence...\",\n",
    "    \"Climate change refers to long-term shifts in global temperatures...\"\n",
    "]\n",
    "\n",
    "# WikiText数据集\n",
    "wiki_samples = [\n",
    "    \"Albert Einstein was a German-born theoretical physicist...\",\n",
    "    \"The history of artificial intelligence began in antiquity...\"\n",
    "]\n",
    "```\n",
    "\n",
    "**效果分析：**\n",
    "- **通用性好**：适用于大多数模型\n",
    "- **获取容易**：标准数据集，无需自己准备\n",
    "- **性能中等**：能保持80-85%的性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fa07d",
   "metadata": {},
   "source": [
    "\n",
    "### **2.4 量化效果对比举例**\n",
    "\n",
    "**假设举例金融模型在不同校准数据下的表现：**\n",
    "\n",
    "```python\n",
    "# 模拟实验结果\n",
    "calibration_results = {\n",
    "    \"GRPO原始数据\": {\n",
    "        \"模型大小\": \"1.2GB → 0.35GB\",\n",
    "        \"推理速度\": \"提升3.2x\",\n",
    "        \"任务准确率\": \"92% → 88%\",  # 下降4%\n",
    "        \"金融专业性\": \"完全保持\"\n",
    "    },\n",
    "    \"金融专业文章\": {\n",
    "        \"模型大小\": \"1.2GB → 0.35GB\", \n",
    "        \"推理速度\": \"提升3.2x\",\n",
    "        \"任务准确率\": \"92% → 85%\",  # 下降7%\n",
    "        \"金融专业性\": \"基本保持\"\n",
    "    },\n",
    "    \"通用C4数据\": {\n",
    "        \"模型大小\": \"1.2GB → 0.35GB\",\n",
    "        \"推理速度\": \"提升3.2x\", \n",
    "        \"任务准确率\": \"92% → 78%\",  # 下降14%\n",
    "        \"金融专业性\": \"明显下降\"\n",
    "    },\n",
    "    \"随机文本\": {\n",
    "        \"模型大小\": \"1.2GB → 0.35GB\",\n",
    "        \"推理速度\": \"提升3.2x\",\n",
    "        \"任务准确率\": \"92% → 65%\",  # 下降27%\n",
    "        \"金融专业性\": \"严重下降\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### **2.5 为什么会有差异**\n",
    "\n",
    "**技术原理：**\n",
    "1. **激活分布差异**：不同数据激活不同的神经元组合\n",
    "2. **统计偏差**：校准数据的分布与实际使用数据的分布不匹配\n",
    "3. **量化误差累积**：统计信息不准确导致量化参数偏差\n",
    "\n",
    "**具体影响：**\n",
    "```python\n",
    "# 金融数据 vs 通用数据的激活差异示例\n",
    "financial_activations = {\n",
    "    \"financial_neurons\": [0.8, 0.9, 0.7],  # 金融相关神经元高激活\n",
    "    \"general_neurons\": [0.3, 0.4, 0.2]     # 通用神经元低激活\n",
    "}\n",
    "\n",
    "general_activations = {\n",
    "    \"financial_neurons\": [0.2, 0.3, 0.1],  # 金融相关神经元低激活\n",
    "    \"general_neurons\": [0.8, 0.9, 0.7]     # 通用神经元高激活\n",
    "}\n",
    "```\n",
    "\n",
    "## 实践建议\n",
    "\n",
    "**第一优先级：训练数据**\n",
    "```python\n",
    "# 最优组合\n",
    "optimal_calibration = {\n",
    "    \"GRPO_prompts\": 0.6,    # 60% - 最贴近实际使用\n",
    "    \"SFT_responses\": 0.3,   # 30% - 增加激活多样性  \n",
    "    \"Finance_articles\": 0.1  # 10% - 补充专业词汇\n",
    "}\n",
    "```\n",
    "\n",
    "**第二优先级：领域专业数据**\n",
    "```python\n",
    "# 次优组合（如果没有训练数据）\n",
    "fallback_calibration = {\n",
    "    \"Finance_QA\": 0.5,      # 50% - 专业问答\n",
    "    \"Finance_articles\": 0.3, # 30% - 专业文章\n",
    "    \"General_C4\": 0.2       # 20% - 通用数据保底\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a068d65",
   "metadata": {},
   "source": [
    "\n",
    "## 总结说明\n",
    "\n",
    "### **核心理解**\n",
    "\n",
    "**可以用任意数据，但效果差别很大：**\n",
    "- **理论上**：任何文本都能激活模型\n",
    "- **实际上**：数据越匹配，量化效果越好\n",
    "\n",
    "**最佳实践：**\n",
    "1. **首选**：训练数据（GRPO + SFT）\n",
    "2. **次选**：金融领域专业数据\n",
    "3. **保底**：通用高质量数据（C4、WikiText）\n",
    "4. **避免**：无关领域或低质量数据\n",
    "\n",
    "### **5.2 实际建议**\n",
    "\n",
    "**对于金融模型：**\n",
    "- 主要使用GRPO问题数据\n",
    "- 适当补充SFT数据增加多样性\n",
    "- 如果数据不足，用金融专业文章补充\n",
    "- 避免使用完全无关的数据\n",
    "\n",
    "**记住：校准数据的选择直接影响量化后的模型性能！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee705d6",
   "metadata": {},
   "source": [
    "## 数据整理代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ac01d",
   "metadata": {},
   "source": [
    "## 1. calibration_fusion.py - 校准数据融合模块\n",
    "\n",
    "### **技术原理**\n",
    "\n",
    "**核心原理**：通过多源数据融合来构建更优质的校准数据集，提升量化效果\n",
    "```python\n",
    "# 数据融合的数学原理\n",
    "final_dataset = Σ(dataset_i × ratio_i) where Σ(ratio_i) = 1\n",
    "```\n",
    "\n",
    "**智能检测算法**：\n",
    "```python\n",
    "def detect_file_type(self, file_path: str) -> str:\n",
    "    # 通过数据结构特征自动识别格式\n",
    "    # SFT格式：包含instruction、input、output字段\n",
    "    # GRPO格式：包含prompt字段\n",
    "    # 避免了手动指定格式的错误\n",
    "```\n",
    "\n",
    "### **设计关注点**\n",
    "\n",
    "**关注点1：自适应数据类型检测**\n",
    "```python\n",
    "# 多重检测机制\n",
    "1. 结构检测：分析JSON字段结构\n",
    "2. 文件名检测：基于文件名模式匹配\n",
    "3. 内容检测：分析实际数据内容\n",
    "```\n",
    "\n",
    "**优势**：\n",
    "- 减少用户配置错误\n",
    "- 支持混合格式数据源\n",
    "- 提升数据处理的鲁棒性\n",
    "\n",
    "**关注点2：差异化文本提取策略**\n",
    "```python\n",
    "# SFT数据提取：instruction+input → prompt, output → response\n",
    "def extract_sft_texts(self, sft_file: str) -> List[Dict]:\n",
    "    # 分别提取两种类型的文本\n",
    "    prompt = f\"{instruction}\\n{input_text}\"  # 合并为完整prompt\n",
    "    output = data.get('output', '').strip()  # 保留<think>标签\n",
    "    \n",
    "# GRPO数据提取：直接提取prompt\n",
    "def extract_grpo_texts(self, grpo_file: str) -> List[Dict]:\n",
    "    # 专门提取强化学习的prompt数据\n",
    "```\n",
    "\n",
    "**技术优势**：\n",
    "- **语义完整性**：保持instruction和input的语义连贯\n",
    "- **标签保留**：保留思维链标签，提升量化质量\n",
    "- **类型标记**：为每个样本标记数据源，便于后续分析\n",
    "\n",
    "**关注点3：智能比例分配算法**\n",
    "```python\n",
    "# 自动比例归一化\n",
    "if abs(sum(ratios) - 1.0) > 0.01:\n",
    "    total_ratio = sum(ratios)\n",
    "    ratios = [r / total_ratio for r in ratios]\n",
    "    \n",
    "# 按比例采样策略\n",
    "target_samples = int(total_samples * ratios[i])\n",
    "if len(texts) > target_samples:\n",
    "    selected_texts = random.sample(texts, target_samples)\n",
    "```\n",
    "\n",
    "**算法优势**：\n",
    "- **数值稳定**：避免浮点数精度问题\n",
    "- **样本均衡**：确保各数据源按设定比例贡献\n",
    "- **随机性控制**：通过种子保证结果可复现\n",
    "\n",
    "### **实际应用价值**\n",
    "\n",
    "**解决的核心问题**：\n",
    "1. **数据质量不均**：通过智能融合提升整体数据质量\n",
    "2. **格式多样性**：统一处理不同格式的训练数据\n",
    "3. **比例控制**：精确控制不同数据源的贡献比例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed08402",
   "metadata": {},
   "source": [
    "```python  \n",
    "# 数据代码实现\n",
    "calibration_fusion.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78fd75",
   "metadata": {},
   "source": [
    "```python \n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "量化校准数据融合工具 - JSON格式输出\n",
    "\n",
    "这个工具的主要作用是：\n",
    "1. 从不同格式的训练数据文件中提取文本\n",
    "2. 将这些文本按指定比例融合成一个统一的校准数据集\n",
    "3. 用于模型量化时的校准过程\n",
    "\n",
    "支持的数据格式：\n",
    "- SFT格式：包含instruction、input、output字段\n",
    "- GRPO格式：包含prompt字段\n",
    "\"\"\"\n",
    "\n",
    "# 导入必要的库\n",
    "import json       # 用于处理JSON格式的数据\n",
    "import random     # 用于随机采样和打乱数据\n",
    "import argparse   # 用于处理命令行参数\n",
    "from pathlib import Path      # 用于处理文件路径\n",
    "from typing import List, Dict # 用于类型注解，提高代码可读性\n",
    "\n",
    "class CalibrationDataFusion:\n",
    "    \"\"\"\n",
    "    校准数据融合类\n",
    "    \n",
    "    这个类的主要功能是将多个不同格式的训练数据文件\n",
    "    融合成一个统一的校准数据集，用于模型量化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        初始化方法\n",
    "        \n",
    "        参数说明：\n",
    "        - random_seed: 随机种子，用于保证结果的可重复性\n",
    "                      相同的种子会产生相同的随机结果\n",
    "        \"\"\"\n",
    "        self.random_seed = random_seed  # 保存随机种子\n",
    "        random.seed(random_seed)        # 设置随机种子，确保结果可重复\n",
    "    \n",
    "    def extract_sft_texts(self, sft_file: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从SFT格式数据文件中提取文本\n",
    "        \n",
    "        SFT格式说明：\n",
    "        - instruction: 指令文本\n",
    "        - input: 输入文本  \n",
    "        - output: 输出文本（可能包含<think>标签）\n",
    "        \n",
    "        参数说明：\n",
    "        - sft_file: SFT格式数据文件的路径\n",
    "        \n",
    "        返回值：\n",
    "        - 返回提取的文本列表，每个元素包含text、source、file字段\n",
    "        \"\"\"\n",
    "        texts = []  # 用于存储提取的文本\n",
    "        \n",
    "        # 打开文件并逐行读取\n",
    "        with open(sft_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # 将每行JSON数据解析为Python字典\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # 第一步：提取instruction和input，合并成prompt\n",
    "                    instruction = data.get('instruction', '').strip()  # 获取指令，如果没有则为空字符串\n",
    "                    input_text = data.get('input', '').strip()         # 获取输入，如果没有则为空字符串\n",
    "                    \n",
    "                    # 如果instruction和input都存在，则合并它们\n",
    "                    if instruction and input_text:\n",
    "                        prompt = f\"{instruction}\\n{input_text}\"  # 用换行符连接\n",
    "                        \n",
    "                        # 只保留长度大于20的文本，过滤掉太短的无效文本\n",
    "                        if len(prompt) > 20:\n",
    "                            texts.append({\n",
    "                                \"text\": prompt,           # 实际的文本内容\n",
    "                                \"source\": \"sft_prompt\",   # 标记来源类型\n",
    "                                \"file\": Path(sft_file).name  # 记录来源文件名\n",
    "                            })\n",
    "                    \n",
    "                    # 第二步：提取output作为回复文本\n",
    "                    output = data.get('output', '').strip()  # 获取输出文本\n",
    "                    \n",
    "                    # 如果output存在且长度足够，则保存\n",
    "                    if output and len(output) > 20:\n",
    "                        texts.append({\n",
    "                            \"text\": output,              # 实际的文本内容\n",
    "                            \"source\": \"sft_response\",    # 标记来源类型\n",
    "                            \"file\": Path(sft_file).name  # 记录来源文件名\n",
    "                        })\n",
    "                        \n",
    "                except Exception:\n",
    "                    # 如果某行数据解析失败，跳过继续处理下一行\n",
    "                    continue\n",
    "        \n",
    "        return texts  # 返回提取的所有文本\n",
    "    \n",
    "    def extract_grpo_texts(self, grpo_file: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        从GRPO格式数据文件中提取文本\n",
    "        \n",
    "        GRPO格式说明：\n",
    "        - prompt: 提示文本，用于强化学习训练\n",
    "        \n",
    "        参数说明：\n",
    "        - grpo_file: GRPO格式数据文件的路径\n",
    "        \n",
    "        返回值：\n",
    "        - 返回提取的文本列表，每个元素包含text、source、file字段\n",
    "        \"\"\"\n",
    "        texts = []  # 用于存储提取的文本\n",
    "        \n",
    "        # 打开文件并逐行读取\n",
    "        with open(grpo_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # 将每行JSON数据解析为Python字典\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # 提取prompt字段\n",
    "                    prompt = data.get('prompt', '').strip()  # 获取提示文本\n",
    "                    \n",
    "                    # 只保留长度大于20的文本，过滤掉太短的无效文本\n",
    "                    if prompt and len(prompt) > 20:\n",
    "                        texts.append({\n",
    "                            \"text\": prompt,              # 实际的文本内容\n",
    "                            \"source\": \"grpo_prompt\",     # 标记来源类型\n",
    "                            \"file\": Path(grpo_file).name # 记录来源文件名\n",
    "                        })\n",
    "                        \n",
    "                except Exception:\n",
    "                    # 如果某行数据解析失败，跳过继续处理下一行\n",
    "                    continue\n",
    "        \n",
    "        return texts  # 返回提取的所有文本\n",
    "    \n",
    "    def detect_file_type(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        自动检测文件的数据格式类型\n",
    "        \n",
    "        检测逻辑：\n",
    "        1. 优先通过文件内容的字段来判断\n",
    "        2. 如果内容检测失败，则通过文件名来判断\n",
    "        \n",
    "        参数说明：\n",
    "        - file_path: 要检测的文件路径\n",
    "        \n",
    "        返回值：\n",
    "        - 'sft': SFT格式文件\n",
    "        - 'grpo': GRPO格式文件  \n",
    "        - 'unknown': 未知格式文件\n",
    "        \"\"\"\n",
    "        # 方法1：通过文件内容检测\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # 解析JSON数据\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # 如果包含instruction和output字段，判断为SFT格式\n",
    "                    if 'instruction' in data and 'output' in data:\n",
    "                        return 'sft'\n",
    "                    # 如果包含prompt字段，判断为GRPO格式\n",
    "                    elif 'prompt' in data:\n",
    "                        return 'grpo'\n",
    "                except:\n",
    "                    # 如果解析失败，继续检查下一行\n",
    "                    continue\n",
    "        \n",
    "        # 方法2：通过文件名检测\n",
    "        file_name = Path(file_path).name.lower()  # 获取文件名并转为小写\n",
    "        \n",
    "        if 'sft' in file_name:\n",
    "            return 'sft'\n",
    "        elif 'grpo' in file_name:\n",
    "            return 'grpo'\n",
    "        else:\n",
    "            return 'unknown'  # 无法识别的格式\n",
    "    \n",
    "    def fuse_data(self, input_files: List[str], output_file: str, \n",
    "                  total_samples: int, ratios: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        融合多个数据文件的核心方法\n",
    "        \n",
    "        工作流程：\n",
    "        1. 检测每个文件的格式类型\n",
    "        2. 根据格式提取文本数据\n",
    "        3. 按指定比例从每个文件中采样\n",
    "        4. 将所有文本合并并随机打乱\n",
    "        5. 保存为统一的JSON格式文件\n",
    "        \n",
    "        参数说明：\n",
    "        - input_files: 输入文件路径列表\n",
    "        - output_file: 输出文件路径\n",
    "        - total_samples: 最终要生成的总样本数\n",
    "        - ratios: 每个文件的采样比例列表\n",
    "        \"\"\"\n",
    "        print(f\"融合 {len(input_files)} 个文件，目标样本数: {total_samples}\")\n",
    "        \n",
    "        # 第一步：归一化比例\n",
    "        # 确保所有比例加起来等于1.0\n",
    "        if abs(sum(ratios) - 1.0) > 0.01:  # 如果比例和不等于1\n",
    "            total_ratio = sum(ratios)      # 计算当前比例总和\n",
    "            ratios = [r / total_ratio for r in ratios]  # 重新计算比例\n",
    "        \n",
    "        all_texts = []  # 用于存储所有提取的文本\n",
    "        \n",
    "        # 第二步：处理每个输入文件\n",
    "        for i, file_path in enumerate(input_files):\n",
    "            print(f\"处理文件: {Path(file_path).name}\")\n",
    "            \n",
    "            # 检测文件类型\n",
    "            file_type = self.detect_file_type(file_path)\n",
    "            \n",
    "            # 根据文件类型选择相应的提取方法\n",
    "            if file_type == 'sft':\n",
    "                texts = self.extract_sft_texts(file_path)\n",
    "            elif file_type == 'grpo':\n",
    "                texts = self.extract_grpo_texts(file_path)\n",
    "            else:\n",
    "                print(f\"跳过未知格式文件: {file_path}\")\n",
    "                continue  # 跳过无法识别的文件\n",
    "            \n",
    "            # 第三步：按比例采样\n",
    "            target_samples = int(total_samples * ratios[i])  # 计算当前文件应该提供的样本数\n",
    "            \n",
    "            if len(texts) > target_samples:\n",
    "                # 如果文本数量超过目标，随机采样\n",
    "                selected_texts = random.sample(texts, target_samples)\n",
    "            else:\n",
    "                # 如果文本数量不足，全部使用\n",
    "                selected_texts = texts\n",
    "                if len(texts) < target_samples:\n",
    "                    print(f\"警告：{file_path} 样本不足，实际获得 {len(texts)} 个\")\n",
    "            \n",
    "            # 将选中的文本添加到总列表中\n",
    "            all_texts.extend(selected_texts)\n",
    "            print(f\"已添加 {len(selected_texts)} 个样本\")\n",
    "        \n",
    "        # 第四步：随机打乱并截取\n",
    "        random.shuffle(all_texts)  # 随机打乱所有文本\n",
    "        \n",
    "        # 如果总数超过目标数量，截取到目标数量\n",
    "        if len(all_texts) > total_samples:\n",
    "            all_texts = all_texts[:total_samples]\n",
    "        \n",
    "        # 第五步：保存为JSON格式文件\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in all_texts:\n",
    "                # 每行写入一个JSON对象\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 输出完成信息\n",
    "        print(f\"融合完成！最终样本数: {len(all_texts)}\")\n",
    "        print(f\"输出文件: {output_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数 - 处理命令行参数并执行数据融合\n",
    "    \n",
    "    支持的命令行参数：\n",
    "    - --input_files: 输入文件路径列表\n",
    "    - --output_file: 输出文件路径\n",
    "    - --total_samples: 总样本数\n",
    "    - --ratios: 各文件的比例列表\n",
    "    - --random_seed: 随机种子\n",
    "    \"\"\"\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser(description='量化校准数据融合工具')\n",
    "    \n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--input_files', '-i', nargs='+', required=True,\n",
    "                        help='输入文件路径列表')\n",
    "    parser.add_argument('--output_file', '-o', required=True,\n",
    "                        help='输出文件路径')\n",
    "    parser.add_argument('--total_samples', '-n', type=int, default=512,\n",
    "                        help='总样本数 (默认: 512)')\n",
    "    parser.add_argument('--ratios', '-r', nargs='+', type=float,\n",
    "                        help='各文件的比例列表 (例如: 0.7 0.3)')\n",
    "    parser.add_argument('--random_seed', '-s', type=int, default=42,\n",
    "                        help='随机种子 (默认: 42)')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 设置默认比例\n",
    "    if args.ratios is None:\n",
    "        if len(args.input_files) == 2:\n",
    "            # 如果有2个文件，默认比例为7:3\n",
    "            args.ratios = [0.7, 0.3]\n",
    "        else:\n",
    "            # 如果有多个文件，平均分配比例\n",
    "            ratio = 1.0 / len(args.input_files)\n",
    "            args.ratios = [ratio] * len(args.input_files)\n",
    "    \n",
    "    # 检查参数有效性\n",
    "    if len(args.ratios) != len(args.input_files):\n",
    "        print(\"错误：比例数量必须与输入文件数量相同\")\n",
    "        return\n",
    "    \n",
    "    # 执行数据融合\n",
    "    fusion = CalibrationDataFusion(random_seed=args.random_seed)\n",
    "    fusion.fuse_data(args.input_files, args.output_file, args.total_samples, args.ratios)\n",
    "\n",
    "# 如果直接运行此脚本，则执行main函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d758a9",
   "metadata": {},
   "source": [
    "```bash\n",
    "python calibration_fusion.py \\\n",
    "    --input_files deepspeek_sft_dataset_1_1k.jsonl grpo_prompts_dataset_5k.jsonl \\\n",
    "    --output_file calibration_data.jsonl \\\n",
    "    --total_samples 512 \\\n",
    "    --ratios 0.7 0.3 \\\n",
    "    --random_seed 42\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8f993",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716110146377.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec3eb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716105459566.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57368f45",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716110007618.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c53616",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716110100886.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88ee24",
   "metadata": {},
   "source": [
    "# 3. 量化实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c754ac1",
   "metadata": {},
   "source": [
    "### **项目整体分析**\n",
    "\n",
    "### **1. 项目定位与核心价值**\n",
    "\n",
    "**项目定位**：专门针对金融领域GRPO模型的GPTQ INT4量化完整解决方案\n",
    "\n",
    "**核心价值**：\n",
    "- 不是通用的量化工具，而是针对金融场景的专业化实现\n",
    "- 从量化、评估到部署的完整工程化流程\n",
    "- 考虑了金融领域的特殊需求和约束条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f19ddc",
   "metadata": {},
   "source": [
    "\n",
    "### **2. 项目特色与关注点**\n",
    "\n",
    "**技术特色**：\n",
    "\n",
    "1. **专业化校准数据策略**：\n",
    "   - 智能数据类型检测（SFT vs GRPO）\n",
    "   - 多源数据融合算法\n",
    "   - 基于业务场景的数据配比优化\n",
    "\n",
    "2. **三维评估体系**：\n",
    "   - 输出一致性评估：语义相似度计算\n",
    "   - 稳定性评估：多次生成的一致性检验\n",
    "   - 性能评估：精确的GPU计时和吞吐量分析\n",
    "\n",
    "3. **生产级部署方案**：\n",
    "   - vLLM高性能推理引擎集成\n",
    "   - 分布式多GPU部署支持\n",
    "   - 自动化部署脚本\n",
    "\n",
    "**工程关注点**：\n",
    "\n",
    "1. **完整的工程化流程**：\n",
    "   ```json\n",
    "   数据准备 → 模型量化 → 效果评估 → 性能测试 → 部署上线\n",
    "   ```\n",
    "\n",
    "2. **详尽的代码注释**：\n",
    "   - 每个函数都有详细的参数说明\n",
    "   - 核心算法有原理解释\n",
    "   - 适合初学者学习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0b01e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **3. 项目架构分析**\n",
    "\n",
    "**代码架构**：\n",
    "\n",
    "```json\n",
    "gptq_model/\n",
    "├── 数据处理层\n",
    "│   └── calibration_fusion.py          # 校准数据融合\n",
    "├── 量化实现层\n",
    "│   └── quantize_model.py               # 核心量化逻辑\n",
    "├── 评估测试层\n",
    "│   ├── financial_model_evaluation.py  # 综合评估系统\n",
    "│   └── test_quantized_model.py        # 简单功能测试\n",
    "└── 部署服务层\n",
    "    ├── vllm_run.sh                    # vLLM服务部署\n",
    "    └── deploy_A800.sh                 # 服务器部署\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525239c0",
   "metadata": {},
   "source": [
    "\n",
    "**技术架构**：\n",
    "\n",
    "```json\n",
    "输入层：原始模型 + 校准数据\n",
    "     ↓\n",
    "处理层：GPTQModel量化框架\n",
    "     ↓\n",
    "评估层：三维评估体系\n",
    "     ↓\n",
    "输出层：量化模型 + 评估报告\n",
    "     ↓\n",
    "部署层：vLLM推理服务\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8b976",
   "metadata": {},
   "source": [
    "\n",
    "**数据流架构**：\n",
    "\n",
    "```json\n",
    "多源数据 → 数据融合 → 校准数据 → 量化过程 → 量化模型\n",
    "                                    ↓\n",
    "评估数据 → 效果评估 → 评估报告 → 部署决策 → 线上服务\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf1079",
   "metadata": {},
   "source": [
    "\n",
    "### **5. 学习路径建议**\n",
    "\n",
    "**第一阶段：理解基础概念**\n",
    "1. 学习量化基础原理\n",
    "2. 理解GPTQ算法核心思想\n",
    "3. 掌握校准数据的作用机制\n",
    "\n",
    "**第二阶段：分析核心实现**\n",
    "1. 深入研究 `quantize_model.py`\n",
    "   - 重点理解 `FinanceModelQuantizer` 类\n",
    "   - 学习量化配置参数的含义\n",
    "   - 掌握量化流程的每个步骤\n",
    "\n",
    "2. 学习校准数据处理\n",
    "   - 研究 `calibration_fusion.py`\n",
    "   - 理解不同数据源的融合策略\n",
    "   - 学习数据预处理的最佳实践\n",
    "\n",
    "**第三阶段：掌握评估体系**\n",
    "1. 学习 `financial_model_evaluation.py`\n",
    "   - 理解三维评估框架设计\n",
    "   - 掌握语义相似度计算方法\n",
    "   - 学习评估报告生成逻辑\n",
    "\n",
    "**第四阶段：实践部署流程**\n",
    "1. 学习部署脚本\n",
    "   - 理解 `vllm_run.sh` 的配置\n",
    "   - 掌握分布式部署的要点\n",
    "   - 学习服务化部署的最佳实践\n",
    "\n",
    "2. 动手实践\n",
    "   - 准备自己的校准数据\n",
    "   - 执行完整的量化流程\n",
    "   - 部署和测试量化模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cea09",
   "metadata": {},
   "source": [
    "\n",
    "## quantize_model.py - 核心量化模块\n",
    "\n",
    "### **技术原理**\n",
    "\n",
    "**GPTQ算法原理**：\n",
    "```python\n",
    "# 基于Hessian矩阵的二阶优化量化\n",
    "# 核心公式：minimize ||W - Ŵ||²_H\n",
    "# 其中H是Hessian矩阵，W是原始权重，Ŵ是量化权重\n",
    "```\n",
    "\n",
    "**量化配置核心**：\n",
    "```python\n",
    "def setup_quantization_config(self):\n",
    "    return QuantizeConfig(\n",
    "        bits=4,                    # 目标位数：4位整数\n",
    "        group_size=128,            # 分组大小：影响精度和速度平衡\n",
    "        damp_percent=0.01,         # 阻尼系数：提升数值稳定性\n",
    "        desc_act=False,            # 描述符激活：权衡精度和速度\n",
    "    )\n",
    "```\n",
    "\n",
    "### **设计关注点**\n",
    "\n",
    "**关注点1：专业级量化配置**\n",
    "```python\n",
    "# 参数详解及原理\n",
    "bits=4                  # 最终存储位数\n",
    "    # 原理：4位可表示16个不同值，相比32位节省87.5%空间\n",
    "    # 权衡：空间节省 vs 精度损失\n",
    "\n",
    "group_size=128          # 量化分组大小\n",
    "    # 原理：每128个权重共享量化参数\n",
    "    # 较小值(64)：更高精度，更多量化参数\n",
    "    # 较大值(256)：更少参数，更高压缩比\n",
    "\n",
    "damp_percent=0.01       # 阻尼系数\n",
    "    # 作用：提升数值稳定性，避免量化过程中的数值问题\n",
    "```\n",
    "\n",
    "**关注点2：渐进式量化流程**\n",
    "```python\n",
    "def quantize_model(self, output_path: str):\n",
    "    # 1. 数据准备阶段\n",
    "    calibration_data = self.load_calibration_data()\n",
    "    \n",
    "    # 2. 配置设置阶段\n",
    "    quantize_config = self.setup_quantization_config()\n",
    "    \n",
    "    # 3. 模型加载阶段\n",
    "    self.model = GPTQModel.from_pretrained(...)\n",
    "    \n",
    "    # 4. 量化执行阶段\n",
    "    self.model.quantize(calibration_data)\n",
    "    \n",
    "    # 5. 结果保存阶段\n",
    "    self.model.save_quantized(output_path)\n",
    "```\n",
    "\n",
    "\n",
    "**关注点3：智能内存管理**\n",
    "```python\n",
    "# 精确的数据类型选择\n",
    "torch_dtype=torch.float16    # 而非float32\n",
    "    # 原理：量化过程中使用float16计算\n",
    "    # 优势：节省50%内存，加速计算\n",
    "    # 注意：最终存储仍为int4\n",
    "```\n",
    "\n",
    "**校准数据格式统一**\n",
    "```python\n",
    "def load_calibration_data(self) -> List[str]:\n",
    "    # 支持多种数据格式的统一处理\n",
    "    # JSON Lines格式 → 纯文本列表\n",
    "    # 自动文本长度过滤和清理\n",
    "    texts = [item[\"text\"] for item in data if len(item[\"text\"]) > 10]\n",
    "```\n",
    "\n",
    "**模型大小对比系统**\n",
    "```python\n",
    "def compare_model_sizes(self, quantized_path: str):\n",
    "    # 精确的文件大小计算\n",
    "    # 自动单位转换(GB, MB, KB)\n",
    "    # 压缩比和节省空间的直观展示\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9768c",
   "metadata": {},
   "source": [
    "pip install gptqmodel[all] -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d8832",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250711162013441.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4d5db",
   "metadata": {},
   "source": [
    "**模型量化代码**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c88bc",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GPTQModel INT4量化脚本 - 针对金融GRPO模型\n",
    "\n",
    "这个脚本的主要作用：\n",
    "1. 将原始的大语言模型从FP16/BF16精度压缩到INT4精度\n",
    "2. 使用GPTQ算法进行量化，保持模型性能的同时显著减少模型大小\n",
    "3. 量化后的模型可以在相同硬件上运行更大的模型或获得更快的推理速度\n",
    "\n",
    "核心概念：\n",
    "- 量化：将模型权重从32位/16位浮点数转换为4位整数，大幅减少存储空间\n",
    "- 校准数据：用于统计模型激活分布的样本数据，帮助确定最佳量化参数\n",
    "- GPTQ：一种先进的量化算法，通过二阶信息优化量化误差\n",
    "\"\"\"\n",
    "\n",
    "# 导入必要的库\n",
    "import torch                                         # PyTorch深度学习框架\n",
    "import json                                          # JSON数据处理\n",
    "from transformers import AutoTokenizer               # 用于处理文本tokenization\n",
    "from gptqmodel import GPTQModel, QuantizeConfig     # GPTQModel量化框架\n",
    "from pathlib import Path                             # 文件路径处理\n",
    "import time                                          # 时间计算\n",
    "\n",
    "class FinanceModelQuantizer:\n",
    "    \"\"\"\n",
    "    金融模型量化器类\n",
    "    \n",
    "    主要功能：\n",
    "    1. 加载原始模型和校准数据\n",
    "    2. 配置量化参数\n",
    "    3. 执行量化过程\n",
    "    4. 保存量化后的模型\n",
    "    5. 对比模型大小变化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, calibration_data_path: str):\n",
    "        \"\"\"\n",
    "        初始化量化器\n",
    "        \n",
    "        参数说明：\n",
    "        model_path: 原始模型的路径，包含模型权重和配置文件\n",
    "        calibration_data_path: 校准数据文件路径，用于量化过程中的统计信息收集\n",
    "        \"\"\"\n",
    "        self.model_path = model_path                     # 存储原始模型路径\n",
    "        self.calibration_data_path = calibration_data_path  # 存储校准数据路径\n",
    "        self.tokenizer = None                            # 用于文本处理的分词器，初始化为空\n",
    "        self.model = None                                # 用于存储加载的模型，初始化为空\n",
    "        \n",
    "    def load_calibration_data(self) -> list:\n",
    "        \"\"\"\n",
    "        加载校准数据\n",
    "        \n",
    "        校准数据的作用：\n",
    "        1. 量化过程中，需要通过校准数据来统计模型各层的激活分布\n",
    "        2. 根据激活分布来确定最佳的量化参数（如缩放因子、零点等）\n",
    "        3. 校准数据应该代表模型的实际使用场景，这样量化后的模型才能保持性能\n",
    "        \n",
    "        数据格式要求：\n",
    "        - JSONL格式，每行一个JSON对象\n",
    "        - 每个对象必须包含'text'字段\n",
    "        - 文本长度建议在10-2048个字符之间\n",
    "        \n",
    "        返回值：\n",
    "        calibration_data: 校准文本列表，每个元素是一个用于校准的文本字符串\n",
    "        \n",
    "        异常处理：\n",
    "        - 跳过格式错误的行\n",
    "        - 过滤掉过短的文本\n",
    "        - 如果没有有效数据会返回空列表\n",
    "        \"\"\"\n",
    "        print(\"正在加载校准数据...\")\n",
    "        calibration_data = []  # 存储校准文本的列表\n",
    "        \n",
    "        # 逐行读取校准数据文件\n",
    "        with open(self.calibration_data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # 解析JSON格式的每一行\n",
    "                    data = json.loads(line.strip())\n",
    "                    # 提取文本内容，去除首尾空白字符\n",
    "                    text = data.get('text', '').strip()\n",
    "                    # 过滤掉太短的文本，确保校准数据质量\n",
    "                    # 长度阈值设为10，避免无意义的短文本影响量化效果\n",
    "                    if text and len(text) > 10:\n",
    "                        calibration_data.append(text)\n",
    "                except:\n",
    "                    # 如果某行数据格式错误，跳过该行继续处理\n",
    "                    continue\n",
    "        \n",
    "        print(f\"成功加载了 {len(calibration_data)} 条校准数据\")\n",
    "        return calibration_data\n",
    "    \n",
    "    def setup_quantization_config(self) -> QuantizeConfig:\n",
    "        \"\"\"\n",
    "        设置量化配置\n",
    "        \n",
    "        量化配置参数详解：\n",
    "        - bits: 量化位数，4表示将权重量化为4位整数\n",
    "        - group_size: 分组大小，128表示每128个权重为一组共享量化参数\n",
    "                     较小的值(64)精度更高但速度慢，较大的值(256)速度快但精度略低\n",
    "        - damp_percent: 阻尼系数，0.01表示在优化过程中的正则化强度\n",
    "                       防止量化过程中的数值不稳定，通常设为0.01\n",
    "        - desc_act: 是否启用描述符激活，False表示不启用（提升速度）\n",
    "                   启用会提高精度但降低推理速度，金融场景通常关闭\n",
    "        - static_groups: 是否使用静态分组，False表示动态分组\n",
    "                        动态分组可以获得更好的量化效果\n",
    "        - sym: 是否使用对称量化，True表示使用对称量化（更简单）\n",
    "               对称量化计算更快，非对称量化精度略高\n",
    "        - true_sequential: 是否使用真正的顺序量化，True表示按层顺序量化\n",
    "                          确保量化过程的稳定性和可重复性\n",
    "        \n",
    "        返回值：\n",
    "        quantize_config: 配置好的量化参数对象\n",
    "        \"\"\"\n",
    "        print(\"正在设置量化配置...\")\n",
    "        \n",
    "        quantize_config = QuantizeConfig(\n",
    "            bits=4,                    # 量化位数：4位整数，相比16位浮点数大幅减少存储\n",
    "            group_size=128,            # 分组大小：每128个权重共享一个量化参数，平衡精度和压缩率\n",
    "            damp_percent=0.01,         # 阻尼系数：较小的值(0.01)减少量化过程中的数值震荡\n",
    "            desc_act=False,            # 描述符激活：关闭以提升推理速度，对精度影响较小\n",
    "            static_groups=False,       # 静态分组：关闭以允许动态优化分组策略\n",
    "            sym=True,                  # 对称量化：启用对称量化，简化计算过程\n",
    "            true_sequential=True,      # 顺序量化：按模型层的顺序进行量化，确保稳定性\n",
    "        )\n",
    "        \n",
    "        print(\"量化配置设置完成\")\n",
    "        return quantize_config\n",
    "    \n",
    "    def quantize_model(self, output_path: str):\n",
    "        \"\"\"\n",
    "        执行模型量化的主要流程\n",
    "        \n",
    "        参数说明：\n",
    "        output_path: 量化后模型的保存路径\n",
    "                    会自动创建目录结构，保存模型权重和配置文件\n",
    "        \n",
    "        量化流程：\n",
    "        1. 加载校准数据 -> 2. 设置量化配置 -> 3. 加载原始模型 \n",
    "        -> 4. 执行量化计算 -> 5. 保存量化模型 -> 6. 对比模型大小\n",
    "        \n",
    "        内存管理：\n",
    "        - 使用torch.float16减少内存占用\n",
    "        - 采用device_map=\"auto\"自动分配GPU/CPU\n",
    "        - 量化过程中可能需要大量内存，建议至少32GB\n",
    "        \n",
    "        时间预估：\n",
    "        - 7B模型通常需要10-30分钟\n",
    "        - 13B模型通常需要30-60分钟\n",
    "        - 具体时间取决于硬件配置和校准数据量\n",
    "        \"\"\"\n",
    "        print(f\"开始量化模型: {self.model_path}\")\n",
    "        \n",
    "        # 步骤1: 加载校准数据\n",
    "        # 校准数据用于统计模型在实际数据上的激活分布\n",
    "        calibration_data = self.load_calibration_data()\n",
    "        \n",
    "        # 步骤2: 设置量化配置\n",
    "        # 配置量化算法的各种参数\n",
    "        quantize_config = self.setup_quantization_config()\n",
    "        \n",
    "        # 步骤3: 加载分词器\n",
    "        # 分词器用于将文本转换为模型可理解的token序列\n",
    "        print(\"正在加载分词器...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        \n",
    "        # 步骤4: 加载原始模型\n",
    "        # 使用GPTQModel框架加载模型，准备进行量化\n",
    "        print(\"正在加载原始模型...\")\n",
    "        self.model = GPTQModel.from_pretrained(\n",
    "            self.model_path,                    # 模型路径\n",
    "            quantize_config=quantize_config,    # 量化配置\n",
    "            device_map=\"auto\",                  # 自动分配设备（CPU/GPU）\n",
    "            torch_dtype=torch.float16,          # 使用float16精度以节省内存\n",
    "            trust_remote_code=True              # 允许执行模型中的自定义代码\n",
    "        )\n",
    "        \n",
    "        # 步骤5: 执行量化过程\n",
    "        # 这是最耗时的步骤，会计算量化参数并转换模型权重\n",
    "        print(\"正在进行量化计算...\")\n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        \n",
    "        # 核心量化步骤：使用校准数据来优化量化参数\n",
    "        # 这个过程会：\n",
    "        # 1. 将校准数据输入模型获取激活值\n",
    "        # 2. 统计每层的激活分布\n",
    "        # 3. 计算最优的量化参数（缩放因子、零点）\n",
    "        # 4. 将FP16权重转换为INT4权重\n",
    "        self.model.quantize(calibration_data)\n",
    "        \n",
    "        end_time = time.time()  # 记录结束时间\n",
    "        print(f\"量化计算完成！耗时: {end_time - start_time:.2f} 秒\")\n",
    "        \n",
    "        # 步骤6: 保存量化模型\n",
    "        # 将量化后的模型和分词器保存到指定路径\n",
    "        print(f\"正在保存量化模型到: {output_path}\")\n",
    "        \n",
    "        # 创建输出目录（如果不存在）\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.model.save_quantized(output_path)      # 保存量化后的模型权重\n",
    "        self.tokenizer.save_pretrained(output_path)  # 保存分词器配置\n",
    "        \n",
    "        # 步骤7: 对比模型大小\n",
    "        # 展示量化前后的模型大小变化\n",
    "        self.compare_model_sizes(output_path)\n",
    "        \n",
    "        print(\"模型量化流程全部完成！\")\n",
    "    \n",
    "    def compare_model_sizes(self, quantized_path: str):\n",
    "        \"\"\"\n",
    "        对比原始模型和量化模型的大小\n",
    "        支持 .bin 和 .safetensors 格式\n",
    "        \n",
    "        参数说明：\n",
    "        quantized_path: 量化模型的保存路径\n",
    "        \n",
    "        支持格式：\n",
    "        - .bin: 传统的PyTorch模型格式\n",
    "        - .safetensors: 新的安全张量格式，加载更快更安全\n",
    "        \n",
    "        计算指标：\n",
    "        - 绝对大小（GB）\n",
    "        - 压缩比（倍数）\n",
    "        - 节省空间（GB）\n",
    "        - 压缩率（百分比）\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 支持多种模型文件格式\n",
    "            def calculate_model_size(path):\n",
    "                \"\"\"\n",
    "                计算模型文件总大小\n",
    "                \n",
    "                参数说明：\n",
    "                path: 模型目录路径\n",
    "                \n",
    "                返回值：\n",
    "                total_size: 总大小（字节）\n",
    "                bin_count: .bin文件数量\n",
    "                safetensors_count: .safetensors文件数量\n",
    "                \"\"\"\n",
    "                total_size = 0\n",
    "                path_obj = Path(path)\n",
    "                \n",
    "                # 查找 .bin 文件（传统PyTorch格式）\n",
    "                bin_files = list(path_obj.rglob('*.bin'))\n",
    "                total_size += sum(f.stat().st_size for f in bin_files if f.is_file())\n",
    "                \n",
    "                # 查找 .safetensors 文件（新格式，更安全）\n",
    "                safetensors_files = list(path_obj.rglob('*.safetensors'))\n",
    "                total_size += sum(f.stat().st_size for f in safetensors_files if f.is_file())\n",
    "                \n",
    "                return total_size, len(bin_files), len(safetensors_files)\n",
    "            \n",
    "            # 计算原始模型大小\n",
    "            original_size, orig_bin_count, orig_safetensors_count = calculate_model_size(self.model_path)\n",
    "            \n",
    "            # 计算量化模型大小\n",
    "            quantized_size, quant_bin_count, quant_safetensors_count = calculate_model_size(quantized_path)\n",
    "            \n",
    "            print(f\"\\n文件格式检测:\")\n",
    "            print(f\"原始模型: {orig_bin_count} 个 .bin 文件, {orig_safetensors_count} 个 .safetensors 文件\")\n",
    "            print(f\"量化模型: {quant_bin_count} 个 .bin 文件, {quant_safetensors_count} 个 .safetensors 文件\")\n",
    "            \n",
    "            # 检查是否找到了模型文件\n",
    "            if original_size == 0:\n",
    "                print(\"警告: 未找到原始模型文件\")\n",
    "                print(\"请检查模型路径是否正确\")\n",
    "                return\n",
    "            \n",
    "            if quantized_size == 0:\n",
    "                print(\"警告: 未找到量化模型文件\")\n",
    "                print(\"量化过程可能未成功完成\")\n",
    "                return\n",
    "            \n",
    "            # 转换为GB单位，方便阅读\n",
    "            original_gb = original_size / (1024**3)        # 将字节转换为GB\n",
    "            quantized_gb = quantized_size / (1024**3)      # 将字节转换为GB\n",
    "            compression_ratio = original_size / quantized_size  # 计算压缩比\n",
    "            \n",
    "            # 显示详细的对比结果\n",
    "            print(f\"\\n模型大小对比:\")\n",
    "            print(f\"原始模型: {original_gb:.2f} GB\")\n",
    "            print(f\"量化模型: {quantized_gb:.2f} GB\")\n",
    "            print(f\"压缩比: {compression_ratio:.2f}倍\")\n",
    "            print(f\"节省空间: {(original_gb - quantized_gb):.2f} GB\")\n",
    "            print(f\"压缩率: {((original_size - quantized_size) / original_size * 100):.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 如果计算过程中出现错误，显示详细的警告信息\n",
    "            print(f\"无法计算模型大小对比: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：配置路径并执行量化流程\n",
    "    \n",
    "    使用说明：\n",
    "    1. 修改model_path为原始模型路径\n",
    "    2. 修改calibration_data_path为校准数据路径\n",
    "    3. 修改output_path为你想要保存量化模型的路径\n",
    "    4. 运行脚本即可开始量化过程\n",
    "    \"\"\"\n",
    "    # 配置路径\n",
    "    model_path = \"/shared/grpo_financial_tuning/output/best_model\"  # 原始模型路径\n",
    "    calibration_data_path = \"calibration_data.jsonl\"  # 校准数据路径\n",
    "    output_path = \"./output/best_model_gptq_int4\"     # 量化模型输出路径\n",
    "    \n",
    "    # 检查路径是否存在\n",
    "    if not Path(model_path).exists():\n",
    "        print(f\"错误：模型路径不存在: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    if not Path(calibration_data_path).exists():\n",
    "        print(f\"错误：校准数据路径不存在: {calibration_data_path}\")\n",
    "        return\n",
    "    \n",
    "    # 创建量化器并执行量化\n",
    "    quantizer = FinanceModelQuantizer(model_path, calibration_data_path)\n",
    "    quantizer.quantize_model(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b1233",
   "metadata": {},
   "source": [
    "```python\n",
    "# 执行命令\n",
    "python quantize_model.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd280d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716112206759.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e8f20",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716111907910.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29d843",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716111927860.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1af96",
   "metadata": {},
   "source": [
    "## 主要差异分析\n",
    "\n",
    "### 1. **文件数量变化**\n",
    "- **原始**: 9个模型文件\n",
    "- **量化**: 2个模型文件\n",
    "- **原因**: 量化过程中会重新组织模型结构，将权重重新分布到更少的文件中\n",
    "\n",
    "### 2. **存储空间差异**\n",
    "- **原始**: 15GB (FP16精度)\n",
    "- **量化**: 5.3GB (INT4精度)\n",
    "- **压缩比**: 2.83倍 (15GB ÷ 5.3GB)\n",
    "- **节省空间**: 9.7GB (64.7%的空间节省)\n",
    "\n",
    "### 3. **新增文件**\n",
    "- **quantize_config.json**: 记录量化配置参数\n",
    "- **quant_log.csv**: 记录量化过程的详细日志\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3917f0b",
   "metadata": {},
   "source": [
    "\n",
    "## 差异产生的技术原因\n",
    "\n",
    "### 1. **数据精度变化**\n",
    "```\n",
    "原始模型: FP16 (16位浮点数)\n",
    "量化模型: INT4 (4位整数) + 量化参数\n",
    "\n",
    "理论压缩比: 16 ÷ 4 = 4倍\n",
    "实际压缩比: 2.83倍 (因为需要额外存储量化参数)\n",
    "```\n",
    "\n",
    "### 2. **权重存储方式**\n",
    "```json\n",
    "原始模型权重存储:\n",
    "- 每个权重: 16位浮点数\n",
    "- 直接存储原始数值\n",
    "\n",
    "量化模型权重存储:\n",
    "- 每个权重: 4位整数\n",
    "- 额外存储: 缩放因子 + 零点偏移\n",
    "- 分组量化: 每128个权重共享一套量化参数\n",
    "```\n",
    "\n",
    "### 3. **文件重新组织**\n",
    "- **原始**: 按照原始模型层结构分片\n",
    "- **量化**: 按照量化后的存储效率重新分片\n",
    "- **结果**: 文件数量减少，每个文件大小更优化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a0e6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716111947713.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6786200",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716112006555.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a5a15",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250711165510695.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3c0cf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716112033878.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873a718",
   "metadata": {},
   "source": [
    "## 日志字段含义解析\n",
    "\n",
    "### 字段说明\n",
    "```json\n",
    "layer: 模型层编号 (0-27，共28层)\n",
    "module: 具体模块类型\n",
    "  - self_attn.k_proj: 注意力机制的Key投影层\n",
    "  - self_attn.v_proj: 注意力机制的Value投影层  \n",
    "  - self_attn.q_proj: 注意力机制的Query投影层\n",
    "  - self_attn.o_proj: 注意力机制的Output投影层\n",
    "  - mlp.up_proj: MLP的上投影层\n",
    "  - mlp.gate_proj: MLP的门控投影层\n",
    "  - mlp.down_proj: MLP的下投影层\n",
    "loss: 量化误差损失值\n",
    "samples: 固定为0.01000\n",
    "damp: 阻尼系数\n",
    "time: 量化该模块耗时（秒）\n",
    "```\n",
    "\n",
    "## 关键数据分析\n",
    "\n",
    "### 1. **量化损失分析**\n",
    "\n",
    "**注意力模块损失 (较低，质量好)**\n",
    "```json\n",
    "K/V/Q/O投影层损失范围: 0.03-90.73\n",
    "- 早期层(0-10): 损失很小 (0.03-7.87)\n",
    "- 中期层(11-20): 损失适中 (1.41-22.35)\n",
    "- 后期层(21-27): 损失较大 (2.37-95.39)\n",
    "```\n",
    "\n",
    "**MLP模块损失 (较高，但正常)**\n",
    "```json\n",
    "MLP层损失范围: 0.12-2792.19\n",
    "- up_proj: 20.90-354.35\n",
    "- gate_proj: 29.11-330.98  \n",
    "- down_proj: 0.12-2792.19 (变化最大)\n",
    "```\n",
    "\n",
    "### 2. **损失分布规律**\n",
    "\n",
    "**层级趋势**\n",
    "- **浅层** (0-5): 损失较小，量化容易\n",
    "- **中层** (6-20): 损失中等，量化稳定  \n",
    "- **深层** (21-27): 损失增大，量化困难\n",
    "\n",
    "**模块类型规律**\n",
    "- **V投影**: 损失最小 (0.03-33.84)\n",
    "- **K投影**: 损失较小 (0.17-4.85)\n",
    "- **Q投影**: 损失中等 (0.77-53.74)\n",
    "- **O投影**: 损失递增 (0.15-95.39)\n",
    "- **MLP**: 损失最大，特别是down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c34fe",
   "metadata": {},
   "source": [
    "# 4. 量化效果检验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa7951",
   "metadata": {},
   "source": [
    "\n",
    "##  test_quantized_model.py - 简单测试模块\n",
    "\n",
    "### **设计关注点**\n",
    "\n",
    "**快速验证**\n",
    "```python\n",
    "# 最小化的功能测试\n",
    "model = GPTQModel.from_quantized(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 直接的功能验证\n",
    "response = model.generate(...)\n",
    "print(f\"模型回答: {response}\")\n",
    "```\n",
    "\n",
    "**价值**：\n",
    "- **快速反馈**：量化完成后立即验证功能\n",
    "- **调试工具**：快速定位量化过程中的问题\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e9103",
   "metadata": {},
   "source": [
    "test_quantized_model.py  最简测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba2c83",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "量化模型简单测试脚本\n",
    "\n",
    "这个脚本的主要作用：\n",
    "1. 快速测试量化模型是否正常工作\n",
    "2. 验证模型加载和推理功能\n",
    "3. 提供基本的模型调用示例\n",
    "4. 用于开发和调试阶段的功能验证\n",
    "\n",
    "使用场景：\n",
    "- 量化完成后的功能验证\n",
    "- 调试模型加载问题\n",
    "- 测试不同的推理参数\n",
    "- 验证模型输出质量\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from gptqmodel import GPTQModel        # 量化模型加载库\n",
    "from transformers import AutoTokenizer  # 文本分词库\n",
    "\n",
    "# 模型路径配置\n",
    "model_path = \"./output/best_model_gptq_int4\"  # 量化模型的保存路径\n",
    "\n",
    "# 加载量化模型\n",
    "# GPTQModel.from_quantized用于加载已经量化的模型\n",
    "print(\"正在加载量化模型...\")\n",
    "model = GPTQModel.from_quantized(\n",
    "    model_path,              # 模型路径：量化模型的文件夹路径\n",
    "    device_map=\"auto\"        # 设备映射：自动分配CPU/GPU资源\n",
    ")\n",
    "\n",
    "# 加载对应的分词器\n",
    "# 分词器用于将文本转换为模型可理解的token序列\n",
    "print(\"正在加载分词器...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 测试推理\n",
    "prompt = \"你是一个专业的金融领域分析师请对以下问题进行详细解答：周三盘前交易时段，金融板块呈现温和上涨态势：金融精选行业SPDR基金(XLF)上涨0.4%，Direxion每日三倍做多金融股ETF(FAS)上涨1.2%，而其反向产品Direxion每日三倍做空金融股ETF(FAZ)下跌1.1%。万事达卡(MA)股价上涨0.7%，因其董事会批准了最高110亿美元的普通股回购计划，并将季度股息提高16%。Cboe全球市场(CBOE)股价上涨0.5%，该公司披露其指数期权合约的11月日均交易量达到近400万份，较去年同期的330万份增长22%。基于这些市场动态，请分析以下问题：在当前市场环境下，影响金融板块ETF(XLF、FAS、FAZ)价格波动的关键驱动因素有哪些？如何评估万事达卡大规模股票回购和股息增长对其长期估值的影响？Cboe交易量的大幅增长可能预示着哪些市场结构变化？请综合考虑宏观经济环境、行业竞争格局和公司特定事件等多重因素展开分析。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"输入: {prompt}\")\n",
    "print(f\"输出: {response}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523cde1",
   "metadata": {},
   "source": [
    "量化前模型\n",
    "```python\n",
    "python -c \"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载原始模型\n",
    "model_path = '/shared/grpo_financial_tuning/output/best_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "\n",
    "# 测试推理\n",
    "prompt = '你是一个专业的金融领域分析师请对以下问题进行详细解答：周三盘前交易时段，金融板块呈现温和上涨态势：金融精选行业SPDR基金(XLF)上涨0.4%，Direxion每日三倍做多金融股ETF(FAS)上涨1.2%，而其反向产品Direxion每日三倍做空金融股ETF(FAZ)下跌1.1%。万事达卡(MA)股价上涨0.7%，因其董事会批准了最高110亿美元的普通股回购计划，并将季度股息提高16%。Cboe全球市场(CBOE)股价上涨0.5%，该公司披露其指数期权合约的11月日均交易量达到近400万份，较去年同期的330万份增长22%。基于这些市场动态，请分析以下问题：在当前市场环境下，影响金融板块ETF(XLF、FAS、FAZ)价格波动的关键驱动因素有哪些？如何评估万事达卡大规模股票回购和股息增长对其长期估值的影响？Cboe交易量的大幅增长可能预示着哪些市场结构变化？请综合考虑宏观经济环境、行业竞争格局和公司特定事件等多重因素展开分析。'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f'输入: {prompt}')\n",
    "print(f'输出: {response[len(prompt):].strip()}')\n",
    "\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f7ef8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716155240995.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e49700",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716155031183.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261143d7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716120819370.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606c279",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716120953534.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b70fef",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716121024757.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e2ddf",
   "metadata": {},
   "source": [
    "**效果验证**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ec7e2",
   "metadata": {},
   "source": [
    "pip install sentence-transformers scikit-learn gptqmodel  -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822c30a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250714192338669.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17bb13",
   "metadata": {},
   "source": [
    "## financial_model_evaluation.py - 综合评估模块\n",
    "\n",
    "### **技术原理**\n",
    "\n",
    "**三维评估理论框架**：\n",
    "```python\n",
    "评估维度 = {\n",
    "    \"consistency\": 语义相似度评估,\n",
    "    \"stability\": 输出稳定性评估,\n",
    "    \"performance\": 推理性能评估\n",
    "}\n",
    "```\n",
    "\n",
    "1. 输出一致性 (Output Consistency)\n",
    "同样的问题 → 原始模型答案 vs 量化模型答案 → 比较相似度\n",
    "\n",
    "2. 稳定性 (Stability)  \n",
    "同样的问题问10次 → 10个答案之间的相似度 → 测试稳定性\n",
    "\n",
    "3. 性能 (Performance)\n",
    "同样的问题 → 原始模型用时 vs 量化模型用时 → 计算加速比\n",
    "\n",
    "**语义相似度算法**：\n",
    "```python\n",
    "# 使用sentence-transformers计算语义相似度\n",
    "similarity_score = cosine_similarity(\n",
    "    embedding_original, \n",
    "    embedding_quantized\n",
    ")\n",
    "# 基于句子级别的语义表示，比词汇重叠更准确\n",
    "```\n",
    "\n",
    "### **设计关注点**\n",
    "\n",
    "**关注点1：多模型并行评估架构**\n",
    "```python\n",
    "# 同时加载两个模型进行对比\n",
    "self.original_model = AutoModelForCausalLM.from_pretrained(...)\n",
    "self.quantized_model = GPTQModel.from_quantized(...)\n",
    "\n",
    "# 智能内存管理\n",
    "torch.cuda.empty_cache()  # 加载间清理GPU缓存\n",
    "```\n",
    "\n",
    "**技术优势**：\n",
    "- **对比公平性**：相同环境下的直接对比\n",
    "- **内存优化**：智能的GPU内存管理\n",
    "- **并行效率**：避免重复加载的时间开销\n",
    "\n",
    "**关注点2：精确的文本生成控制**\n",
    "```python\n",
    "def generate_response(self, model, tokenizer, prompt: str) -> str:\n",
    "    # 输入长度限制\n",
    "    max_input_length = 1536  # 为生成预留空间\n",
    "    \n",
    "    # 生成参数精确控制\n",
    "    outputs = model.generate(\n",
    "        max_new_tokens=self.config.max_new_tokens,\n",
    "        temperature=self.config.temperature,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,  # 减少重复\n",
    "        top_p=0.9               # 核采样\n",
    "    )\n",
    "```\n",
    "\n",
    "**参数优化原理**：\n",
    "- **temperature=0.7**：平衡创造性和一致性\n",
    "- **top_p=0.9**：保留90%概率质量，过滤低概率token\n",
    "- **repetition_penalty=1.1**：轻微惩罚重复，保持自然性\n",
    "\n",
    "**关注点3：多轮一致性测试**\n",
    "```python\n",
    "def evaluate_stability(self, prompts: List[str]) -> Dict:\n",
    "    # 每个prompt生成多次响应\n",
    "    responses = []\n",
    "    for _ in range(self.config.num_consistency_trials):\n",
    "        response = self.generate_response(model, tokenizer, prompt)\n",
    "        responses.append(response)\n",
    "    \n",
    "    # 计算内部一致性\n",
    "    consistency_score = self.calculate_self_consistency(responses)\n",
    "```\n",
    "\n",
    "**算法创新**：\n",
    "- **自一致性度量**：同一输入多次生成的一致性\n",
    "- **稳定性量化**：通过方差分析量化稳定性\n",
    "- **异常检测**：识别异常输出并分析原因\n",
    "\n",
    "### **评估方法论创新**\n",
    "\n",
    "**创新1：语义级别的输出对比**\n",
    "```python\n",
    "# 不是简单的文本匹配，而是语义理解\n",
    "def calculate_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "    # 使用预训练的语义模型\n",
    "    embeddings1 = self.similarity_model.encode([text1])\n",
    "    embeddings2 = self.similarity_model.encode([text2])\n",
    "    return cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "```\n",
    "\n",
    "**创新2：多维度性能分析**\n",
    "```python\n",
    "def evaluate_performance(self, prompts: List[str]) -> Dict:\n",
    "    # 不只是速度，还包括吞吐量、内存使用等\n",
    "    metrics = {\n",
    "        \"avg_latency\": 平均延迟,\n",
    "        \"tokens_per_second\": 每秒生成token数,\n",
    "        \"memory_usage\": 内存使用情况,\n",
    "        \"gpu_utilization\": GPU利用率\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63355e2c",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "金融模型量化效果评估系统\n",
    "\n",
    "这个脚本的主要作用：\n",
    "1. 全面评估量化模型与原始模型的性能差异\n",
    "2. 从三个维度进行评估：输出一致性、稳定性、推理性能\n",
    "3. 生成详细的评估报告，帮助判断量化效果是否满足要求\n",
    "4. 为模型部署决策提供数据支持\n",
    "\n",
    "评估方法说明：\n",
    "- 输出一致性：对比两个模型在相同输入下的输出相似度\n",
    "- 稳定性：测试模型在多次生成中的一致性\n",
    "- 推理性能：测试模型的推理速度和吞吐量\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from gptqmodel import GPTQModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 配置日志系统：设置日志级别和输出格式\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"\n",
    "    评估配置类\n",
    "    \n",
    "    这个类定义了评估过程中的所有配置参数\n",
    "    使用dataclass装饰器自动生成构造函数和其他方法\n",
    "    \"\"\"\n",
    "    # 模型路径配置\n",
    "    original_model_path: str = \"/shared/grpo_financial_tuning/output/best_model\"      # 原始模型路径\n",
    "    quantized_model_path: str = \"/shared/gptq_model/output/best_model_gptq_int4\"     # 量化模型路径\n",
    "    \n",
    "    # 数据和输出配置\n",
    "    test_data_path: str = \"/shared/gptq_model/grpo_prompts_dataset_5k.jsonl\"         # 测试数据路径\n",
    "    output_dir: str = \"./evaluation_results\"                                          # 结果输出目录\n",
    "    \n",
    "    # 评估参数配置\n",
    "    num_test_samples: int = 10           # 测试样本数量：用于一致性评估的样本数\n",
    "    num_consistency_trials: int = 10     # 一致性测试轮数：每个样本重复生成的次数\n",
    "    max_new_tokens: int = 512           # 最大生成token数：控制生成文本的长度\n",
    "    temperature: float = 0.7            # 温度参数：控制生成文本的随机性，0.7是平衡值\n",
    "    random_seed: int = 42               # 随机种子：确保实验结果可复现\n",
    "    warmup_rounds: int = 3              # 预热轮数：GPU预热次数，提高计时准确性\n",
    "\n",
    "class FinancialModelEvaluator:\n",
    "    \"\"\"\n",
    "    金融模型评估器主类\n",
    "    \n",
    "    功能概述：\n",
    "    1. 加载原始模型和量化模型\n",
    "    2. 执行三个维度的评估\n",
    "    3. 生成详细的评估报告\n",
    "    4. 保存评估结果\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        \"\"\"\n",
    "        初始化评估器\n",
    "        \n",
    "        参数说明：\n",
    "        config: 评估配置对象，包含所有评估参数\n",
    "        \"\"\"\n",
    "        self.config = config  # 存储配置对象\n",
    "        \n",
    "        # 语义相似度模型：用于计算文本之间的语义相似度\n",
    "        self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 模型存储变量：初始化为None，在load_models方法中赋值\n",
    "        self.original_model = None      # 原始模型\n",
    "        self.quantized_model = None     # 量化模型\n",
    "        self.original_tokenizer = None  # 原始模型的分词器\n",
    "        self.quantized_tokenizer = None # 量化模型的分词器\n",
    "        \n",
    "        # 设置随机种子，确保结果可复现\n",
    "        random.seed(config.random_seed)\n",
    "        torch.manual_seed(config.random_seed)\n",
    "        np.random.seed(config.random_seed)\n",
    "        \n",
    "        # 创建输出目录\n",
    "        Path(config.output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        加载原始模型和量化模型\n",
    "        \n",
    "        这是评估的第一步，需要将两个模型都加载到内存中\n",
    "        加载过程包括：\n",
    "        1. 加载分词器（将文本转换为token）\n",
    "        2. 加载模型权重\n",
    "        3. 设置合适的精度和设备分配\n",
    "        \"\"\"\n",
    "        logger.info(\"开始加载模型...\")\n",
    "        \n",
    "        try:\n",
    "            # 加载原始模型\n",
    "            logger.info(\"正在加载原始模型...\")\n",
    "            # 加载分词器：用于文本预处理\n",
    "            self.original_tokenizer = AutoTokenizer.from_pretrained(self.config.original_model_path)\n",
    "            # 加载模型：使用transformers库加载标准模型\n",
    "            self.original_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.original_model_path,  # 模型路径\n",
    "                device_map=\"auto\",                # 自动分配设备（CPU/GPU）\n",
    "                torch_dtype=torch.bfloat16,       # 使用bfloat16精度节省内存\n",
    "                trust_remote_code=True            # 允许执行自定义代码\n",
    "            )\n",
    "            logger.info(\"原始模型加载完成\")\n",
    "            \n",
    "            # 清理GPU缓存，为加载第二个模型腾出空间\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # 加载量化模型\n",
    "            logger.info(\"正在加载量化模型...\")\n",
    "            # 加载量化模型的分词器\n",
    "            self.quantized_tokenizer = AutoTokenizer.from_pretrained(self.config.quantized_model_path)\n",
    "            # 加载量化模型：使用GPTQModel框架加载量化后的模型\n",
    "            self.quantized_model = GPTQModel.from_quantized(\n",
    "                self.config.quantized_model_path,  # 量化模型路径\n",
    "                device_map=\"auto\",                 # 自动分配设备\n",
    "                torch_dtype=torch.bfloat16         # 使用bfloat16精度\n",
    "            )\n",
    "            logger.info(\"量化模型加载完成\")\n",
    "            \n",
    "            # 记录模型设备分布信息\n",
    "            self._log_model_device_info()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型加载失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _log_model_device_info(self):\n",
    "        \"\"\"\n",
    "        记录模型设备分布信息\n",
    "        \n",
    "        功能：\n",
    "        1. 显示GPU内存使用情况\n",
    "        2. 帮助用户了解模型在设备上的分布\n",
    "        3. 用于调试和优化内存使用\n",
    "        \"\"\"\n",
    "        logger.info(\"=== 模型设备分布信息 ===\")\n",
    "        \n",
    "        # 检查是否有可用的GPU\n",
    "        if torch.cuda.is_available():\n",
    "            # 遍历所有GPU设备\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                # 获取已分配的GPU内存（单位：字节）\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # 转换为GB\n",
    "                # 获取已缓存的GPU内存（单位：字节）\n",
    "                memory_cached = torch.cuda.memory_reserved(i) / 1024**3      # 转换为GB\n",
    "                logger.info(f\"GPU {i}: 已分配 {memory_allocated:.2f}GB, 已缓存 {memory_cached:.2f}GB\")\n",
    "    \n",
    "    def load_test_data(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        加载测试数据\n",
    "        \n",
    "        功能：\n",
    "        1. 从指定文件加载测试数据\n",
    "        2. 过滤掉质量不好的数据\n",
    "        3. 随机选择指定数量的样本\n",
    "        4. 返回用于评估的prompt列表\n",
    "        \n",
    "        返回：\n",
    "        prompts: 测试prompt列表，每个元素是一个字符串\n",
    "        \"\"\"\n",
    "        logger.info(f\"从 {self.config.test_data_path} 加载测试数据...\")\n",
    "        \n",
    "        prompts = []  # 存储所有有效prompt的列表\n",
    "        \n",
    "        # 逐行读取测试数据文件\n",
    "        with open(self.config.test_data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    # 解析JSON格式的数据\n",
    "                    data = json.loads(line.strip())\n",
    "                    # 提取prompt字段\n",
    "                    prompt = data.get('prompt', '').strip()\n",
    "                    # 过滤条件：prompt不为空且长度大于50个字符\n",
    "                    if prompt and len(prompt) > 50:\n",
    "                        prompts.append(prompt)\n",
    "                except:\n",
    "                    # 如果数据格式错误，跳过该行\n",
    "                    continue\n",
    "        \n",
    "        # 随机抽取指定数量的样本\n",
    "        if len(prompts) > self.config.num_test_samples:\n",
    "            prompts = random.sample(prompts, self.config.num_test_samples)\n",
    "        \n",
    "        logger.info(f\"成功加载了 {len(prompts)} 个测试样本\")\n",
    "        return prompts\n",
    "    \n",
    "    def generate_response(self, model, tokenizer, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        生成模型响应\n",
    "        \n",
    "        这是核心的文本生成方法，用于获取模型对给定prompt的响应\n",
    "        \n",
    "        参数说明：\n",
    "        model: 要使用的模型（原始模型或量化模型）\n",
    "        tokenizer: 对应的分词器\n",
    "        prompt: 输入的提示文本\n",
    "        \n",
    "        返回：\n",
    "        response: 模型生成的响应文本（去除了原始prompt）\n",
    "        \"\"\"\n",
    "        # 将文本转换为模型输入格式\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # 限制输入长度，避免超出模型最大长度限制\n",
    "        max_input_length = 1536  # 为生成的token预留空间\n",
    "        if inputs['input_ids'].shape[1] > max_input_length:\n",
    "            # 如果输入过长，进行截断\n",
    "            inputs['input_ids'] = inputs['input_ids'][:, :max_input_length]\n",
    "            inputs['attention_mask'] = inputs['attention_mask'][:, :max_input_length]\n",
    "        \n",
    "        # 生成文本\n",
    "        with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "            outputs = model.generate(\n",
    "                **inputs,                                    # 输入数据\n",
    "                max_new_tokens=self.config.max_new_tokens,   # 最大生成token数\n",
    "                temperature=self.config.temperature,         # 温度参数：控制随机性\n",
    "                do_sample=True,                              # 启用采样：产生多样化的输出\n",
    "                pad_token_id=tokenizer.eos_token_id,        # 填充token ID\n",
    "                repetition_penalty=1.1,                     # 重复惩罚：减少重复内容\n",
    "                top_p=0.9                                   # 核采样：保留概率前90%的token\n",
    "            )\n",
    "        \n",
    "        # 解码生成的token为文本\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 移除原始prompt，只保留生成的部分\n",
    "        response = response[len(prompt):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        计算两个文本之间的语义相似度\n",
    "        \n",
    "        使用预训练的sentence-transformers模型计算语义相似度\n",
    "        这比简单的文本匹配更能反映内容的实际相似性\n",
    "        \n",
    "        参数说明：\n",
    "        text1: 第一个文本\n",
    "        text2: 第二个文本\n",
    "        \n",
    "        返回：\n",
    "        similarity: 相似度分数，范围[0,1]，1表示完全相似\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 将文本转换为语义向量\n",
    "            embeddings1 = self.similarity_model.encode([text1])\n",
    "            embeddings2 = self.similarity_model.encode([text2])\n",
    "            \n",
    "            # 计算余弦相似度\n",
    "            similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "            \n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"相似度计算失败: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_output_consistency(self, prompts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        评估输出一致性\n",
    "        \n",
    "        这是核心评估方法之一，用于测试量化模型和原始模型的输出相似度\n",
    "        \n",
    "        评估原理：\n",
    "        1. 对每个prompt，让两个模型都生成响应\n",
    "        2. 计算两个响应之间的语义相似度\n",
    "        3. 统计所有样本的相似度分布\n",
    "        4. 评估量化是否显著影响了模型输出\n",
    "        \n",
    "        参数说明：\n",
    "        prompts: 测试prompt列表\n",
    "        \n",
    "        返回：\n",
    "        consistency_stats: 包含相似度统计信息的字典\n",
    "        \"\"\"\n",
    "        logger.info(\"=== 开始输出一致性评估 ===\")\n",
    "        \n",
    "        similarities = []        # 存储所有相似度分数\n",
    "        detailed_results = []    # 存储详细的对比结果\n",
    "        \n",
    "        # 对每个prompt进行评估\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            logger.info(f\"处理第 {i+1}/{len(prompts)} 个样本...\")\n",
    "            \n",
    "            try:\n",
    "                # 生成原始模型的响应\n",
    "                original_response = self.generate_response(\n",
    "                    self.original_model, self.original_tokenizer, prompt\n",
    "                )\n",
    "                \n",
    "                # 生成量化模型的响应\n",
    "                quantized_response = self.generate_response(\n",
    "                    self.quantized_model, self.quantized_tokenizer, prompt\n",
    "                )\n",
    "                \n",
    "                # 计算语义相似度\n",
    "                similarity = self.calculate_semantic_similarity(original_response, quantized_response)\n",
    "                similarities.append(similarity)\n",
    "                \n",
    "                # 记录详细结果（用于后续分析）\n",
    "                detailed_results.append({\n",
    "                    \"prompt\": prompt[:100] + \"...\",  # 截断prompt用于显示\n",
    "                    \"original_response\": original_response,\n",
    "                    \"quantized_response\": quantized_response,\n",
    "                    \"similarity\": similarity\n",
    "                })\n",
    "                \n",
    "                logger.info(f\"相似度: {similarity:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"处理样本 {i+1} 时出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 计算统计指标\n",
    "        consistency_stats = {\n",
    "            \"mean_similarity\": np.mean(similarities),      # 平均相似度\n",
    "            \"std_similarity\": np.std(similarities),        # 相似度标准差\n",
    "            \"min_similarity\": np.min(similarities),        # 最小相似度\n",
    "            \"max_similarity\": np.max(similarities),        # 最大相似度\n",
    "            \"percentiles\": {                               # 百分位数分析\n",
    "                \"25th\": np.percentile(similarities, 25),   # 25%分位数\n",
    "                \"50th\": np.percentile(similarities, 50),   # 中位数\n",
    "                \"75th\": np.percentile(similarities, 75),   # 75%分位数\n",
    "                \"90th\": np.percentile(similarities, 90)    # 90%分位数\n",
    "            },\n",
    "            \"samples_processed\": len(similarities),        # 成功处理的样本数\n",
    "            \"detailed_results\": detailed_results           # 详细结果\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"一致性评估完成，平均相似度: {consistency_stats['mean_similarity']:.3f}\")\n",
    "        return consistency_stats\n",
    "    \n",
    "    def evaluate_stability(self, test_prompts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        评估模型稳定性（自一致性）\n",
    "        \n",
    "        稳定性评估的目的：\n",
    "        1. 测试模型在相同输入下的输出一致性\n",
    "        2. 评估量化是否增加了模型的不稳定性\n",
    "        3. 比较原始模型和量化模型的稳定性保持程度\n",
    "        \n",
    "        评估方法：\n",
    "        1. 选择部分prompt\n",
    "        2. 每个prompt重复生成多次\n",
    "        3. 计算多次生成结果之间的相似度\n",
    "        4. 对比两个模型的稳定性\n",
    "        \n",
    "        参数说明：\n",
    "        test_prompts: 测试prompt列表\n",
    "        \n",
    "        返回：\n",
    "        stability_stats: 包含稳定性统计信息的字典\n",
    "        \"\"\"\n",
    "        logger.info(\"=== 开始稳定性评估 ===\")\n",
    "        \n",
    "        # 选择部分prompt进行稳定性测试（稳定性测试计算量大）\n",
    "        stability_prompts = random.sample(test_prompts, min(10, len(test_prompts)))\n",
    "        \n",
    "        original_stability_scores = []   # 原始模型的稳定性分数\n",
    "        quantized_stability_scores = []  # 量化模型的稳定性分数\n",
    "        \n",
    "        # 对每个prompt进行稳定性测试\n",
    "        for i, prompt in enumerate(stability_prompts):\n",
    "            logger.info(f\"稳定性测试 {i+1}/{len(stability_prompts)}\")\n",
    "            \n",
    "            # 测试原始模型的稳定性\n",
    "            original_responses = []\n",
    "            for trial in range(self.config.num_consistency_trials):\n",
    "                response = self.generate_response(self.original_model, self.original_tokenizer, prompt)\n",
    "                original_responses.append(response)\n",
    "            \n",
    "            # 计算原始模型的自一致性分数\n",
    "            original_stability = self._calculate_self_consistency(original_responses)\n",
    "            original_stability_scores.append(original_stability)\n",
    "            \n",
    "            # 测试量化模型的稳定性\n",
    "            quantized_responses = []\n",
    "            for trial in range(self.config.num_consistency_trials):\n",
    "                response = self.generate_response(self.quantized_model, self.quantized_tokenizer, prompt)\n",
    "                quantized_responses.append(response)\n",
    "            \n",
    "            # 计算量化模型的自一致性分数\n",
    "            quantized_stability = self._calculate_self_consistency(quantized_responses)\n",
    "            quantized_stability_scores.append(quantized_stability)\n",
    "            \n",
    "            logger.info(f\"原始模型稳定性: {original_stability:.3f}, 量化模型稳定性: {quantized_stability:.3f}\")\n",
    "        \n",
    "        # 计算稳定性统计指标\n",
    "        stability_stats = {\n",
    "            \"original_model\": {\n",
    "                \"mean_stability\": np.mean(original_stability_scores),  # 原始模型平均稳定性\n",
    "                \"std_stability\": np.std(original_stability_scores),    # 原始模型稳定性标准差\n",
    "                \"scores\": original_stability_scores                    # 所有稳定性分数\n",
    "            },\n",
    "            \"quantized_model\": {\n",
    "                \"mean_stability\": np.mean(quantized_stability_scores), # 量化模型平均稳定性\n",
    "                \"std_stability\": np.std(quantized_stability_scores),   # 量化模型稳定性标准差\n",
    "                \"scores\": quantized_stability_scores                   # 所有稳定性分数\n",
    "            },\n",
    "            # 稳定性保持率：量化模型稳定性 / 原始模型稳定性\n",
    "            \"stability_retention\": np.mean(quantized_stability_scores) / np.mean(original_stability_scores)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"稳定性评估完成，稳定性保持率: {stability_stats['stability_retention']:.3f}\")\n",
    "        return stability_stats\n",
    "    \n",
    "    def _calculate_self_consistency(self, responses: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        计算自一致性分数\n",
    "        \n",
    "        自一致性的定义：同一个prompt多次生成的结果之间的相似度\n",
    "        \n",
    "        计算方法：\n",
    "        1. 对所有响应进行两两相似度计算\n",
    "        2. 计算所有相似度的平均值\n",
    "        3. 返回平均相似度作为自一致性分数\n",
    "        \n",
    "        参数说明：\n",
    "        responses: 同一个prompt的多次响应列表\n",
    "        \n",
    "        返回：\n",
    "        self_consistency: 自一致性分数，范围[0,1]\n",
    "        \"\"\"\n",
    "        # 如果响应数量少于2，无法计算相似度\n",
    "        if len(responses) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        similarities = []  # 存储所有相似度分数\n",
    "        \n",
    "        # 计算所有响应之间的两两相似度\n",
    "        for i in range(len(responses)):\n",
    "            for j in range(i + 1, len(responses)):\n",
    "                sim = self.calculate_semantic_similarity(responses[i], responses[j])\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        # 返回平均相似度\n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    def evaluate_performance(self, test_prompts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        评估推理性能\n",
    "        \n",
    "        性能评估的目的：\n",
    "        1. 测试量化模型相比原始模型的速度提升\n",
    "        2. 评估量化带来的实际性能收益\n",
    "        3. 为部署决策提供性能数据\n",
    "        \n",
    "        评估方法：\n",
    "        1. 分别测试两个模型的推理时间\n",
    "        2. 计算加速比\n",
    "        3. 进行统计分析\n",
    "        \n",
    "        参数说明：\n",
    "        test_prompts: 测试prompt列表\n",
    "        \n",
    "        返回：\n",
    "        performance_stats: 包含性能统计信息的字典\n",
    "        \"\"\"\n",
    "        logger.info(\"=== 开始性能评估 ===\")\n",
    "        \n",
    "        # 选择部分prompt进行性能测试\n",
    "        performance_prompts = random.sample(test_prompts, min(10, len(test_prompts)))\n",
    "        \n",
    "        # 分别测试每个模型，避免交替影响\n",
    "        original_times = []   # 原始模型推理时间列表\n",
    "        quantized_times = []  # 量化模型推理时间列表\n",
    "        \n",
    "        # 测试原始模型性能\n",
    "        logger.info(\"测试原始模型性能...\")\n",
    "        for i, prompt in enumerate(performance_prompts):\n",
    "            logger.info(f\"原始模型测试 {i+1}/{len(performance_prompts)}\")\n",
    "            \n",
    "            # GPU预热：避免冷启动影响计时准确性\n",
    "            inputs = self.original_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1536).to(self.original_model.device)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(self.config.warmup_rounds):\n",
    "                    self.original_model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            \n",
    "            # 准确计时：使用GPU同步确保计时准确\n",
    "            torch.cuda.synchronize()  # 等待GPU操作完成\n",
    "            start_time = time.time()\n",
    "            _ = self.generate_response(self.original_model, self.original_tokenizer, prompt)\n",
    "            torch.cuda.synchronize()  # 等待GPU操作完成\n",
    "            original_time = time.time() - start_time\n",
    "            original_times.append(original_time)\n",
    "        \n",
    "        # 清理GPU缓存\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # 测试量化模型性能\n",
    "        logger.info(\"测试量化模型性能...\")\n",
    "        for i, prompt in enumerate(performance_prompts):\n",
    "            logger.info(f\"量化模型测试 {i+1}/{len(performance_prompts)}\")\n",
    "            \n",
    "            # GPU预热\n",
    "            inputs = self.quantized_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1536).to(self.quantized_model.device)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(self.config.warmup_rounds):\n",
    "                    self.quantized_model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "            \n",
    "            # 准确计时\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            _ = self.generate_response(self.quantized_model, self.quantized_tokenizer, prompt)\n",
    "            torch.cuda.synchronize()\n",
    "            quantized_time = time.time() - start_time\n",
    "            quantized_times.append(quantized_time)\n",
    "        \n",
    "        # 计算性能统计指标\n",
    "        performance_stats = {\n",
    "            \"original_model\": {\n",
    "                \"mean_time\": np.mean(original_times),   # 原始模型平均时间\n",
    "                \"std_time\": np.std(original_times),     # 原始模型时间标准差\n",
    "                \"times\": original_times                 # 所有时间记录\n",
    "            },\n",
    "            \"quantized_model\": {\n",
    "                \"mean_time\": np.mean(quantized_times),  # 量化模型平均时间\n",
    "                \"std_time\": np.std(quantized_times),    # 量化模型时间标准差\n",
    "                \"times\": quantized_times                # 所有时间记录\n",
    "            },\n",
    "            # 加速比：原始模型时间 / 量化模型时间\n",
    "            \"speedup\": np.mean(original_times) / np.mean(quantized_times)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"原始模型平均时间: {performance_stats['original_model']['mean_time']:.2f}秒\")\n",
    "        logger.info(f\"量化模型平均时间: {performance_stats['quantized_model']['mean_time']:.2f}秒\")\n",
    "        logger.info(f\"加速比: {performance_stats['speedup']:.2f}x\")\n",
    "        \n",
    "        return performance_stats\n",
    "    \n",
    "    def run_complete_evaluation(self) -> Dict:\n",
    "        \"\"\"\n",
    "        运行完整的评估流程\n",
    "        \n",
    "        这是主要的评估方法，按顺序执行所有评估步骤：\n",
    "        1. 加载模型\n",
    "        2. 加载测试数据\n",
    "        3. 执行三个维度的评估\n",
    "        4. 生成综合报告\n",
    "        5. 保存结果\n",
    "        \n",
    "        返回：\n",
    "        results: 包含所有评估结果的字典\n",
    "        \"\"\"\n",
    "        logger.info(\"=== 开始金融模型量化评估 ===\")\n",
    "        \n",
    "        # 步骤1: 加载模型\n",
    "        self.load_models()\n",
    "        \n",
    "        # 步骤2: 加载测试数据\n",
    "        test_prompts = self.load_test_data()\n",
    "        \n",
    "        # 步骤3: 执行各项评估\n",
    "        results = {\n",
    "            \"config\": {\n",
    "                \"num_test_samples\": len(test_prompts),\n",
    "                \"max_new_tokens\": self.config.max_new_tokens,\n",
    "                \"temperature\": self.config.temperature,\n",
    "                \"random_seed\": self.config.random_seed\n",
    "            },\n",
    "            # 一致性评估：测试输出相似度\n",
    "            \"consistency_evaluation\": self.evaluate_output_consistency(test_prompts),\n",
    "            # 稳定性评估：测试输出稳定性\n",
    "            \"stability_evaluation\": self.evaluate_stability(test_prompts),\n",
    "            # 性能评估：测试推理速度\n",
    "            \"performance_evaluation\": self.evaluate_performance(test_prompts)\n",
    "        }\n",
    "        \n",
    "        # 步骤4: 保存结果\n",
    "        self._save_results(results)\n",
    "        \n",
    "        # 步骤5: 生成评估报告\n",
    "        self._generate_report(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_results(self, results: Dict):\n",
    "        \"\"\"\n",
    "        保存评估结果到JSON文件\n",
    "        \n",
    "        参数说明：\n",
    "        results: 包含所有评估结果的字典\n",
    "        \"\"\"\n",
    "        output_file = Path(self.config.output_dir) / \"evaluation_results.json\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"结果已保存到: {output_file}\")\n",
    "    \n",
    "    def _generate_report(self, results: Dict):\n",
    "        \"\"\"\n",
    "        生成评估报告\n",
    "        \n",
    "        根据评估结果生成人类可读的报告，包括：\n",
    "        1. 基本配置信息\n",
    "        2. 各项评估指标\n",
    "        3. 综合评估结论\n",
    "        4. 部署建议\n",
    "        \n",
    "        参数说明：\n",
    "        results: 包含所有评估结果的字典\n",
    "        \"\"\"\n",
    "        # 提取各项评估结果\n",
    "        consistency = results[\"consistency_evaluation\"]\n",
    "        stability = results[\"stability_evaluation\"]\n",
    "        performance = results[\"performance_evaluation\"]\n",
    "        \n",
    "        # 生成报告文本\n",
    "        report = f\"\"\"\n",
    "=== 金融模型量化评估报告 ===\n",
    "\n",
    "评估配置:\n",
    "- 测试样本数量: {results['config']['num_test_samples']}\n",
    "- 生成token数: {results['config']['max_new_tokens']}\n",
    "- 温度参数: {results['config']['temperature']}\n",
    "\n",
    "输出一致性评估:\n",
    "- 平均语义相似度: {consistency['mean_similarity']:.3f}\n",
    "- 相似度标准差: {consistency['std_similarity']:.3f}\n",
    "- 最低相似度: {consistency['min_similarity']:.3f}\n",
    "- 90%分位数: {consistency['percentiles']['90th']:.3f}\n",
    "\n",
    "稳定性评估:\n",
    "- 原始模型自一致性: {stability['original_model']['mean_stability']:.3f}\n",
    "- 量化模型自一致性: {stability['quantized_model']['mean_stability']:.3f}\n",
    "- 稳定性保持率: {stability['stability_retention']:.3f}\n",
    "\n",
    "性能评估:\n",
    "- 原始模型平均推理时间: {performance['original_model']['mean_time']:.2f}秒\n",
    "- 量化模型平均推理时间: {performance['quantized_model']['mean_time']:.2f}秒\n",
    "- 推理速度提升: {performance['speedup']:.2f}倍\n",
    "\n",
    "综合评估结论:\n",
    "\"\"\"\n",
    "        \n",
    "        # 根据评估结果添加结论\n",
    "        if consistency['mean_similarity'] > 0.85:\n",
    "            report += \"输出一致性: 优秀 (相似度 > 0.85)\\n\"\n",
    "        elif consistency['mean_similarity'] > 0.8:\n",
    "            report += \"输出一致性: 良好 (相似度 > 0.8)\\n\"\n",
    "        else:\n",
    "            report += \"输出一致性: 需要改进 (相似度 < 0.8)\\n\"\n",
    "        \n",
    "        if stability['stability_retention'] > 0.9:\n",
    "            report += \"稳定性保持: 优秀 (保持率 > 90%)\\n\"\n",
    "        elif stability['stability_retention'] > 0.8:\n",
    "            report += \"稳定性保持: 良好 (保持率 > 80%)\\n\"\n",
    "        else:\n",
    "            report += \"稳定性保持: 需要改进 (保持率 < 80%)\\n\"\n",
    "        \n",
    "        if performance['speedup'] > 2.0:\n",
    "            report += \"性能提升: 优秀 (加速比 > 2x)\\n\"\n",
    "        elif performance['speedup'] > 1.5:\n",
    "            report += \"性能提升: 良好 (加速比 > 1.5x)\\n\"\n",
    "        else:\n",
    "            report += \"性能提升: 不明显 (加速比 < 1.5x)\\n\"\n",
    "        \n",
    "        # 最终建议\n",
    "        if (consistency['mean_similarity'] > 0.85 and \n",
    "            stability['stability_retention'] > 0.8 and \n",
    "            performance['speedup'] > 1.5):\n",
    "            report += \"\\n建议: 量化效果良好，可以考虑部署到生产环境\\n\"\n",
    "        else:\n",
    "            report += \"\\n建议: 量化效果有待改进，建议调整量化参数或方法\\n\"\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        # 保存报告到文件\n",
    "        report_file = Path(self.config.output_dir) / \"evaluation_report.txt\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        logger.info(f\"报告已保存到: {report_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    \n",
    "    创建评估配置和评估器，执行完整的评估流程\n",
    "    \"\"\"\n",
    "    # 创建评估配置\n",
    "    config = EvaluationConfig(\n",
    "        test_data_path=\"/shared/gptq_model/grpo_prompts_dataset_5k.jsonl\",\n",
    "        output_dir=\"./evaluation_results\",\n",
    "        num_test_samples=20,        # 可调整测试样本数\n",
    "        warmup_rounds=5,            # 预热轮数\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # 创建评估器\n",
    "    evaluator = FinancialModelEvaluator(config)\n",
    "    \n",
    "    try:\n",
    "        # 运行评估\n",
    "        results = evaluator.run_complete_evaluation()\n",
    "        logger.info(\"评估完成!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"评估过程中出现错误: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76054d72",
   "metadata": {},
   "source": [
    "```python \n",
    " python financial_model_evaluation.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57fa4e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716122121728.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214b0c2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716121429523.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9a6a6",
   "metadata": {},
   "source": [
    "# 5. 量化模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955372e3",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# vLLM量化模型部署脚本     注意删除注释使用、注意删除注释使用、注意删除注释使用\n",
    "#\n",
    "# 这个脚本的主要作用：\n",
    "# 1. 启动Ray分布式计算框架（用于多GPU并行推理）\n",
    "# 2. 部署量化模型为API服务\n",
    "# 3. 提供OpenAI兼容的API接口\n",
    "# 4. 支持高并发推理请求\n",
    "#\n",
    "# 使用场景：\n",
    "# - 生产环境部署量化模型\n",
    "# - 提供HTTP API接口供其他系统调用\n",
    "# - 支持多用户并发访问\n",
    "# - 实现模型推理服务化\n",
    "\n",
    "# 基本配置参数\n",
    "HEAD_NODE=\"10.60.68.220\"                                    # 主节点IP地址：Ray集群的协调节点\n",
    "MODEL_PATH=\"/shared/gptq_model/output/best_model_gptq_int4\" # 量化模型路径：要部署的模型文件位置\n",
    "PORT=8000                                                   # API服务端口：HTTP服务监听的端口号\n",
    "\n",
    "echo \"启动Ray集群\"\n",
    "\n",
    "# 激活conda环境\n",
    "# conda是Python包管理器，这里激活特定的Python环境\n",
    "source ~/miniconda3/etc/profile.d/conda.sh  # 加载conda命令\n",
    "conda activate reward3                        # 激活名为reward3的conda环境\n",
    "\n",
    "# 设置分布式训练环境变量\n",
    "# 这些环境变量用于配置多机多卡的分布式计算\n",
    "export MASTER_ADDR=$HEAD_NODE    # 主节点地址：分布式训练的协调节点\n",
    "export MASTER_PORT=29500         # 主节点端口：用于节点间通信的端口\n",
    "export GLOO_SOCKET_IFNAME=eth0   # 网络接口名：指定用于通信的网络接口\n",
    "export NCCL_SOCKET_IFNAME=eth0   # NCCL通信接口：GPU间通信使用的网络接口\n",
    "\n",
    "# 停止现有的Ray进程\n",
    "# 确保没有残留的Ray进程影响新的启动\n",
    "ray stop --force 2>/dev/null || true  # --force强制停止，2>/dev/null忽略错误输出\n",
    "\n",
    "# 启动Ray头节点\n",
    "# Ray是分布式计算框架，用于管理多GPU资源\n",
    "ray start --head \\              # 启动头节点模式\n",
    "  --port=6379 \\                 # Ray服务端口：默认6379\n",
    "  --dashboard-host=0.0.0.0 \\    # 监控面板地址：0.0.0.0表示允许所有IP访问\n",
    "  --dashboard-port=8265 \\       # 监控面板端口：Web界面端口\n",
    "  --num-gpus=1 \\                # GPU数量：当前节点的GPU数量\n",
    "  --num-cpus=24                 # CPU数量：当前节点的CPU核心数\n",
    "\n",
    "echo \"启动vLLM单机服务器...\"\n",
    "\n",
    "# 启动vLLM API服务器\n",
    "# vLLM是高性能推理引擎，支持量化模型推理\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model $MODEL_PATH \\                    # 模型路径：要部署的量化模型\n",
    "  --host 0.0.0.0 \\                        # 监听地址：0.0.0.0表示接受所有IP的请求\n",
    "  --port $PORT \\                          # 服务端口：API服务的HTTP端口\n",
    "  --tensor-parallel-size 1 \\              # 张量并行度：单GPU设置为1\n",
    "  --dtype bfloat16 \\                      # 数据类型：使用bfloat16精度节省内存\n",
    "  --max-model-len 2048 \\                  # 最大序列长度：限制输入和输出的总长度\n",
    "  --gpu-memory-utilization 0.8 \\         # GPU内存利用率：使用80%的GPU内存\n",
    "  --disable-log-requests \\                # 禁用请求日志：减少日志输出\n",
    "  --api-key gptq-model-key \\              # API密钥：用于验证客户端身份\n",
    "  --distributed-executor-backend ray      # 分布式后端：使用Ray管理多GPU资源\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f3793",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716124200155.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd47a8",
   "metadata": {},
   "source": [
    "```python \n",
    "# 测试API服务是否正常启动\n",
    "# 使用Python发送HTTP请求测试API接口\n",
    "python -c \"\n",
    "import requests\n",
    "resp = requests.post('http://localhost:8000/v1/chat/completions', \n",
    "    headers={'Authorization': 'Bearer gptq-model-key'},\n",
    "    json={'model': './output/best_model_gptq_int4', 'messages': [{'role': 'user', 'content': '你是一个专业的金融领域分析师请对以下问题进行详细解答：周三盘前交易时段，金融板块呈现温和上涨态势：金融精选行业SPDR基金(XLF)上涨0.4%，Direxion每日三倍做多金融股ETF(FAS)上涨1.2%，而其反向产品Direxion每日三倍做空金融股ETF(FAZ)下跌1.1%。万事达卡(MA)股价上涨0.7%，因其董事会批准了最高110亿美元的普通股回购计划，并将季度股息提高16%。Cboe全球市场(CBOE)股价上涨0.5%，该公司披露其指数期权合约的11月日均交易量达到近400万份，较去年同期的330万份增长22%。基于这些市场动态，请分析以下问题：在当前市场环境下，影响金融板块ETF(XLF、FAS、FAZ)价格波动的关键驱动因素有哪些？如何评估万事达卡大规模股票回购和股息增长对其长期估值的影响？Cboe交易量的大幅增长可能预示着哪些市场结构变化？请综合考虑宏观经济环境、行业竞争格局和公司特定事件等多重因素展开分析。'}], 'max_tokens': 1024})\n",
    "print('Status Code:', resp.status_code)\n",
    "print('Response:', resp.json())\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be862721",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716123345322.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae5090",
   "metadata": {},
   "source": [
    "# 6. 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe3c9b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250716171337199.png\" width=100%></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
