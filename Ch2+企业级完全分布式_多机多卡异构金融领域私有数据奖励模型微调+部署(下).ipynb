{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe98a9e5-4646-47d0-9445-19bf22ef3a54",
   "metadata": {},
   "source": [
    "# <center>Ch2 企业级完全分布式_多机多卡异构金融领域私有数据奖励模型微调+部署(下)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62520f00",
   "metadata": {},
   "source": [
    "# 1. 本地开发连接服务器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28616f3e",
   "metadata": {},
   "source": [
    "## 方式1：使用Remote - SSH  \n",
    "\n",
    "直接连接服务器方便代码更改，可以直接保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cdbe9",
   "metadata": {},
   "source": [
    "本次连通方式使用为 visual Studio Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e5095",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250623155250822.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f1856",
   "metadata": {},
   "source": [
    "直接进入扩展中进行名称搜索后安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284f456",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153107297.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29fa30",
   "metadata": {},
   "source": [
    "选择搜索栏找到显示并运行命令，或者直接按快捷键(当前电脑环境为mac环境) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc61234",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153347272.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7f491",
   "metadata": {},
   "source": [
    "使用当前命令  增加目标IP的录入  Remote-SSH: Add New SSH Host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe007c6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153425819.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b889e",
   "metadata": {},
   "source": [
    "这里会要求输入目标的ssh命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaecd7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153450588.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab472a5",
   "metadata": {},
   "source": [
    "输入目标服务器ssh命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257eea73",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153513184.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88122d89",
   "metadata": {},
   "source": [
    " 会弹出选择信息保存路径  选择 `~/.ssh/config` 作为配置文件保存位置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488eb0cb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153647334.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf74d3",
   "metadata": {},
   "source": [
    "**使用秘钥形式进行链接**\n",
    "\n",
    "IdentityFile ~/.ssh/id_rsa   \n",
    "  ServerAliveInterval 60   \n",
    "  ServerAliveCountMax 5   \n",
    "\n",
    "  每 60 秒向服务器发送一次保活信号（空数据包），检测连接是否存活。    \n",
    "  如果连续 5 次保活信号（总计 5 × 60 = 300 秒）未收到服务器响应断开连接      \n",
    "\n",
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605162300856.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe57fc",
   "metadata": {},
   "source": [
    "显示添加完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f597480",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153606598.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b8893",
   "metadata": {},
   "source": [
    "\n",
    "输入选择想要连接度服务器IP   Remote-SSH: Connect to Host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc253c5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153721394.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4537747",
   "metadata": {},
   "source": [
    "选择想要连接的服务器地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb274ac",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153748177.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245ddc2",
   "metadata": {},
   "source": [
    "输入连接密码 确认 后连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7303bd8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605153838180.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83752650",
   "metadata": {},
   "source": [
    "选择打开 找到项目目标路径 点击确定 可以进行项目编辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f24db4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605154811870.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc3d73",
   "metadata": {},
   "source": [
    "## 方式二： rsync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17be8b",
   "metadata": {},
   "source": [
    "**window系统安装 在WSL中同步Windows文件到远程服务器**    \n",
    "rsync -avz --progress /mnt/c/Users/YourName/Documents/project/ username@server:/remote/path/\n",
    "\n",
    "**从远程服务器同步到Windows**    \n",
    "rsync -avz --progress username@server:/remote/path/ /mnt/c/Users/YourName/Documents/project/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571480c5",
   "metadata": {},
   "source": [
    "mac电脑本身自带 rsync 软件 \n",
    "\n",
    "如果想要更新或者安装 需要使用 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f2fbc",
   "metadata": {},
   "source": [
    "\n",
    "进入项目文件夹下 cd \"/financial_reward_model\" 创建deploy.sh 脚本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9ee80",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605160358114.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af7878",
   "metadata": {},
   "source": [
    " 创建文件 touch deploy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d4263",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "# 内容如下\n",
    "echo \"🚀 开始同步项目到服务器...\"\n",
    "\n",
    "rsync -avz \\\n",
    "  --exclude '.git' \\\n",
    "  --exclude '__pycache__' \\\n",
    "  --exclude '.DS_Store' \\\n",
    "  ./ \\\n",
    "  ubuntu@117.50.34.28:/shared/financial_reward_model/\n",
    "\n",
    "echo \"✅ 文件同步完成，执行远程操作...\"\n",
    "\n",
    "ssh ubuntu@117.50.34.28 << 'EOF'\n",
    "cd /shared/financial_reward_model\n",
    "# 如果有需要重启的服务或运行命令，请在下面加\n",
    "# 示例: pkill -f app.py && nohup python3 app.py > out.log 2>&1 &\n",
    "echo \"🚀 已进入远程项目目录，可根据需要添加执行命令\"\n",
    "EOF\n",
    "\n",
    "echo \"🎉 部署完毕！\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c817dd",
   "metadata": {},
   "source": [
    "添加可执行权限：chmod +x deploy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d0ad0",
   "metadata": {},
   "source": [
    "目标文件夹要优势用户授权 sudo chown -R ubuntu:ubuntu /shared  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601df8f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605160507639.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e967e7",
   "metadata": {},
   "source": [
    "执行文件 就会将本地代码内容同步到服务器中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e0ae1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605160601767.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050414ee",
   "metadata": {},
   "source": [
    "执行会要求输入服务器连接密码 与ssh连接方式相同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc1432",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605160727146.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc6acf",
   "metadata": {},
   "source": [
    "如果嫌麻烦可以选择将本地秘钥上传到服务器中，之后就不用每次上传重新输入密码\n",
    "\n",
    "**本地生成密钥**根据当前用户创建秘钥\n",
    "\n",
    "ssh-keygen -t rsa -C \"个人电脑用户\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dc4ea",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605160953565.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c007fef",
   "metadata": {},
   "source": [
    "**将公钥发送到服务器** \n",
    "\n",
    "ssh-copy-id ubuntu@106.75.127.84\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d696c54",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605161138954.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9e4c7",
   "metadata": {},
   "source": [
    "直接执行脚本，不用二次输入密码  ./deploy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87944a8d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605161203070.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc40747",
   "metadata": {},
   "source": [
    "## 基础环境校验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a86a27",
   "metadata": {},
   "source": [
    "创建检查文件：\n",
    "\n",
    "check_data.py(可选)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca5516",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def find_data_files(data_base_path):\n",
    "    \"\"\"查找数据文件\"\"\"\n",
    "    train_dir = os.path.join(data_base_path, \"train\")\n",
    "    eval_dir = os.path.join(data_base_path, \"eval\")\n",
    "    \n",
    "    print(f\"查找数据文件...\")\n",
    "    print(f\"训练数据目录: {train_dir}\")\n",
    "    print(f\"评估数据目录: {eval_dir}\")\n",
    "    \n",
    "    # 查找训练数据文件\n",
    "    train_files = []\n",
    "    if os.path.exists(train_dir):\n",
    "        train_files = [f for f in os.listdir(train_dir) if f.endswith('.jsonl')]\n",
    "        print(f\"训练目录中的jsonl文件: {train_files}\")\n",
    "    \n",
    "    # 查找评估数据文件\n",
    "    eval_files = []\n",
    "    if os.path.exists(eval_dir):\n",
    "        eval_files = [f for f in os.listdir(eval_dir) if f.endswith('.jsonl')]\n",
    "        print(f\"评估目录中的jsonl文件: {eval_files}\")\n",
    "    \n",
    "    return train_files, eval_files\n",
    "\n",
    "# ============================================================================\n",
    "#  偏好数据检查函数：验证preference数据的结构和内容\n",
    "# \n",
    "#  这个函数在项目中的作用：\n",
    "# 1. 【数据验证】：确保训练数据存在且格式正确\n",
    "# 2. 【问题预防】：在训练开始前发现数据问题\n",
    "# 3. 【格式检查】：验证JSON结构是否符合期望\n",
    "# 4. 【统计报告】：提供数据集的基本统计信息\n",
    "# \n",
    "#  注意：这个函数在项目中【很少被调用】\n",
    "# - 主要用于调试和问题排查\n",
    "# - 不是训练流程的必需部分\n",
    "# ============================================================================\n",
    "def check_preference_data_structure():\n",
    "    \"\"\"\n",
    "    检查preference数据结构和内容的主要函数\n",
    "    \n",
    "    这个函数会检查以下内容：\n",
    "    1. 数据目录结构是否正确\n",
    "    2. 数据文件是否存在\n",
    "    3. JSON格式是否正确\n",
    "    4. 必需字段是否完整\n",
    "    5. 数据内容是否合理\n",
    "    \n",
    "    返回：\n",
    "    - 如果检查通过：(True, train_file, eval_file)\n",
    "    - 如果检查失败：(False, None, None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 第1步：设置数据路径（硬编码路径）\n",
    "    data_base_path = \"../reward_model_data/reward_data\"\n",
    "    \n",
    "    print(\"=== 数据结构检查 ===\")\n",
    "    print(f\"数据基础路径: {data_base_path}\")\n",
    "    \n",
    "    # 第2步：检查基础路径是否存在\n",
    "    if not os.path.exists(data_base_path):\n",
    "        print(f\" 数据基础路径不存在: {data_base_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 第3步：查找数据文件\n",
    "    train_files, eval_files = find_data_files(data_base_path)\n",
    "    \n",
    "    # 第4步：选择数据文件（优先使用preference_dataset.jsonl）\n",
    "    train_file = \"preference_dataset.jsonl\" if \"preference_dataset.jsonl\" in train_files else (train_files[0] if train_files else None)\n",
    "    eval_file = \"preference_dataset.jsonl\" if \"preference_dataset.jsonl\" in eval_files else (eval_files[0] if eval_files else None)\n",
    "    \n",
    "    # 第5步：验证数据文件存在性\n",
    "    if not train_file:\n",
    "        print(f\" 训练目录中没有找到jsonl文件\")\n",
    "        return False\n",
    "    \n",
    "    if not eval_file:\n",
    "        print(f\" 评估目录中没有找到jsonl文件\")\n",
    "        return False\n",
    "    \n",
    "    # 第6步：构建完整文件路径\n",
    "    train_path = os.path.join(data_base_path, \"train\", train_file)\n",
    "    eval_path = os.path.join(data_base_path, \"eval\", eval_file)\n",
    "    \n",
    "    print(f\" 使用训练文件: {train_file}\")\n",
    "    print(f\" 使用评估文件: {eval_file}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    #  数据内容检查子函数：验证单个数据文件的内容\n",
    "    # \n",
    "    #  这个内部函数的作用：\n",
    "    # 1. 【格式验证】：检查每行是否是有效的JSON\n",
    "    # 2. 【字段检查】：验证必需字段是否存在\n",
    "    # 3. 【内容分析】：分析数据内容的合理性\n",
    "    # 4. 【统计报告】：提供数据集的详细统计\n",
    "    # ========================================================================\n",
    "    def check_file_content(file_path, split_name):\n",
    "        \"\"\"\n",
    "        检查单个数据文件的内容\n",
    "        \n",
    "        参数说明：\n",
    "        - file_path: 要检查的文件路径\n",
    "        - split_name: 数据集名称（\"训练\" 或 \"评估\"）\n",
    "        \n",
    "        返回：\n",
    "        - True: 检查通过\n",
    "        - False: 检查失败\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- {split_name} 数据检查 ---\")\n",
    "        try:\n",
    "            # 读取所有行\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            print(f\"数据行数: {len(lines)}\")\n",
    "            \n",
    "            # 检查是否为空\n",
    "            if len(lines) == 0:\n",
    "                print(f\" {split_name} 数据为空\")\n",
    "                return False\n",
    "            \n",
    "            # 检查前几行数据格式（避免检查所有数据，节省时间）\n",
    "            for i, line in enumerate(lines[:2]):  # 只检查前2行作为样本\n",
    "                try:\n",
    "                    data = json.loads(line.strip())  # 解析JSON\n",
    "                    \n",
    "                    print(f\"样本 {i+1}:\")\n",
    "                    print(f\"  字段: {list(data.keys())}\")\n",
    "                    \n",
    "                    # 检查question字段\n",
    "                    if \"question\" in data:\n",
    "                        print(f\"   包含question字段\")\n",
    "                        print(f\"  问题长度: {len(data['question'])}\")\n",
    "                    else:\n",
    "                        print(f\"  缺少question字段\")\n",
    "                        return False\n",
    "                    \n",
    "                    # 检查answers字段（用于preference训练）\n",
    "                    if \"answers\" in data:\n",
    "                        print(f\"   包含answers字段\")\n",
    "                        if isinstance(data[\"answers\"], list):\n",
    "                            print(f\"  答案数量: {len(data['answers'])}\")\n",
    "                            if len(data[\"answers\"]) >= 2:\n",
    "                                print(f\"   至少有2个答案，可用于preference训练\")\n",
    "                            else:\n",
    "                                print(f\"    只有{len(data['answers'])}个答案，可能不足以进行preference训练\")\n",
    "                        else:\n",
    "                            print(f\"    answers不是列表格式\")\n",
    "                    \n",
    "                    # 检查其他字段并统计信息\n",
    "                    for key, value in data.items():\n",
    "                        if key not in [\"question\", \"answers\"]:\n",
    "                            if isinstance(value, str):\n",
    "                                print(f\"  {key}: {len(value)} 字符\")\n",
    "                            else:\n",
    "                                print(f\"  {key}: {type(value).__name__}\")\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\" 第{i+1}行JSON格式错误: {e}\")\n",
    "                    return False\n",
    "            \n",
    "            print(f\" {split_name} 数据格式基本正确\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" 读取{split_name}数据时出错: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # 第7步：检查训练和评估数据\n",
    "    train_ok = check_file_content(train_path, \"训练\")\n",
    "    eval_ok = check_file_content(eval_path, \"评估\")\n",
    "    \n",
    "    # 第8步：汇总检查结果\n",
    "    if train_ok and eval_ok:\n",
    "        print(\"\\n 数据检查通过！\")\n",
    "        print(f\" 数据文件信息:\")\n",
    "        print(f\"  训练文件: {train_file}\")\n",
    "        print(f\"  评估文件: {eval_file}\")\n",
    "        print(f\"\\n  注意: 需要根据实际数据格式修改数据集类\")\n",
    "        return True, train_file, eval_file\n",
    "    else:\n",
    "        print(\"\\n 数据检查失败，请修复数据问题后再试。\")\n",
    "        return False, None, None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  模型路径检查函数：验证预训练模型文件是否完整\n",
    "# \n",
    "#  这个函数在项目中的作用：\n",
    "# 1. 【模型验证】：确保预训练模型文件存在且完整\n",
    "# 2. 【路径检查】：验证模型路径配置是否正确\n",
    "# 3. 【文件完整性】：检查关键模型文件是否齐全\n",
    "# 4. 【格式支持】：检查模型格式（safetensors/pytorch）\n",
    "# \n",
    "#  注意：这个函数在项目中【很少被调用】\n",
    "# - 主要用于环境配置验证\n",
    "# - 硬编码了模型路径，可能需要根据实际情况调整\n",
    "# ============================================================================\n",
    "def check_model_path():\n",
    "    \"\"\"\n",
    "    检查预训练模型路径和文件完整性\n",
    "    \n",
    "    这个函数会检查以下内容：\n",
    "    1. 模型目录是否存在\n",
    "    2. 关键配置文件是否存在（config.json, tokenizer.json）\n",
    "    3. 模型权重文件是否存在（safetensors或pytorch格式）\n",
    "    4. 文件格式是否支持\n",
    "    \n",
    "    返回：\n",
    "    - True: 模型检查通过\n",
    "    - False: 模型检查失败\n",
    "    \"\"\"\n",
    "    # 硬编码的模型路径（ 可能需要根据实际情况调整）\n",
    "    model_path = \"/shared/QRM-Llama3.1-8B-v2\"\n",
    "    \n",
    "    print(f\"\\n=== 模型路径检查 ===\")\n",
    "    print(f\"模型路径: {model_path}\")\n",
    "    \n",
    "    # 第1步：检查模型目录是否存在\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\" 模型路径不存在: {model_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 第2步：检查关键配置文件\n",
    "    required_files = [\"config.json\", \"tokenizer.json\"]  # 必需的配置文件\n",
    "    missing_files = []\n",
    "    \n",
    "    for file_name in required_files:\n",
    "        file_path = os.path.join(model_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\" {file_name} 存在\")\n",
    "        else:\n",
    "            missing_files.append(file_name)\n",
    "    \n",
    "    # 第3步：检查模型权重文件\n",
    "    model_files_found = False\n",
    "    \n",
    "    # 获取目录中的所有文件\n",
    "    all_files = os.listdir(model_path)\n",
    "    \n",
    "    # 检查safetensors格式（推荐格式）\n",
    "    safetensors_files = [f for f in all_files if f.endswith(\".safetensors\")]\n",
    "    if safetensors_files:\n",
    "        print(f\" 找到safetensors模型文件: {len(safetensors_files)} 个\")\n",
    "        model_files_found = True\n",
    "    \n",
    "    # 检查pytorch格式（传统格式）\n",
    "    pytorch_files = [f for f in all_files if f.endswith(\".bin\") and \"pytorch_model\" in f]\n",
    "    if pytorch_files:\n",
    "        print(f\" 找到pytorch模型文件: {len(pytorch_files)} 个\")\n",
    "        model_files_found = True\n",
    "    \n",
    "    # 第4步：检查是否找到模型权重文件\n",
    "    if not model_files_found:\n",
    "        print(f\"未找到模型权重文件\")\n",
    "        missing_files.append(\"model_weights\")\n",
    "    \n",
    "    # 第5步：汇总检查结果\n",
    "    if missing_files:\n",
    "        print(f\" 缺少关键文件: {missing_files}\")\n",
    "        return False\n",
    "    \n",
    "    print(\" 模型文件检查通过\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  主执行函数：脚本的入口点，协调所有检查流程\n",
    "# \n",
    "#  这个部分在项目中的作用：\n",
    "# 1. 【流程协调】：协调数据检查和模型检查\n",
    "# 2. 【结果汇总】：汇总所有检查结果\n",
    "# 3. 【状态返回】：通过退出码告知调用者检查结果\n",
    "# 4. 【用户友好】：提供清晰的成功/失败信息\n",
    "# \n",
    "#  注意：这个脚本在项目中【很少被实际使用】\n",
    "# - 被run_training.sh引用，但run_training.sh本身也未被使用\n",
    "# - 主要用于手动调试和环境验证\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    脚本主入口：当直接运行此脚本时执行\n",
    "    \n",
    "    执行流程：\n",
    "    1. 执行数据结构和内容检查\n",
    "    2. 执行模型路径和文件检查\n",
    "    3. 汇总所有检查结果\n",
    "    4. 根据检查结果设置退出码\n",
    "    \n",
    "    退出码说明：\n",
    "    - 0: 所有检查通过\n",
    "    - 1: 检查失败\n",
    "    \"\"\"\n",
    "    print(\"开始环境和数据检查...\\n\")\n",
    "    \n",
    "    # 第1步：执行数据检查\n",
    "    data_result = check_preference_data_structure()\n",
    "    if isinstance(data_result, tuple):\n",
    "        data_ok, train_file, eval_file = data_result  # 解包返回值\n",
    "    else:\n",
    "        data_ok = data_result\n",
    "        train_file = eval_file = None\n",
    "    \n",
    "    # 第2步：执行模型检查\n",
    "    model_ok = check_model_path()\n",
    "    \n",
    "    # 第3步：汇总检查结果\n",
    "    print(f\"\\n=== 检查结果汇总 ===\")\n",
    "    print(f\"数据检查: {' 通过' if data_ok else '失败'}\")\n",
    "    print(f\"模型检查: {' 通过' if model_ok else '失败'}\")\n",
    "    \n",
    "    # 第4步：根据检查结果提供相应的信息和退出码\n",
    "    if data_ok and model_ok:\n",
    "        print(\"\\n 所有检查通过！环境准备就绪。\")\n",
    "        if train_file and eval_file:\n",
    "            print(f\"\\n 建议使用的数据文件:\")\n",
    "            print(f\"   训练文件: {train_file}\")\n",
    "            print(f\"   评估文件: {eval_file}\")\n",
    "        sys.exit(0)  # 成功退出\n",
    "    else:\n",
    "        print(\"\\n 检查失败，请修复问题后再试。\")\n",
    "        sys.exit(1)  # 失败退出\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90fd64",
   "metadata": {},
   "source": [
    "python scripts/setup/check_data.py (注意执行路径以及文件对应的相对路径)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18446059",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605163307140.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9790b2",
   "metadata": {},
   "source": [
    "创建deepspeed 环境检查路径   test_deepspeed.py(可选)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46b549",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DeepSpeed Stage 3测试脚本 - 最大内存优化\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import deepspeed\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForSequenceClassification\n",
    "import gc\n",
    "\n",
    "def main():\n",
    "    \"\"\"DeepSpeed Stage 3测试 - 参数分片\"\"\"\n",
    "    \n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    \n",
    "    print(f\"进程 {rank}/{world_size} 在GPU {local_rank}上启动 (Stage 3)\")\n",
    "    \n",
    "    # 设置环境变量\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # 设置GPU和内存优化\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    model_path = \"/home/ubuntu/QRM-Llama3.1-8B-v2\"\n",
    "    \n",
    "    try:\n",
    "        # 1. 加载tokenizer\n",
    "        if rank == 0:\n",
    "            print(\" 加载tokenizer...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # 2. 加载模型配置\n",
    "        if rank == 0:\n",
    "            print(\"🤖 加载模型配置...\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        config.num_labels = 1\n",
    "        \n",
    "        # 3. Stage 3需要在CPU上初始化模型\n",
    "        if rank == 0:\n",
    "            print(\"🔧 在CPU上初始化模型（Stage 3模式）...\")\n",
    "        \n",
    "        with torch.device('cpu'):\n",
    "            model = LlamaForSequenceClassification.from_pretrained(\n",
    "                model_path,\n",
    "                config=config,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=None\n",
    "            )\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\" 模型初始化成功（CPU）\")\n",
    "            print(f\"   总参数量: {total_params:,}\")\n",
    "            print(f\"   模型大小: {total_params * 2 / 1024**3:.2f}GB (bf16)\")\n",
    "        \n",
    "        # 4. DeepSpeed Stage 3配置\n",
    "        if rank == 0:\n",
    "            print(\" 配置DeepSpeed Stage 3...\")\n",
    "        \n",
    "        ds_config = {\n",
    "            \"train_batch_size\": 4,\n",
    "            \"train_micro_batch_size_per_gpu\": 2,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \n",
    "            \"zero_allow_untested_optimizer\": True,\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 3,                            # Stage 3 - 参数分片\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",                   # 优化器状态卸载到CPU\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"offload_param\": {\n",
    "                    \"device\": \"cpu\",                   # 参数卸载到CPU\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True,\n",
    "                \"sub_group_size\": 1e9,\n",
    "                \"reduce_bucket_size\": 1e8,\n",
    "                \"stage3_prefetch_bucket_size\": 1e8,\n",
    "                \"stage3_param_persistence_threshold\": 1e6,\n",
    "                \"stage3_max_live_parameters\": 1e9,\n",
    "                \"stage3_max_reuse_distance\": 1e9\n",
    "            },\n",
    "            \n",
    "            \"optimizer\": {\n",
    "                \"type\": \"SGD\",\n",
    "                \"params\": {\n",
    "                    \"lr\": 1e-3,\n",
    "                    \"momentum\": 0.9,\n",
    "                    \"weight_decay\": 0.01\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            \"bf16\": {\"enabled\": True},\n",
    "            \"wall_clock_breakdown\": False,\n",
    "            \"steps_per_print\": 10\n",
    "        }\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\" DeepSpeed Stage 3配置完成\")\n",
    "            print(\"   - 参数分片: 启用\")\n",
    "            print(\"   - 优化器CPU卸载: 启用\") \n",
    "            print(\"   - 参数CPU卸载: 启用\")\n",
    "        \n",
    "        # 5. 初始化DeepSpeed引擎\n",
    "        if rank == 0:\n",
    "            print(\" 初始化DeepSpeed Stage 3引擎...\")\n",
    "        \n",
    "        engine, optimizer, _, _ = deepspeed.initialize(\n",
    "            model=model,\n",
    "            config=ds_config\n",
    "        )\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\" DeepSpeed Stage 3引擎初始化成功\")\n",
    "            print(f\"   引擎类型: {type(engine).__name__}\")\n",
    "        \n",
    "        # 6. 测试前向传播\n",
    "        if rank == 0:\n",
    "            print(\" 测试前向传播（Stage 3）...\")\n",
    "        \n",
    "        batch_size = 1\n",
    "        seq_length = 128\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        input_ids = torch.randint(\n",
    "            0, vocab_size, \n",
    "            (batch_size, seq_length), \n",
    "            device=f\"cuda:{local_rank}\"\n",
    "        )\n",
    "        attention_mask = torch.ones(\n",
    "            (batch_size, seq_length), \n",
    "            device=f\"cuda:{local_rank}\"\n",
    "        )\n",
    "        \n",
    "        outputs = engine(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\" 前向传播成功\")\n",
    "            print(f\"   输入形状: {input_ids.shape}\")\n",
    "            print(f\"   输出形状: {logits.shape}\")\n",
    "        \n",
    "        # 7. 测试反向传播\n",
    "        if rank == 0:\n",
    "            print(\" 测试反向传播（Stage 3）...\")\n",
    "        \n",
    "        loss = logits.mean()\n",
    "        engine.backward(loss)\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\" 反向传播成功，损失值: {loss.item():.6f}\")\n",
    "        \n",
    "        # 8. 测试优化器步骤\n",
    "        if rank == 0:\n",
    "            print(\"⚡ 测试优化器步骤（Stage 3）...\")\n",
    "        \n",
    "        engine.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\" 优化器步骤成功\")\n",
    "        \n",
    "        # 9. 内存统计\n",
    "        if rank == 0:\n",
    "            print(\" Stage 3内存使用统计:\")\n",
    "            for i in range(world_size):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"   GPU {i}: 分配 {memory_allocated:.2f}GB, 保留 {memory_reserved:.2f}GB\")\n",
    "        \n",
    "        # 10. 成功总结\n",
    "        if rank == 0:\n",
    "            print(\"\\n DeepSpeed Stage 3测试成功！\")\n",
    "            print(\" 验证结果:\")\n",
    "            print(\"   - Stage 3参数分片正常 \")\n",
    "            print(\"   - CPU卸载功能正常 \")\n",
    "            print(\"   - 内存使用大幅降低 \")\n",
    "            print(\"   - 8B模型可在双4090上运行 \")\n",
    "            print(f\"\\n Stage 3可以处理更大的模型和批次大小\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" 进程 {rank} Stage 3测试失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        print(\" 错误：此脚本必须通过torchrun启动\")\n",
    "        print(\"正确启动命令:\")\n",
    "        print(\"   torchrun --nproc_per_node=2 scripts/setup/test_deepspeed.py\")\n",
    "        exit(1)\n",
    "    \n",
    "    success = main()\n",
    "    \n",
    "    if int(os.environ.get('RANK', 0)) == 0:\n",
    "        if success:\n",
    "            print(\"\\n DeepSpeed Stage 3测试完全通过！\")\n",
    "            print(\" 结论：Stage 3可以在双4090上训练8B模型\")\n",
    "        else:\n",
    "            print(\"\\n Stage 3测试失败！\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c898893",
   "metadata": {},
   "source": [
    "torchrun --nproc_per_node=2 scripts/setup/test_deepspeed.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50524fdc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605211645562.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461a36a",
   "metadata": {},
   "source": [
    "环境校验查看GPU变化情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2994d7a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605211722414.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5165f9",
   "metadata": {},
   "source": [
    "最终输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2663d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250605211744988.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde62dd5",
   "metadata": {},
   "source": [
    "# 2. 搭建微调集群环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff4575",
   "metadata": {},
   "source": [
    "## 1.基础搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af365cf",
   "metadata": {},
   "source": [
    "当前租赁服务器为三台，对应会因网络传递速度不同调整使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48755b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624105248164.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6c530",
   "metadata": {},
   "source": [
    "首先使用ping命令访问目标机器保证网络连通性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430f092",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616151248339.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde12c60",
   "metadata": {},
   "source": [
    "```bash   \n",
    "# 服务器之间相互免密登录\n",
    "\n",
    "#在host0上执行 \n",
    "ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N \"\" \n",
    "```\n",
    "\n",
    "```bash   #查看生成的公钥\n",
    "cat ~/.ssh/id_rsa.pub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fdd0a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616151132830.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2121a",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#方法1：使用ssh-copy-id（推荐）\n",
    "ssh-copy-id -o StrictHostKeyChecking=no ubuntu@10.60.240.249\n",
    "   #方法2：如果ssh-copy-id不可用，手动复制\n",
    "   #先复制公钥内容，然后在host1上执行：\n",
    "   #mkdir -p ~/.ssh\n",
    "   #echo \"你的公钥内容\" >> ~/.ssh/authorized_keys\n",
    "   #chmod 600 ~/.ssh/authorized_keys\n",
    "   #chmod 700 ~/.ssh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f651b90",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616151909597.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a521d9d",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#在host0上测试\n",
    "ssh ubuntu@10.60.240.249 \"hostname && whoami\"\n",
    "  #如果成功，应该显示host1的主机名和用户\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105acbf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152026233.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d676d46",
   "metadata": {},
   "source": [
    "host1 也做同样免密登录，以及后续如果有更多服务器增加都要增加免密登录打通节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d2508",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152322086.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5f3e1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152344974.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e65046",
   "metadata": {},
   "source": [
    "## 环境同步方式一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba0022",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#  做环境同步(优先在本地验证环境没问题再进行环境同步)\n",
    "# 方式一 ：导出当前环境依赖包版本\n",
    "#在host0上执行\n",
    "conda activate reward\n",
    "  #导出完整环境\n",
    "conda env export > /tmp/reward_environment.yml\n",
    "  #导出pip包列表（备用）\n",
    "pip freeze > /tmp/requirements.txt\n",
    "   #查看关键包版本\n",
    "python -c \"\n",
    "import torch, deepspeed, transformers\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'DeepSpeed: {deepspeed.__version__}')  \n",
    "print(f'Transformers: {transformers.__version__}')\n",
    "print(f'CUDA: {torch.version.cuda}')\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426025a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152513408.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bfeea",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#在host0上执行  当前环境版本发送到目标服务器中\n",
    "scp /tmp/reward_environment.yml ubuntu@10.60.240.249:/tmp/\n",
    "scp /tmp/requirements.txt ubuntu@10.60.240.249:/tmp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af5da9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152600121.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646a8d2",
   "metadata": {},
   "source": [
    "Host1 进行查看 校验是否收到\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933e8c1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616152650884.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a734c12",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#检查目标服务器是否已安装conda  没有的话进行安装\n",
    "which conda\n",
    "​    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "​    bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\n",
    "​    echo 'export PATH=\"$HOME/miniconda3/bin:$PATH\"' >> ~/.bashrc\n",
    "​    source ~/.bashrc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd47da",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616153759089.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee241ede",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616153843648.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ec6b9",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#根据传输过来的conda内容创建reward环境\n",
    "conda env create -f /tmp/reward_environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852c93a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616154042672.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880b9f1",
   "metadata": {},
   "source": [
    "```bash   \n",
    "\n",
    "#安装完成激活环境并验证\n",
    "conda activate reward\n",
    "python -c \"\n",
    "import torch, deepspeed, transformers\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'DeepSpeed: {deepspeed.__version__}')\n",
    "print(f'Transformers: {transformers.__version__}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'GPU Count: {torch.cuda.device_count()}')\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820a1e9",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 配置conda环境镜像源\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge\n",
    "conda config --add channels defaults\n",
    "conda config --set show_channel_urls true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea45015",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616155641076.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc9f86",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616155605679.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9e0c1",
   "metadata": {},
   "source": [
    "遇到这种无法切换以及使用的状况，建议重新连接服务器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6a3e3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616160135411.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2657b",
   "metadata": {},
   "source": [
    "source ~/.bashrc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691500ac",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616160155467.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2db9c",
   "metadata": {},
   "source": [
    "## 环境同步方式二(推荐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d22080",
   "metadata": {},
   "source": [
    "\n",
    "```bash   \n",
    "# 环境同步 方式二 主要想法是将当前环境进行打包，发送到目标服务器解压后安装\n",
    "#在host0上执行\n",
    "conda activate reward\n",
    " #方法1：使用conda-pack（推荐）\n",
    " #先安装conda-pack\n",
    "conda install conda-pack -y\n",
    "  # 打包环境\n",
    "conda pack -n reward -o reward_env.tar.gz\n",
    "  #传输到host1\n",
    "scp reward_env.tar.gz ubuntu@10.60.240.249:/tmp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e190a60",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616162210429.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf30dc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616163038898.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e46cf",
   "metadata": {},
   "source": [
    "在host1 中环境查看"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bbce",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616163107428.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158e752",
   "metadata": {},
   "source": [
    "```bash   \n",
    "\n",
    "#在host1上执行\n",
    "#创建环境目录\n",
    "mkdir -p ~/miniconda3/envs/reward\n",
    "\n",
    " #解压环境\n",
    "cd ~/miniconda3/envs/reward\n",
    "tar -xzf /tmp/reward_env.tar.gz\n",
    "\n",
    "#激活环境\n",
    "source ~/miniconda3/envs/reward/bin/activate\n",
    "#修复路径\n",
    "conda-unpack\n",
    " #测试环境\n",
    "python -c \"import torch, deepspeed, transformers; print('Environment OK')\"\n",
    "\n",
    "python -c \"\n",
    "import torch, deepspeed, transformers\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'DeepSpeed: {deepspeed.__version__}')\n",
    "print(f'Transformers: {transformers.__version__}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
    "print(f'GPU Count: {torch.cuda.device_count()}')\n",
    "\"\n",
    "同步完成\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5360ae9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616172814809.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd441167",
   "metadata": {},
   "source": [
    "## 数据挂载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d418f04",
   "metadata": {},
   "source": [
    "### 安装NFS(必须)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd7ab9",
   "metadata": {},
   "source": [
    "```bash   \n",
    "# 当前操作也可以使用相互同步的方式只要保证每台服务器节点对应数据以及程序文件相同即可\n",
    "#退出conda环境，回到系统级操作  \n",
    "conda deactivate\n",
    "#安装NFS客户端\n",
    "sudo apt update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13459bc0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616174057336.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a4e7c",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#在host0上执行 当前使用host0 为主节点\n",
    "sudo apt update\n",
    "sudo apt install nfs-kernel-server -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea971303",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616174303816.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90457b73",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#创建共享目录\n",
    "sudo mkdir -p /shared/\n",
    "#设置权限\n",
    "sudo chown -R ubuntu:ubuntu /shared/\n",
    "chmod -R 755 /shared/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11ca97",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616174627165.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4d749",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616174652212.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13173b25",
   "metadata": {},
   "source": [
    "如果在别处存储文件建议直接放到共享目录中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18723f00",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616175224916.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d510b",
   "metadata": {},
   "source": [
    "```bash   \n",
    "\n",
    "#配置NFS导出（使用内网网段）\n",
    "sudo tee /etc/exports << EOF\n",
    "/shared 10.60.0.0/16(rw,sync,no_subtree_check,no_root_squash,insecure)\n",
    "EOF\n",
    "#启动NFS服务\n",
    "sudo systemctl enable nfs-kernel-server\n",
    "sudo systemctl start nfs-kernel-server\n",
    "sudo exportfs -ra\n",
    " #验证NFS配置\n",
    "sudo exportfs -v\n",
    "showmount -e localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42ff84",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616180631692.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc173cab",
   "metadata": {},
   "source": [
    "```bash  \n",
    " #SSH到host1\n",
    "ssh ubuntu@10.60.240.249\n",
    "#安装NFS客户端\n",
    "sudo apt update\n",
    "sudo apt install nfs-common -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c6b44",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616181449090.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39e634",
   "metadata": {},
   "source": [
    "\n",
    "```bash   \n",
    "#创建挂载点\n",
    "sudo mkdir -p /shared\n",
    "#测试挂载\n",
    "sudo mount -t nfs 10.60.11.131:/shared /shared\n",
    " #验证挂载\n",
    "ls -la /shared/\n",
    "df -h | grep shared \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4dc91",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616181553612.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e1db0",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#设置开机自动挂载\n",
    "echo \"10.60.11.131:/shared /shared nfs defaults,_netdev 0 0\" | sudo tee -a /etc/fstab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132546",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616181635576.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777926b",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 创建软连接(可选 看个人习惯)\n",
    "ln -s /shared/financial_reward_model /home/ubuntu/financial_reward_model\n",
    "ln -s /shared/QRM-Llama3.1-8B-v2 /home/ubuntu/QRM-Llama3.1-8B-v2\n",
    "ln -s /shared/reward_model_data /home/ubuntu/reward_model_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a68f9",
   "metadata": {},
   "source": [
    "\n",
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616181933571.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3bff3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616181959724.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3a94c",
   "metadata": {},
   "source": [
    "### 安装pdsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e650683",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#在host0上执行  只是为了在单节点方便控制其他服务器\n",
    "sudo apt install pdsh -y\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baed175",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616190355492.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee4d96",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#创建PDSH配置目录\n",
    "mkdir -p ~/.pdsh\n",
    "\n",
    " #创建机器列表\n",
    "cat > ~/.pdsh/machines << EOF\n",
    "\n",
    " #训练集群节点\n",
    "10.60.11.131   # host0 - 主节点\n",
    "10.60.240.249  # host1 - 从节点\n",
    "EOF\n",
    "\n",
    "#创建组配置\n",
    "cat > ~/.pdsh/groups << EOF\n",
    "all: 10.60.11.131,10.60.240.249\n",
    "workers: 10.60.240.249\n",
    "master: 10.60.11.131\n",
    "EOF\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee76473",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616191950570.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0f87e",
   "metadata": {},
   "source": [
    "```bash   \n",
    "# 添加自己的主机密钥到known_hosts 个人主机IP\n",
    "ssh-keyscan -H 10.60.240.249 >> ~/.ssh/known_hosts\n",
    "ssh-keyscan -H $(hostname) >> ~/.ssh/known_hosts\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc6475",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616205030372.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea3193",
   "metadata": {},
   "source": [
    "```bash\n",
    "# host0\n",
    "echo 'export PDSH_RCMD_TYPE=ssh' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a465343",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616204249403.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18caf5e",
   "metadata": {},
   "source": [
    "```bash\n",
    "# host0\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 600 ~/.ssh/authorized_keys\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c9d4a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616192205559.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45600e60",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#测试PDSH连接\n",
    "pdsh -w 10.60.11.131,10.60.240.249 \"nvidia-smi --query-gpu=count --format=csv,noheader,nounits\"\n",
    "pdsh -w 10.60.11.131,10.60.240.249 'echo \"$(hostname): $(whoami) - $(date)\"'\n",
    "pdsh -w 10.60.11.131,10.60.240.249 \"hostname\"\n",
    "pdsh -w 10.60.11.131,10.60.240.249 'echo \"PDSH test from $(hostname)\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d4d39",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616192253895.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801edb44",
   "metadata": {},
   "source": [
    "```bash   \n",
    "#安装完成后配置防火墙 当前只是防火墙配置 也可以不使用pdsh单独执行\n",
    "\n",
    "pdsh -w 10.60.11.131,10.60.240.249 \"\n",
    "​    echo '=== \\$(hostname) 配置防火墙 ===' && \\\n",
    "​    sudo ufw --force enable && \\\n",
    "​    sudo ufw allow 22 && \\\n",
    "​    sudo ufw allow 29500 && \\\n",
    "​    sudo ufw allow 29501 && \\·\n",
    "​    sudo ufw allow 2049 && \\\n",
    "​    sudo ufw allow from 10.60.0.0/16 && \\\n",
    "​    sudo ufw status numbered\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49712034",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250616205304441.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e541f2",
   "metadata": {},
   "source": [
    "```bash \n",
    "# 两个节点都需要\n",
    "sudo apt update\n",
    "sudo apt install -y ninja-build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448bdff",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250617191918965.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb78ca",
   "metadata": {},
   "source": [
    "# 3. 微调项目构建  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66308e",
   "metadata": {},
   "source": [
    "集群搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283c279",
   "metadata": {},
   "source": [
    "以上主要主要依赖版本适用于QRM-Llama3.1-8B-v2模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fc544",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Skywork/Skywork-Reward-Llama-3.1-8B版本环境\n",
    "conda create -n reward2 python=3.11 -y\n",
    "pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1  -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install accelerate==0.29.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install transformers==4.51.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install deepspeed==0.15.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install datasets==2.14.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe123142",
   "metadata": {},
   "source": [
    "## 项目整体结构分析\n",
    "\n",
    "这是一个基于LLaMA-3.1-8B的奖励模型训练和部署项目，用于评估AI回答的质量，通过对比学习训练模型判断哪个回答更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6046d4",
   "metadata": {},
   "source": [
    "\n",
    "### 核心目录结构\n",
    "\n",
    "```json\n",
    "financial_reward_model/\n",
    "├── configs/           # 配置文件目录\n",
    "├── src/              # 源代码目录  \n",
    "├── scripts/          # 脚本工具目录\n",
    "├── inference/        # 推理部署目录\n",
    "└── deploy.sh         # 部署脚本\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87ce18",
   "metadata": {},
   "source": [
    "### 1. **`configs/` - 配置管理中心**\n",
    "```json\n",
    "configs/\n",
    "├── training/\n",
    "    ├── config.json           # 训练主配置文件\n",
    "    └── deepspeed_only.json   # DeepSpeed优化配置\n",
    "```\n",
    "\n",
    "**作用**：\n",
    "- `config.json`：核心训练参数（学习率、批次大小、模型路径、freeze tuning配置等）\n",
    "- `deepspeed_only.json`：DeepSpeed Stage 2配置，实现内存优化和分布式训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fd8b4",
   "metadata": {},
   "source": [
    "\n",
    "### 2. **`src/` - 核心代码模块**\n",
    "```json\n",
    "src/\n",
    "├── train_reward_model.py     # 主训练脚本 \n",
    "└── data/\n",
    "    └── dataset.py           # 数据处理模块\n",
    "```\n",
    "\n",
    "**核心功能**：\n",
    "- **`train_reward_model.py`**：整个项目的训练核心\n",
    "  - DeepSpeed分布式训练\n",
    "  - Freeze tuning（只训练最后4层）\n",
    "  - 自定义RewardTrainer类\n",
    "  \n",
    "- **`dataset.py`**：数据处理引擎\n",
    "  - 加载preference数据（question/chosen/rejected格式）\n",
    "  - LLaMA-3对话格式转换\n",
    "  - 自定义PairwiseDataCollator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add30d2",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **`scripts/` - 工具脚本集合**\n",
    "```json\n",
    "scripts/\n",
    "├── setup/                    # 环境配置工具\n",
    "│   ├── check_data.py        # 数据完整性检查\n",
    "│   └── test_deepspeed.py    # DeepSpeed功能测试\n",
    "└── training/                # 训练启动工具\n",
    "    ├── deepspeed_cluster_launcher.sh  # 多机分布式启动脚本 \n",
    "    └── run_training.sh              # 单机训练脚本\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1f5c3",
   "metadata": {},
   "source": [
    "\n",
    "**关键脚本**：\n",
    "- **`deepspeed_cluster_launcher.sh`**：实际使用的分布式训练启动器\n",
    "  - 管理10.60.11.131(4GPU) + 10.60.240.249(2GPU)集群\n",
    "  - 自动创建hostfile配置\n",
    "  - 支持start/stop/logs/resume操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f271f8",
   "metadata": {},
   "source": [
    "\n",
    "### 4. **`inference/` - 推理部署模块**\n",
    "```json\n",
    "inference/\n",
    "├── ray_reward_service.py     # Ray分布式推理服务 \n",
    "├── test_ray_service.py       # 模型验证测试工具\n",
    "├── start_ray_cluster.sh      # Ray集群启动脚本\n",
    "└── debug_response.py         # 调试工具\n",
    "```\n",
    "\n",
    "**部署架构**：\n",
    "- **Ray Serve**：分布式推理框架，支持多GPU负载均衡\n",
    "- **验证流程**：加载测试数据 → 并发推理 → 计算准确率\n",
    "- **健康检查**：提供服务状态监控\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670616e",
   "metadata": {},
   "source": [
    "\n",
    "##  **完整工作流程**\n",
    "\n",
    "### **训练阶段**\n",
    "```bash\n",
    "# 1. 启动分布式训练\n",
    "bash scripts/training/deepspeed_cluster_launcher.sh start\n",
    "\n",
    "# 2. 监控训练日志  \n",
    "bash scripts/training/deepspeed_cluster_launcher.sh logs\n",
    "\n",
    "# 3. 从checkpoint恢复\n",
    "bash scripts/training/deepspeed_cluster_launcher.sh resume output/checkpoint-700\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3327e3",
   "metadata": {},
   "source": [
    "数据结构\n",
    "```json\n",
    "output_dir/\n",
    "├── train/\n",
    "│   ├── qa_dataset.jsonl          # 问题-答案格式\n",
    "│   └── preference_dataset.jsonl  # 偏好对格式\n",
    "├── eval/\n",
    "│   ├── qa_dataset.jsonl\n",
    "│   └── preference_dataset.jsonl\n",
    "└── test/\n",
    "    ├── qa_dataset.jsonl\n",
    "    └── preference_dataset.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731d68c",
   "metadata": {},
   "source": [
    "dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4b10c",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "================================================================================\n",
    " 文件作用说明：\n",
    "这是奖励模型的数据处理核心模块，负责preference数据的加载和处理\n",
    "\n",
    " 项目中的整体作用：\n",
    "1. 【数据加载】：加载包含chosen/rejected回答对的preference数据\n",
    "2. 【格式转换】：将原始数据转换为模型可以理解的格式\n",
    "3. 【分词处理】：使用tokenizer将文本转换为token序列\n",
    "4. 【批次整理】：提供数据整理器，将多个样本组织成训练批次\n",
    "5. 【LLaMA格式】：按照LLaMA-3的对话格式处理数据\n",
    "\n",
    " 数据流程：\n",
    "原始JSON → 加载数据 → 格式化对话 → 分词 → 创建数据集 → 批次整理 → 训练\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Any\n",
    "from transformers import PreTrainedTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  金融奖励数据集类：核心的数据处理类\n",
    "# \n",
    "#  这个类在项目中的作用：\n",
    "# 1. 【数据核心】：整个奖励模型训练的数据处理核心\n",
    "# 2. 【格式统一】：将不同格式的preference数据统一处理\n",
    "# 3. 【LLaMA适配】：专门适配LLaMA-3模型的对话格式\n",
    "# 4. 【内存高效】：采用懒加载方式，节省内存使用\n",
    "# ============================================================================\n",
    "class FinancialRewardDataset:\n",
    "    \"\"\"\n",
    "    金融奖励数据集处理类\n",
    "    \n",
    "    这个类负责处理包含chosen/rejected回答对的数据：\n",
    "    - 加载JSONL格式的preference数据\n",
    "    - 转换为LLaMA-3的对话格式\n",
    "    - 提供给训练器使用的标准数据集\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,                    # 数据文件路径\n",
    "        tokenizer: PreTrainedTokenizer,    # 分词器\n",
    "        max_length: int = 2048,            # 最大序列长度\n",
    "        split: str = \"train\"               # 数据集分割（train/eval）\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        \n",
    "        初始化过程：\n",
    "        1. 保存配置参数\n",
    "        2. 确定数据文件路径\n",
    "        3. 加载原始数据到内存\n",
    "        4. 打印数据集统计信息\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer      # 保存分词器引用\n",
    "        self.max_length = max_length    # 保存最大长度设置\n",
    "        self.split = split             # 保存数据集类型\n",
    "        \n",
    "        # 根据split参数确定数据文件路径\n",
    "        if split == \"train\":\n",
    "            data_file = os.path.join(data_path, \"train\", \"preference_dataset.jsonl\")\n",
    "        else:\n",
    "            data_file = os.path.join(data_path, \"eval\", \"preference_dataset.jsonl\")\n",
    "        \n",
    "        # 加载原始数据：读取JSONL文件，每行是一个JSON对象\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            self.raw_data = [json.loads(line) for line in f]\n",
    "        \n",
    "        print(f\"{split.upper()} 数据集大小: {len(self.raw_data)}\")\n",
    "    \n",
    "    def preprocess_dataset(self) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        预处理数据集：将原始数据转换为模型训练格式\n",
    "        \n",
    "        这个方法的作用：\n",
    "        1. 【格式转换】：将question+chosen/rejected转换为完整对话\n",
    "        2. 【LLaMA格式】：使用LLaMA-3的特殊token格式化对话\n",
    "        3. 【分词处理】：将文本转换为token ID序列\n",
    "        4. 【数据组织】：组织成chosen/rejected对的形式供训练使用\n",
    "        \n",
    "        数据格式转换：\n",
    "        输入：{\"question\": \"...\", \"chosen\": \"...\", \"rejected\": \"...\"}\n",
    "        输出：{\"chosen_input_ids\": [...], \"rejected_input_ids\": [...], ...}\n",
    "        \"\"\"\n",
    "        model_inputs = defaultdict(list)  # 用于收集处理后的数据\n",
    "        \n",
    "        # 遍历所有原始数据项\n",
    "        for item in self.raw_data:\n",
    "            # 提取数据的三个核心部分\n",
    "            question = item[\"question\"]    # 用户问题\n",
    "            chosen = item[\"chosen\"]        # 人类偏好的回答（好回答）\n",
    "            rejected = item[\"rejected\"]    # 人类不偏好的回答（差回答）\n",
    "            \n",
    "            # 构建LLaMA-3格式的对话prompt\n",
    "            # 这个格式包含特殊的对话控制token\n",
    "            prompt_text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            \n",
    "            # 处理chosen回答：prompt + chosen回答 + 结束token\n",
    "            chosen_full = prompt_text + chosen + \"<|eot_id|>\"\n",
    "            chosen_inputs = self.tokenizer(\n",
    "                chosen_full,\n",
    "                truncation=True,              # 如果超长则截断\n",
    "                max_length=self.max_length,   # 最大长度限制\n",
    "                return_tensors=None           # 返回Python list而不是tensor\n",
    "            )\n",
    "            \n",
    "            # 处理rejected回答：prompt + rejected回答 + 结束token\n",
    "            rejected_full = prompt_text + rejected + \"<|eot_id|>\"\n",
    "            rejected_inputs = self.tokenizer(\n",
    "                rejected_full,\n",
    "                truncation=True,              # 如果超长则截断\n",
    "                max_length=self.max_length,   # 最大长度限制\n",
    "                return_tensors=None           # 返回Python list而不是tensor\n",
    "            )\n",
    "            \n",
    "            # 将处理后的数据添加到结果中\n",
    "            model_inputs[\"chosen_input_ids\"].append(chosen_inputs[\"input_ids\"])\n",
    "            model_inputs[\"chosen_attention_mask\"].append(chosen_inputs[\"attention_mask\"])\n",
    "            model_inputs[\"rejected_input_ids\"].append(rejected_inputs[\"input_ids\"])\n",
    "            model_inputs[\"rejected_attention_mask\"].append(rejected_inputs[\"attention_mask\"])\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    def to_dataset(self) -> Dataset:\n",
    "        \"\"\"\n",
    "        转换为HuggingFace Dataset格式\n",
    "        \n",
    "        这个方法的作用：\n",
    "        1. 【标准化】：转换为HuggingFace生态系统的标准数据格式\n",
    "        2. 【兼容性】：确保与Transformers库的Trainer完全兼容\n",
    "        3. 【性能优化】：利用HuggingFace Dataset的优化功能\n",
    "        \n",
    "        注意：这个方法在项目中被train_reward_model.py调用\n",
    "        \"\"\"\n",
    "        processed_data = self.preprocess_dataset()  # 获取预处理后的数据\n",
    "        return Dataset.from_dict(processed_data)    # 转换为HuggingFace Dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  创建数据集的便捷函数：项目的对外接口\n",
    "# \n",
    "#  这个函数在项目中的作用：\n",
    "# 1. 【对外接口】：这是train_reward_model.py实际调用的函数\n",
    "# 2. 【同时创建】：一次性创建训练集和验证集\n",
    "# 3. 【简化调用】：简化数据集创建的复杂度\n",
    "# 4. 【返回标准】：返回标准的HuggingFace Dataset对象\n",
    "# ============================================================================\n",
    "def create_reward_dataset(data_path: str, tokenizer: PreTrainedTokenizer, max_length: int = 2048):\n",
    "    \"\"\"\n",
    "    创建训练和评估数据集的主要接口\n",
    "    \n",
    "    这个函数是项目中train_reward_model.py调用的主要数据接口：\n",
    "    1. 创建训练数据集\n",
    "    2. 创建验证数据集\n",
    "    3. 返回两个数据集供训练使用\n",
    "    \n",
    "    参数说明：\n",
    "    - data_path: 数据根目录路径\n",
    "    - tokenizer: 用于分词的tokenizer\n",
    "    - max_length: 序列的最大长度\n",
    "    \n",
    "    返回：\n",
    "    - train_dataset: 训练数据集\n",
    "    - eval_dataset: 验证数据集\n",
    "    \"\"\"\n",
    "    # 创建训练数据集\n",
    "    train_data = FinancialRewardDataset(data_path, tokenizer, max_length, \"train\")\n",
    "    \n",
    "    # 创建验证数据集\n",
    "    eval_data = FinancialRewardDataset(data_path, tokenizer, max_length, \"eval\")\n",
    "    \n",
    "    # 转换为HuggingFace Dataset格式并返回\n",
    "    return train_data.to_dataset(), eval_data.to_dataset()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  Pairwise数据整理器：专门处理chosen/rejected数据对的批次整理\n",
    "# \n",
    "#  这个类在项目中的作用：\n",
    "# 1. 【批次处理】：将多个数据样本组织成训练批次\n",
    "# 2. 【长度对齐】：通过padding将不同长度的序列对齐\n",
    "# 3. 【内存优化】：高效地组织数据，减少内存浪费\n",
    "# 4. 【训练配合】：与SafeRewardTrainer的compute_loss方法完美配合\n",
    "# ============================================================================\n",
    "class PairwiseDataCollator:\n",
    "    \"\"\"\n",
    "    Pairwise数据整理器 - 专门处理奖励模型的chosen/rejected数据\n",
    "    \n",
    "    这个类的核心功能：\n",
    "    - 将一个batch中的所有chosen回答padding到相同长度\n",
    "    - 将一个batch中的所有rejected回答padding到相同长度\n",
    "    - 组织数据格式供SafeRewardTrainer使用\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, padding: str = \"longest\"):\n",
    "        \"\"\"\n",
    "        初始化数据整理器\n",
    "        \n",
    "        参数说明：\n",
    "        - tokenizer: 用于padding的分词器\n",
    "        - padding: padding策略，\"longest\"表示padding到batch中最长的序列\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer    # 保存分词器引用\n",
    "        self.padding = padding        # 保存padding策略\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        整理一个batch的pairwise数据\n",
    "        \n",
    "        这个方法的作用：\n",
    "        1. 【数据分离】：将chosen和rejected数据分别处理\n",
    "        2. 【批次padding】：对每组数据分别进行padding\n",
    "        3. 【格式组织】：组织成训练器期望的格式\n",
    "        \n",
    "        处理流程：\n",
    "        输入：[{chosen_input_ids: [...], rejected_input_ids: [...]}, ...]\n",
    "        输出：{chosen_input_ids: tensor, rejected_input_ids: tensor, ...}\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # 第1步：分离chosen和rejected数据\n",
    "        chosen_features = []    # 收集所有chosen回答的数据\n",
    "        rejected_features = []  # 收集所有rejected回答的数据\n",
    "        \n",
    "        for feature in features:\n",
    "            # 提取chosen回答的数据\n",
    "            chosen_features.append({\n",
    "                \"input_ids\": feature[\"chosen_input_ids\"],\n",
    "                \"attention_mask\": feature[\"chosen_attention_mask\"]\n",
    "            })\n",
    "            # 提取rejected回答的数据\n",
    "            rejected_features.append({\n",
    "                \"input_ids\": feature[\"rejected_input_ids\"], \n",
    "                \"attention_mask\": feature[\"rejected_attention_mask\"]\n",
    "            })\n",
    "        \n",
    "        # 第2步：分别对chosen和rejected数据进行padding\n",
    "        chosen_batch = self.tokenizer.pad(\n",
    "            chosen_features,\n",
    "            padding=self.padding,     # 使用指定的padding策略\n",
    "            return_tensors=\"pt\"       # 返回PyTorch tensor\n",
    "        )\n",
    "        \n",
    "        rejected_batch = self.tokenizer.pad(\n",
    "            rejected_features,\n",
    "            padding=self.padding,     # 使用指定的padding策略\n",
    "            return_tensors=\"pt\"       # 返回PyTorch tensor\n",
    "        )\n",
    "        \n",
    "        # 第3步：组合为最终的batch格式\n",
    "        # 这个格式正好是SafeRewardTrainer.compute_loss方法期望的输入\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_batch[\"input_ids\"],           # chosen回答的token IDs\n",
    "            \"chosen_attention_mask\": chosen_batch[\"attention_mask\"], # chosen回答的注意力掩码\n",
    "            \"rejected_input_ids\": rejected_batch[\"input_ids\"],       # rejected回答的token IDs\n",
    "            \"rejected_attention_mask\": rejected_batch[\"attention_mask\"], # rejected回答的注意力掩码\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "#  创建数据整理器的便捷函数：项目的标准接口\n",
    "# \n",
    "#  这个函数在项目中的作用：\n",
    "# 1. 【标准接口】：这是train_reward_model.py调用的标准接口\n",
    "# 2. 【简化创建】：简化PairwiseDataCollator的创建过程\n",
    "# 3. 【配置统一】：使用统一的默认配置\n",
    "# \n",
    "# ⚠️ 注意：这个函数在train_reward_model.py中被调用\n",
    "# ============================================================================\n",
    "def create_data_collator(tokenizer: PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    创建pairwise数据整理器的便捷函数\n",
    "    \n",
    "    这个函数是train_reward_model.py中调用的标准接口：\n",
    "    - 创建并返回配置好的PairwiseDataCollator\n",
    "    - 使用\"longest\"padding策略（padding到batch中最长序列的长度）\n",
    "    \n",
    "    参数说明：\n",
    "    - tokenizer: 用于padding的分词器\n",
    "    \n",
    "    返回：\n",
    "    - PairwiseDataCollator实例\n",
    "    \"\"\"\n",
    "    return PairwiseDataCollator(tokenizer, padding=\"longest\") \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77990f",
   "metadata": {},
   "source": [
    "train_reward_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af63ea",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Skywork奖励模型微调脚本\n",
    "基于LlamaForSequenceClassification架构\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import deepspeed\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback, set_seed\n",
    ")\n",
    "\n",
    "# 解决PyTorch weights_only问题的补丁\n",
    "# 这是为了兼容不同版本的PyTorch，避免加载checkpoint时出现权限错误\n",
    "original_torch_load = torch.load\n",
    "def patched_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):\n",
    "    # 如果是加载checkpoint相关文件，强制设置weights_only=False\n",
    "    # 这样可以避免某些版本的PyTorch过于严格的安全检查\n",
    "    if weights_only is True and (\n",
    "        isinstance(f, str) and ('rng_state' in f or 'checkpoint' in f)\n",
    "    ):\n",
    "        weights_only = False\n",
    "    return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, \n",
    "                              weights_only=weights_only, **kwargs)\n",
    "\n",
    "torch.load = patched_torch_load\n",
    "\n",
    "# 添加项目路径到Python搜索路径\n",
    "# 这样可以导入项目中的自定义模块\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# 导入项目自定义模块\n",
    "from data.dataset import create_reward_dataset, create_data_collator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FinetuningArguments:\n",
    "    \"\"\"\n",
    "    微调参数配置类\n",
    "    \n",
    "    这个类定义了freeze tuning的相关参数\n",
    "    freeze tuning是一种只训练模型部分层的技术，可以节省显存和计算资源\n",
    "    \"\"\"\n",
    "    freeze_trainable_layers: int = field(default=4)        # 可训练的层数（从后往前数）\n",
    "    freeze_trainable_modules: str = field(default=\"all\")   # 可训练的模块类型\n",
    "    freeze_extra_modules: Optional[str] = field(default=None)  # 额外的可训练模块\n",
    "\n",
    "\n",
    "def setup_freeze_tuning(model, finetuning_args: FinetuningArguments):\n",
    "    \"\"\"\n",
    "    设置freeze tuning参数\n",
    "    \n",
    "    这个函数实现部分参数微调，只训练模型的最后几层和分类头\n",
    "    这样可以在保持模型大部分知识的同时，用较少的计算资源进行微调\n",
    "    \n",
    "    参数:\n",
    "        model: 要设置的模型\n",
    "        finetuning_args: 微调参数配置\n",
    "    \n",
    "    工作原理:\n",
    "        1. 获取模型总层数\n",
    "        2. 确定哪些层需要训练（通常是最后几层）\n",
    "        3. 冻结其他层的参数，只训练指定层\n",
    "        4. 统计可训练参数数量\n",
    "    \"\"\"\n",
    "    print(\"配置Freeze Tuning...\")\n",
    "    \n",
    "    # 获取模型的总层数\n",
    "    # 对于LLaMA模型，这通常是32层\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    \n",
    "    # 确定可训练层的范围\n",
    "    # 例如：如果总共32层，freeze_trainable_layers=4，则训练第28-31层（最后4层）\n",
    "    trainable_layer_ids = range(\n",
    "        max(0, num_layers - finetuning_args.freeze_trainable_layers), \n",
    "        num_layers\n",
    "    )\n",
    "    \n",
    "    # 构建可训练参数的匹配模式\n",
    "    # 这些模式用于识别哪些参数需要训练\n",
    "    trainable_patterns = []\n",
    "    for idx in trainable_layer_ids:\n",
    "        if finetuning_args.freeze_trainable_modules == \"all\":\n",
    "            # 如果是\"all\"，则该层的所有模块都可训练\n",
    "            trainable_patterns.append(f\".{idx}.\")\n",
    "        else:\n",
    "            # 否则只训练指定的模块（如attention、mlp等）\n",
    "            modules = [m.strip() for m in finetuning_args.freeze_trainable_modules.split(\",\")]\n",
    "            for module in modules:\n",
    "                trainable_patterns.append(f\".{idx}.{module}\")\n",
    "    \n",
    "    # 添加额外的可训练模块\n",
    "    # 例如分类头（score层）通常总是需要训练的\n",
    "    if finetuning_args.freeze_extra_modules:\n",
    "        extra_modules = [m.strip() for m in finetuning_args.freeze_extra_modules.split(\",\")]\n",
    "        trainable_patterns.extend(extra_modules)\n",
    "    \n",
    "    # 遍历模型的所有参数，设置是否可训练\n",
    "    trainable_params = 0  # 可训练参数数量\n",
    "    total_params = 0      # 总参数数量\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        # 检查参数名是否匹配可训练模式\n",
    "        is_trainable = any(pattern in name for pattern in trainable_patterns)\n",
    "        param.requires_grad = is_trainable  # 设置是否计算梯度\n",
    "        if is_trainable:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    # 打印配置信息\n",
    "    print(f\"模型层数: {num_layers}\")\n",
    "    print(f\"可训练层: {list(trainable_layer_ids)}\")\n",
    "    print(f\"可训练参数: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    奖励模型训练器\n",
    "    \n",
    "    这个类继承自HuggingFace的Trainer，专门用于训练奖励模型\n",
    "    重写了损失计算和评估方法，实现了pairwise对比学习\n",
    "    \n",
    "    奖励模型的核心思想：\n",
    "    - 输入两个回答（chosen和rejected）\n",
    "    - 模型分别给出分数\n",
    "    - 训练目标是让chosen的分数高于rejected的分数\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        计算奖励模型的对比损失\n",
    "        \n",
    "        这是奖励模型训练的核心方法，实现了pairwise对比学习\n",
    "        \n",
    "        参数:\n",
    "            model: 奖励模型\n",
    "            inputs: 输入数据，包含chosen和rejected的token序列\n",
    "            return_outputs: 是否返回模型输出\n",
    "            num_items_in_batch: 批次中的样本数量\n",
    "        \n",
    "        返回:\n",
    "            损失值或(损失值, 模型输出)\n",
    "        \n",
    "        工作原理:\n",
    "            1. 分别对chosen和rejected回答进行推理\n",
    "            2. 获取两个回答的分数\n",
    "            3. 计算对比损失：-log(sigmoid(chosen_score - rejected_score))\n",
    "            4. 这个损失函数鼓励chosen分数高于rejected分数\n",
    "        \"\"\"\n",
    "        # 对chosen回答进行推理\n",
    "        # chosen回答是人类标注的更好的回答\n",
    "        chosen_outputs = model(\n",
    "            input_ids=inputs[\"chosen_input_ids\"],\n",
    "            attention_mask=inputs[\"chosen_attention_mask\"]\n",
    "        )\n",
    "        # 提取chosen回答的分数\n",
    "        # squeeze(-1)是为了去掉最后一个维度，得到标量分数\n",
    "        chosen_rewards = chosen_outputs.logits.squeeze(-1)\n",
    "        \n",
    "        # 对rejected回答进行推理\n",
    "        # rejected回答是人类标注的较差的回答\n",
    "        rejected_outputs = model(\n",
    "            input_ids=inputs[\"rejected_input_ids\"],\n",
    "            attention_mask=inputs[\"rejected_attention_mask\"]\n",
    "        )\n",
    "        # 提取rejected回答的分数\n",
    "        rejected_rewards = rejected_outputs.logits.squeeze(-1)\n",
    "        \n",
    "        # 计算对比损失\n",
    "        # logsigmoid(chosen - rejected)鼓励chosen分数高于rejected分数\n",
    "        # 负号是因为我们要最小化损失（最大化chosen相对于rejected的优势）\n",
    "        loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "        \n",
    "        if return_outputs:\n",
    "            return loss, {\"chosen_rewards\": chosen_rewards, \"rejected_rewards\": rejected_rewards}\n",
    "        return loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        重写预测步骤，正确处理自定义数据格式\n",
    "        \n",
    "        这个方法在评估时被调用，用于计算验证集上的损失和预测结果\n",
    "        \n",
    "        参数:\n",
    "            model: 模型\n",
    "            inputs: 输入数据\n",
    "            prediction_loss_only: 是否只返回损失\n",
    "            ignore_keys: 忽略的键\n",
    "        \n",
    "        返回:\n",
    "            (损失, 预测结果, 标签)\n",
    "        \"\"\"\n",
    "        model.eval()  # 设置为评估模式\n",
    "        \n",
    "        with torch.no_grad():  # 不计算梯度，节省内存\n",
    "            # 使用自定义的compute_loss方法计算损失\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "            \n",
    "            # 如果只需要损失，直接返回\n",
    "            if prediction_loss_only:\n",
    "                return (loss, None, None)\n",
    "            \n",
    "            # 计算chosen和rejected的奖励分数\n",
    "            chosen_outputs = model(\n",
    "                input_ids=inputs[\"chosen_input_ids\"],\n",
    "                attention_mask=inputs[\"chosen_attention_mask\"]\n",
    "            )\n",
    "            rejected_outputs = model(\n",
    "                input_ids=inputs[\"rejected_input_ids\"],\n",
    "                attention_mask=inputs[\"rejected_attention_mask\"]\n",
    "            )\n",
    "                \n",
    "            chosen_rewards = chosen_outputs.logits.squeeze(-1)\n",
    "            rejected_rewards = rejected_outputs.logits.squeeze(-1)\n",
    "            \n",
    "            # 创建预测结果：chosen > rejected 为正确预测\n",
    "            # 这里将布尔值转换为浮点数（True->1.0, False->0.0）\n",
    "            predictions = (chosen_rewards > rejected_rewards).float()\n",
    "            \n",
    "            # 创建标签：全部为1（chosen应该总是比rejected好）\n",
    "            # 在奖励模型中，理想情况下chosen总是应该得到更高分数\n",
    "            labels = torch.ones_like(predictions)\n",
    "            \n",
    "            return (loss, predictions, labels)\n",
    "    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"\n",
    "        重写评估方法，计算奖励模型的准确率\n",
    "        \n",
    "        这个方法在验证时被调用，计算模型在验证集上的表现\n",
    "        \n",
    "        参数:\n",
    "            eval_dataset: 评估数据集\n",
    "            ignore_keys: 忽略的键\n",
    "            metric_key_prefix: 指标前缀\n",
    "        \n",
    "        返回:\n",
    "            评估结果字典\n",
    "        \n",
    "        准确率计算：\n",
    "            准确率 = 正确预测数量 / 总预测数量\n",
    "            正确预测 = chosen_score > rejected_score\n",
    "        \"\"\"\n",
    "        # 调用父类评估方法，获取基本的评估结果\n",
    "        eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # 手动计算准确率\n",
    "        if eval_dataset is not None:\n",
    "            correct = 0  # 正确预测的数量\n",
    "            total = 0    # 总预测数量\n",
    "            \n",
    "            self.model.eval()  # 设置为评估模式\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            \n",
    "            with torch.no_grad():  # 不计算梯度\n",
    "                for batch in eval_dataloader:\n",
    "                    # 将数据移动到GPU\n",
    "                    batch = {k: v.to(self.args.device) for k, v in batch.items()}\n",
    "                    \n",
    "                    # 分别对chosen和rejected进行推理\n",
    "                    chosen_outputs = self.model(\n",
    "                        input_ids=batch[\"chosen_input_ids\"],\n",
    "                        attention_mask=batch[\"chosen_attention_mask\"]\n",
    "                    )\n",
    "                    rejected_outputs = self.model(\n",
    "                        input_ids=batch[\"rejected_input_ids\"],\n",
    "                        attention_mask=batch[\"rejected_attention_mask\"]\n",
    "                    )\n",
    "                    \n",
    "                    chosen_rewards = chosen_outputs.logits.squeeze(-1)\n",
    "                    rejected_rewards = rejected_outputs.logits.squeeze(-1)\n",
    "                    \n",
    "                    # 统计正确预测的数量\n",
    "                    # 如果chosen分数高于rejected分数，则预测正确\n",
    "                    correct += (chosen_rewards > rejected_rewards).sum().item()\n",
    "                    total += chosen_rewards.size(0)\n",
    "            \n",
    "            # 计算准确率\n",
    "            accuracy = correct / total if total > 0 else 0.0\n",
    "            eval_results[f\"{metric_key_prefix}_accuracy\"] = accuracy\n",
    "            \n",
    "            # 打印评估结果\n",
    "            print(f\"评估结果 - Loss: {eval_results[f'{metric_key_prefix}_loss']:.6f}, \"\n",
    "                  f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        \n",
    "        return eval_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主训练函数\n",
    "    \n",
    "    这是整个训练脚本的入口点，负责：\n",
    "    1. 解析命令行参数\n",
    "    2. 初始化分布式训练环境\n",
    "    3. 加载模型和数据\n",
    "    4. 配置训练参数\n",
    "    5. 开始训练\n",
    "    6. 保存最终模型\n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser(description=\"Skywork奖励模型微调\")\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None,\n",
    "                       help=\"从checkpoint恢复训练\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/training/config.json\",\n",
    "                       help=\"配置文件路径\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                       help=\"DeepSpeed本地rank\")\n",
    "    parser.add_argument(\"--deepspeed\", type=str, default=None,\n",
    "                       help=\"DeepSpeed配置文件\")\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 初始化DeepSpeed分布式训练环境\n",
    "    # DeepSpeed是微软开发的深度学习优化库，支持大模型训练\n",
    "    deepspeed.init_distributed()\n",
    "    \n",
    "    # 获取分布式训练的相关信息\n",
    "    # 这些环境变量由DeepSpeed或其他分布式训练框架设置\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', args.local_rank if args.local_rank != -1 else 0))\n",
    "    world_size = int(os.environ.get('WORLD_SIZE', 1))  # 总进程数\n",
    "    rank = int(os.environ.get('RANK', 0))              # 当前进程的全局rank\n",
    "    \n",
    "    print(f\"启动进程 {rank}/{world_size} (GPU {local_rank})\")\n",
    "    \n",
    "    # 如果指定了checkpoint，打印恢复信息\n",
    "    if args.resume_from_checkpoint:\n",
    "        print(f\"从checkpoint恢复训练: {args.resume_from_checkpoint}\")\n",
    "    \n",
    "    # 设置当前进程使用的GPU\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    # 加载训练配置文件\n",
    "    with open(args.config, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 创建输出目录（只在主进程中创建，避免竞争条件）\n",
    "    if rank == 0:\n",
    "        os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "        os.makedirs(config[\"logging_dir\"], exist_ok=True)\n",
    "    \n",
    "    # 设置随机种子，确保实验可重现\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    # 创建微调参数配置\n",
    "    finetuning_args = FinetuningArguments(\n",
    "        freeze_trainable_layers=config[\"freeze_trainable_layers\"],\n",
    "        freeze_trainable_modules=config[\"freeze_trainable_modules\"],\n",
    "        freeze_extra_modules=config.get(\"freeze_extra_modules\")\n",
    "    )\n",
    "    \n",
    "    # 打印配置信息（只在主进程中打印）\n",
    "    if rank == 0:\n",
    "        print(f\"基础模型: {config['model_name_or_path']}\")\n",
    "        print(f\"Freeze层数: {finetuning_args.freeze_trainable_layers}\")\n",
    "        print(f\"输出目录: {config['output_dir']}\")\n",
    "    \n",
    "    # 加载tokenizer（文本分词器）\n",
    "    # tokenizer负责将文本转换为模型可以理解的数字序列\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name_or_path\"], \n",
    "        trust_remote_code=config[\"trust_remote_code\"]\n",
    "    )\n",
    "    # 如果没有pad_token，使用eos_token作为pad_token\n",
    "    # pad_token用于将不同长度的序列填充到相同长度\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 加载模型配置\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        config[\"model_name_or_path\"], \n",
    "        trust_remote_code=config[\"trust_remote_code\"]\n",
    "    )\n",
    "    # 设置输出标签数为1，因为奖励模型输出单个分数\n",
    "    model_config.num_labels = 1\n",
    "    \n",
    "    # 在CPU上初始化模型\n",
    "    # 这是为了节省GPU内存，DeepSpeed会自动将模型移动到GPU\n",
    "    with torch.device('cpu'):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config[\"model_name_or_path\"],\n",
    "            config=model_config,\n",
    "            torch_dtype=getattr(torch, config[\"torch_dtype\"]),\n",
    "            trust_remote_code=config[\"trust_remote_code\"]\n",
    "        )\n",
    "    \n",
    "    # 设置freeze tuning，只训练部分参数\n",
    "    setup_freeze_tuning(model, finetuning_args)\n",
    "    \n",
    "    # 创建数据集\n",
    "    # 这会加载训练数据和验证数据，并进行预处理\n",
    "    train_dataset, eval_dataset = create_reward_dataset(\n",
    "        config[\"data_path\"], tokenizer, config[\"max_length\"]\n",
    "    )\n",
    "    # 创建数据整理器，负责将数据组织成批次\n",
    "    data_collator = create_data_collator(tokenizer)\n",
    "    \n",
    "    # 打印数据集信息\n",
    "    if rank == 0:\n",
    "        print(f\"数据集 - 训练: {len(train_dataset)}, 验证: {len(eval_dataset)}\")\n",
    "    \n",
    "    # 创建训练参数配置\n",
    "    training_args = TrainingArguments(\n",
    "        # 基本训练参数\n",
    "        output_dir=config[\"output_dir\"],                    # 输出目录\n",
    "        num_train_epochs=config[\"num_train_epochs\"],        # 训练轮数\n",
    "        learning_rate=config[\"learning_rate\"],              # 学习率\n",
    "        weight_decay=config[\"weight_decay\"],                # 权重衰减（正则化）\n",
    "        warmup_ratio=config[\"warmup_ratio\"],                # 学习率预热比例\n",
    "        max_grad_norm=config[\"max_grad_norm\"],              # 梯度裁剪阈值\n",
    "        seed=config[\"seed\"],                                # 随机种子\n",
    "        \n",
    "        # 批次大小设置\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],    # 每个设备的训练批次大小\n",
    "        per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],      # 每个设备的评估批次大小\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],    # 梯度累积步数\n",
    "        \n",
    "        # 评估策略\n",
    "        eval_strategy=config[\"evaluation_strategy\"],        # 评估策略（按步数评估）\n",
    "        eval_steps=config[\"eval_steps\"],                    # 评估间隔步数\n",
    "        do_eval=True,                                       # 启用评估\n",
    "        \n",
    "        # 保存策略\n",
    "        save_strategy=config[\"save_strategy\"],              # 保存策略（按步数保存）\n",
    "        save_steps=config[\"save_steps\"],                    # 保存间隔步数\n",
    "        save_total_limit=config[\"save_total_limit\"],        # 最多保存的checkpoint数量\n",
    "        save_safetensors=True,                              # 使用safetensors格式保存\n",
    "        \n",
    "        # 模型选择策略\n",
    "        load_best_model_at_end=config[\"load_best_model_at_end\"],        # 训练结束时加载最佳模型\n",
    "        metric_for_best_model=config[\"metric_for_best_model\"],          # 最佳模型的评估指标\n",
    "        greater_is_better=config[\"greater_is_better\"],                  # 指标是否越大越好\n",
    "        \n",
    "        # 日志设置\n",
    "        logging_steps=config[\"logging_steps\"],              # 日志记录间隔\n",
    "        logging_dir=config[\"logging_dir\"],                  # 日志目录\n",
    "        \n",
    "        # 精度设置\n",
    "        bf16=config[\"bf16\"],                                # 使用bfloat16精度\n",
    "        fp16=config[\"fp16\"],                                # 使用float16精度\n",
    "        tf32=config[\"tf32\"],                                # 使用TensorFloat-32\n",
    "        \n",
    "        # DeepSpeed配置\n",
    "        deepspeed=\"configs/training/deepspeed_only.json\",   # DeepSpeed配置文件\n",
    "        \n",
    "        # 分布式训练设置\n",
    "        local_rank=local_rank,                              # 本地rank\n",
    "        ddp_find_unused_parameters=False,                   # 不查找未使用的参数\n",
    "        dataloader_num_workers=0,                           # 数据加载器工作进程数\n",
    "        remove_unused_columns=False,                        # 不移除未使用的列（重要：保留自定义数据格式）\n",
    "        report_to=[],                                       # 不上报到外部服务\n",
    "    )\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,                                        # 要训练的模型\n",
    "        args=training_args,                                 # 训练参数\n",
    "        train_dataset=train_dataset,                        # 训练数据集\n",
    "        eval_dataset=eval_dataset,                          # 验证数据集\n",
    "        data_collator=data_collator,                        # 数据整理器\n",
    "        processing_class=tokenizer,                         # 处理类（tokenizer）\n",
    "        callbacks=[\n",
    "            # 早停回调：如果验证损失不再改善，提前停止训练\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=config.get(\"early_stopping_patience\", 5),  # 容忍的评估次数\n",
    "                early_stopping_threshold=0.001                                      # 改善的最小阈值\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 开始训练\n",
    "    if rank == 0:\n",
    "        print(\"开始训练...\")\n",
    "    \n",
    "    # 执行训练过程\n",
    "    # 如果指定了checkpoint，会从该checkpoint恢复训练\n",
    "    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
    "    \n",
    "    # 训练完成后的处理（只在主进程中执行）\n",
    "    if rank == 0:\n",
    "        print(\"训练完成\")\n",
    "        \n",
    "        # 保存最终模型\n",
    "        # 这会保存完整的模型和tokenizer到final_model目录\n",
    "        final_model_path = os.path.join(config[\"output_dir\"], \"final_model\")\n",
    "        trainer.save_model(final_model_path)\n",
    "        print(f\"最终模型保存到: {final_model_path}\")\n",
    "\n",
    "\n",
    "# 如果这个文件被直接运行（而不是被导入），则执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdd1c5",
   "metadata": {},
   "source": [
    "deepspeed_cluster_launcher.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbca012",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# ================================================================================\n",
    "#  文件作用说明：\n",
    "# 这是DeepSpeed多机多卡分布式训练的主启动脚本\n",
    "# \n",
    "#  项目中的整体作用：\n",
    "# 1. 作为分布式训练的入口点，负责启动多台服务器上的训练进程\n",
    "# 2. 自动管理集群节点配置，创建DeepSpeed需要的hostfile文件\n",
    "# 3. 提供训练进程的启动、停止、日志查看等管理功能\n",
    "# 4. 这是你在多机训练时实际使用的主要脚本\n",
    "#\n",
    "#  使用方式：\n",
    "# bash deepspeed_cluster_launcher.sh start  # 启动训练\n",
    "# bash deepspeed_cluster_launcher.sh stop   # 停止训练  \n",
    "# bash deepspeed_cluster_launcher.sh logs   # 查看日志\n",
    "# ================================================================================\n",
    "\n",
    "# 修正版DeepSpeed多机多卡启动脚本\n",
    "# 解决GPU设备编号问题\n",
    "\n",
    "# ============================================================================\n",
    "#  颜色定义：用于在终端中显示不同颜色的日志信息，提高可读性\n",
    "# ============================================================================\n",
    "RED='\\033[0;31m'      # 红色：用于错误信息\n",
    "GREEN='\\033[0;32m'    # 绿色：用于成功信息  \n",
    "BLUE='\\033[0;34m'     # 蓝色：用于一般信息\n",
    "YELLOW='\\033[1;33m'   # 黄色：用于调试信息\n",
    "NC='\\033[0m'          # 无颜色：重置颜色\n",
    "\n",
    "# ============================================================================\n",
    "#  日志输出函数：统一管理日志格式，方便调试和监控\n",
    "# 这些函数在整个项目中用于：输出格式化的日志信息，便于运维人员查看\n",
    "# ============================================================================\n",
    "log_info() { echo -e \"${BLUE}[INFO]${NC} $1\"; }      # 输出蓝色的信息日志\n",
    "log_success() { echo -e \"${GREEN}[SUCCESS]${NC} $1\"; } # 输出绿色的成功日志\n",
    "log_error() { echo -e \"${RED}[ERROR]${NC} $1\"; }     # 输出红色的错误日志  \n",
    "log_debug() { echo -e \"${YELLOW}[DEBUG]${NC} $1\"; }  # 输出黄色的调试日志\n",
    "\n",
    "# ============================================================================\n",
    "#  集群节点配置：定义所有参与训练的服务器节点和GPU数量\n",
    "# \n",
    "#  项目中的整体作用：\n",
    "# 1. 这是分布式训练的核心配置，告诉DeepSpeed有哪些服务器可以用\n",
    "# 2. 每个IP对应一台服务器，数字表示该服务器有多少张GPU卡\n",
    "# 3. DeepSpeed会根据这个配置自动分配训练任务到各个GPU上\n",
    "# 4. 如果要增加/减少训练节点，只需要修改这个数组即可\n",
    "# ============================================================================\n",
    "declare -A NODES=(\n",
    "    [\"10.60.11.131\"]=4    # 主节点（服务器1）：拥有4张GPU卡\n",
    "    [\"10.60.240.249\"]=2   # 工作节点2：拥有2张GPU卡\n",
    "    # [\"10.60.79.49\"]=2    # 工作节点3：拥有2张GPU卡（已注释，表示暂时不使用）\n",
    "    # [\"10.60.11.133\"]=2    # 工作节点4：拥有2张GPU卡（已注释，表示暂时不使用）\n",
    ")\n",
    "\n",
    "# 主节点（第一个节点）：负责协调整个训练过程的服务器\n",
    "MASTER_ADDR=\"10.60.11.131\"\n",
    "\n",
    "# ============================================================================\n",
    "#  创建hostfile函数：生成DeepSpeed分布式训练需要的节点配置文件\n",
    "# \n",
    "#  项目中的整体作用：\n",
    "# 1. DeepSpeed需要一个hostfile文件来知道有哪些服务器参与训练\n",
    "# 2. 这个文件告诉DeepSpeed每台服务器的IP地址和GPU数量\n",
    "# 3. 训练开始前必须先创建这个文件，否则DeepSpeed无法启动\n",
    "# 4. 每次启动训练都会重新生成，确保配置是最新的\n",
    "# ============================================================================\n",
    "create_hostfile() {\n",
    "    log_info \"创建DeepSpeed hostfile...\"\n",
    "    \n",
    "    # 清空现有hostfile文件（如果存在的话）\n",
    "    > /shared/financial_reward_model/hostfile\n",
    "    \n",
    "    # 遍历所有配置的节点，将每个节点信息写入hostfile\n",
    "    for node_ip in \"${!NODES[@]}\"; do\n",
    "        gpu_count=${NODES[$node_ip]}  # 获取该节点的GPU数量\n",
    "        # 按DeepSpeed要求的格式写入：IP地址 slots=GPU数量\n",
    "        echo \"$node_ip slots=$gpu_count\" >> /shared/financial_reward_model/hostfile\n",
    "    done\n",
    "    \n",
    "    log_success \"Hostfile创建完成\"\n",
    "    # 显示创建的hostfile内容，便于确认配置正确\n",
    "    cat /shared/financial_reward_model/hostfile\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "#  停止训练函数：安全地停止所有服务器上正在运行的训练进程\n",
    "# \n",
    "#  项目中的整体作用：\n",
    "# 1. 在启动新训练前，先停止可能存在的旧训练进程，避免冲突\n",
    "# 2. 确保所有服务器上的训练进程都被正确清理\n",
    "# 3. 防止出现多个训练进程同时运行的问题\n",
    "# 4. 为新的训练启动做准备\n",
    "# ============================================================================\n",
    "stop_all_training() {\n",
    "    log_info \" 停止所有节点的训练进程...\"\n",
    "    \n",
    "    # 遍历所有配置的节点\n",
    "    for node_ip in \"${!NODES[@]}\"; do\n",
    "        if [[ \"$node_ip\" == \"$MASTER_ADDR\" ]]; then\n",
    "            # 如果是主节点，直接在本地执行停止命令\n",
    "            pkill -f \"deepspeed.*train_reward_model\" 2>/dev/null || true\n",
    "        else\n",
    "            # 如果是其他节点，通过SSH远程执行停止命令\n",
    "            # 使用&符号让SSH命令在后台并行执行，提高效率\n",
    "            ssh ubuntu@$node_ip \"pkill -f 'deepspeed.*train_reward_model' 2>/dev/null || true\" &\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    wait  # 等待所有SSH命令完成\n",
    "    sleep 3  # 稍等片刻，确保进程完全停止\n",
    "    log_success \"所有节点训练进程清理完成\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "#  启动训练函数：这是整个脚本的核心功能，负责启动分布式训练\n",
    "# \n",
    "#  项目中的整体作用：\n",
    "# 1. 这是分布式训练的主入口，协调整个训练流程的启动\n",
    "# 2. 按顺序执行：停止旧进程 → 创建配置文件 → 激活环境 → 启动训练\n",
    "# 3. 负责管理训练日志的保存和进程监控\n",
    "# 4. 确保训练能够正确启动并提供监控信息\n",
    "# ============================================================================\n",
    "start_training() {\n",
    "    log_info \" 启动DeepSpeed分布式训练...\"\n",
    "    \n",
    "    # 第1步：清理环境，停止可能存在的旧训练进程\n",
    "    stop_all_training\n",
    "    \n",
    "    # 第2步：创建DeepSpeed需要的hostfile配置文件\n",
    "    create_hostfile\n",
    "    \n",
    "    # 第3步：切换到项目根目录\n",
    "    cd /shared/financial_reward_model\n",
    "    \n",
    "    # 第4步：创建日志存储目录\n",
    "    mkdir -p logs/distributed\n",
    "    TIMESTAMP=$(date +%Y%m%d_%H%M%S)  # 生成时间戳，用于日志文件命名\n",
    "    \n",
    "    # 第5步：激活conda虚拟环境（包含所需的Python包）\n",
    "    log_info \"激活conda环境...\"\n",
    "    source ~/miniconda3/etc/profile.d/conda.sh\n",
    "    conda activate reward2  # 激活名为\"reward\"的conda环境\n",
    "    \n",
    "    log_info \"使用修正的配置启动多机训练...\"\n",
    "    \n",
    "    # 🔑 支持额外参数的启动命令\n",
    "    EXTRA_ARGS=\"$2\"  # 接收额外参数\n",
    "    if [[ -n \"$EXTRA_ARGS\" ]]; then\n",
    "        LAUNCH_CMD=\"deepspeed --hostfile=hostfile src/train_reward_model.py $EXTRA_ARGS\"\n",
    "    else\n",
    "        LAUNCH_CMD=\"deepspeed --hostfile=hostfile src/train_reward_model.py\"\n",
    "    fi\n",
    "    LOG_FILE=\"logs/distributed/deepspeed_final_${TIMESTAMP}.log\"  # 日志文件路径\n",
    "    \n",
    "    log_debug \"启动命令: $LAUNCH_CMD\"\n",
    "    log_debug \"日志文件: $LOG_FILE\"\n",
    "    \n",
    "    # 第7步：启动训练进程\n",
    "    # > $LOG_FILE 2>&1 将标准输出和错误输出都重定向到日志文件\n",
    "    # & 让训练在后台运行，不阻塞当前终端\n",
    "    $LAUNCH_CMD > $LOG_FILE 2>&1 &\n",
    "    TRAIN_PID=$!  # 获取训练进程的PID\n",
    "    \n",
    "    log_success \"训练启动完成，PID: $TRAIN_PID\"\n",
    "    \n",
    "    # 第8步：等待并检查训练是否成功启动\n",
    "    sleep 15  # 等待15秒，让训练进程有时间初始化\n",
    "    \n",
    "    # 检查训练进程是否还在运行\n",
    "    if ps -p $TRAIN_PID > /dev/null; then\n",
    "        log_success \"训练进程运行中\"\n",
    "        log_info \"日志文件: $LOG_FILE\"\n",
    "        log_info \"监控命令: tail -f -n 300 $LOG_FILE\"\n",
    "        \n",
    "        # 显示训练日志的前30行，便于快速确认启动状态\n",
    "        sleep 5\n",
    "        echo \"=== 修正版训练日志前30行 ===\"\n",
    "        head -30 $LOG_FILE\n",
    "    else\n",
    "        # 如果进程已经退出，说明启动失败\n",
    "        log_error \"训练进程已退出\"\n",
    "        echo \"=== 错误日志 ===\"\n",
    "        cat $LOG_FILE  # 显示完整的错误日志\n",
    "        return 1\n",
    "    fi\n",
    "    \n",
    "    return 0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "#  主函数：根据用户输入的参数执行相应的操作\n",
    "# \n",
    "# 项目中的整体作用：\n",
    "# 1. 这是脚本的入口点，解析用户的命令行参数\n",
    "# 2. 提供统一的界面来管理分布式训练的各种操作\n",
    "# 3. 支持start（启动）、stop（停止）、logs（查看日志）三种操作\n",
    "# 4. 让运维人员可以方便地管理整个训练流程\n",
    "# ============================================================================\n",
    "case \"$1\" in\n",
    "    \"start\")\n",
    "        start_training \"$@\"  # 传递所有参数\n",
    "        ;;\n",
    "    \"resume\")\n",
    "        #  新增resume命令\n",
    "        if [[ -z \"$2\" ]]; then\n",
    "            log_error \"请指定checkpoint路径\"\n",
    "            echo \"用法: $0 resume <checkpoint_path>\"\n",
    "            exit 1\n",
    "        fi\n",
    "        start_training \"start\" \"--resume_from_checkpoint $2\"\n",
    "        ;;\n",
    "    \"stop\")\n",
    "        # 停止训练：调用stop_all_training函数\n",
    "        stop_all_training\n",
    "        ;;\n",
    "    \"logs\")\n",
    "        # 查看日志：找到最新的日志文件并实时显示\n",
    "        LATEST_LOG=$(ls -t /shared/financial_reward_model/logs/distributed/deepspeed_final_*.log 2>/dev/null | head -1)\n",
    "        if [[ -n \"$LATEST_LOG\" ]]; then\n",
    "            tail -f -n 300 \"$LATEST_LOG\"  # 实时显示日志内容\n",
    "        else\n",
    "            # 如果没找到deepspeed_final开头的日志，尝试查找deepspeed开头的\n",
    "            LATEST_LOG=$(ls -t /shared/financial_reward_model/logs/distributed/deepspeed_*.log 2>/dev/null | head -1)\n",
    "            if [[ -n \"$LATEST_LOG\" ]]; then\n",
    "                tail -f \"$LATEST_LOG\"\n",
    "            else\n",
    "                log_error \"未找到日志文件\"\n",
    "            fi\n",
    "        fi\n",
    "        ;;\n",
    "    *)\n",
    "        echo \"修正版DeepSpeed多机多卡训练\"\n",
    "        echo \"用法: $0 {start|stop|logs|resume}\"\n",
    "        echo \"\"\n",
    "        echo \"  start  - 启动新训练\"\n",
    "        echo \"  stop   - 停止训练\"\n",
    "        echo \"  logs   - 监控日志\"\n",
    "        echo \"  resume <checkpoint> - 从checkpoint恢复训练\"\n",
    "        ;;\n",
    "esac\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7942b8",
   "metadata": {},
   "source": [
    "run_training.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d4267",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# ================================================================================\n",
    "#  文件作用说明：\n",
    "# 这是单机多卡训练的启动脚本（使用torchrun方式）\n",
    "# \n",
    "# 原因：项目使用的是DeepSpeed分布式训练，而不是torchrun方式\n",
    "# 实际使用的是：deepspeed_cluster_launcher.sh\n",
    "# \n",
    "#  原本的设计作用：\n",
    "# 1. 在单台服务器上使用4张GPU进行训练\n",
    "# 2. 使用PyTorch原生的torchrun进行分布式训练\n",
    "# 3. 提供后台训练和日志管理功能\n",
    "# \n",
    "#  保留原因：作为备用方案，如果需要单机训练时可以参考\n",
    "# ================================================================================\n",
    "\n",
    "# 4张GPU训练脚本（nohup模式）\n",
    "echo \" 开始后台训练金融奖励模型（4张4090 + DeepSpeed Stage 2）\"\n",
    "\n",
    "# ============================================================================\n",
    "#  环境变量设置：配置CUDA和训练相关的环境参数\n",
    "# \n",
    "#  这些变量的作用：\n",
    "# 1. CUDA_VISIBLE_DEVICES：告诉程序使用哪些GPU（0,1,2,3表示使用前4张卡）\n",
    "# 2. NCCL_DEBUG：启用NCCL通信库的调试信息，帮助排查分布式训练问题\n",
    "# 3. OMP_NUM_THREADS：限制OpenMP线程数，避免CPU资源竞争\n",
    "# ============================================================================\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3  # 指定使用GPU 0,1,2,3\n",
    "export NCCL_DEBUG=INFO                # 启用NCCL调试信息\n",
    "export OMP_NUM_THREADS=1              # 设置OpenMP线程数为1\n",
    "\n",
    "# ============================================================================\n",
    "#  多机多卡配置：定义分布式训练的网络参数\n",
    "# \n",
    "#  这些参数的作用：\n",
    "# 1. MASTER_ADDR：主节点的IP地址，所有节点都要连接到这个地址\n",
    "# 2. MASTER_PORT：主节点的端口号，用于节点间通信\n",
    "# 3. NNODES：总节点数（这里设为1，表示单机训练）\n",
    "# 4. NODE_RANK：当前节点的编号（从0开始）\n",
    "# 5. NPROC_PER_NODE：每个节点的进程数（等于GPU数量）\n",
    "# ============================================================================\n",
    "MASTER_ADDR=${MASTER_ADDR:-\"localhost\"}  # 默认主节点地址为localhost\n",
    "MASTER_PORT=${MASTER_PORT:-29500}        # 默认端口号为29500\n",
    "NNODES=${NNODES:-1}                      # 默认节点数为1（单机）\n",
    "NODE_RANK=${NODE_RANK:-0}                # 默认节点编号为0（主节点）\n",
    "NPROC_PER_NODE=${NPROC_PER_NODE:-4}      # 默认每节点4个进程（4张GPU）\n",
    "\n",
    "# ============================================================================\n",
    "#  项目路径设置：确定项目根目录并切换到该目录\n",
    "# \n",
    "#  这段代码的作用：\n",
    "# 1. 自动找到项目的根目录（向上查找两级目录）\n",
    "# 2. 切换到项目根目录，确保所有相对路径都是正确的\n",
    "# 3. 打印项目信息，便于确认配置\n",
    "# ============================================================================\n",
    "PROJECT_ROOT=$(cd \"$(dirname \"$0\")/../..\" && pwd)  # 获取项目根目录的绝对路径\n",
    "cd $PROJECT_ROOT  # 切换到项目根目录\n",
    "\n",
    "echo \" 项目根目录: $PROJECT_ROOT\"\n",
    "echo \" GPU配置: 4张RTX 4090\"\n",
    "echo \" 批次配置: 总批次大小 = 4 GPU × 2 batch × 4 accumulation = 32\"\n",
    "\n",
    "# ============================================================================\n",
    "#  预检查：训练开始前的环境和数据检查\n",
    "# \n",
    "#  这个检查的作用：\n",
    "# 1. 验证训练数据是否存在且格式正确\n",
    "# 2. 验证模型文件是否存在且完整\n",
    "# 3. 如果检查失败，提前终止，避免浪费计算资源\n",
    "# \n",
    "# ============================================================================\n",
    "echo \" 检查数据和模型...\"\n",
    "python scripts/setup/check_data.py  # 执行数据和模型检查脚本\n",
    "if [ $? -ne 0 ]; then              # 如果检查脚本返回非0值（表示失败）\n",
    "    echo \"❌ 数据或模型检查失败，退出训练\"\n",
    "    exit 1                          # 退出脚本\n",
    "fi\n",
    "\n",
    "echo \"✅ 所有检查通过\"\n",
    "\n",
    "# ============================================================================\n",
    "#  日志管理：创建日志目录和文件名\n",
    "# \n",
    "#  这个设置的作用：\n",
    "# 1. 为每次训练创建独特的日志文件（使用时间戳）\n",
    "# 2. 保存进程PID，便于后续管理和停止训练\n",
    "# 3. 将所有输出重定向到日志文件，便于后续分析\n",
    "# ============================================================================\n",
    "mkdir -p logs  # 创建logs目录（如果不存在）\n",
    "\n",
    "# 生成时间戳，格式：年月日_时分秒\n",
    "TIMESTAMP=$(date +\"%Y%m%d_%H%M%S\")\n",
    "LOG_FILE=\"logs/training_${TIMESTAMP}.log\"      # 日志文件路径\n",
    "PID_FILE=\"logs/training_${TIMESTAMP}.pid\"      # PID文件路径\n",
    "\n",
    "echo \" 日志文件: $LOG_FILE\"\n",
    "echo \" PID文件: $PID_FILE\"\n",
    "\n",
    "# ============================================================================\n",
    "#  启动训练：使用nohup和torchrun启动后台训练\n",
    "# \n",
    "#  这个命令的作用：\n",
    "# 1. nohup：让训练在后台运行，即使终端关闭也不会停止\n",
    "# 2. torchrun：PyTorch的分布式训练启动器\n",
    "# 3. 各种参数告诉torchrun如何分配GPU和管理进程\n",
    "# \n",
    "# ⚠️ 重要：这个方式在当前项目中未被使用，实际使用的是DeepSpeed\n",
    "# ============================================================================\n",
    "echo \" 启动后台训练...\"\n",
    "nohup torchrun \\\n",
    "    --nnodes=$NNODES \\                    # 节点数量\n",
    "    --nproc_per_node=$NPROC_PER_NODE \\    # 每节点进程数\n",
    "    --node_rank=$NODE_RANK \\              # 节点编号\n",
    "    --master_addr=$MASTER_ADDR \\          # 主节点地址\n",
    "    --master_port=$MASTER_PORT \\          # 主节点端口\n",
    "    src/train_reward_model.py \\           # 训练脚本路径\n",
    "    configs/training/config.json \\       # 配置文件路径（注意：这个配置文件不存在）\n",
    "    > $LOG_FILE 2>&1 &                   # 重定向输出到日志文件并后台运行\n",
    "\n",
    "# ============================================================================\n",
    "#  保存PID和提供监控信息\n",
    "# \n",
    "#  这部分的作用：\n",
    "# 1. 保存训练进程的PID，便于后续停止训练\n",
    "# 2. 向用户提供各种监控和管理命令\n",
    "# 3. 让用户知道如何查看训练状态和停止训练\n",
    "# ============================================================================\n",
    "TRAIN_PID=$!                    # 获取后台进程的PID\n",
    "echo $TRAIN_PID > $PID_FILE     # 将PID保存到文件\n",
    "\n",
    "echo \" 训练已在后台启动！\"\n",
    "echo \" 进程ID: $TRAIN_PID\"\n",
    "echo \" 日志文件: $LOG_FILE\"\n",
    "echo \" PID文件: $PID_FILE\"\n",
    "echo \"\"\n",
    "echo \" 监控命令:\"\n",
    "echo \"  查看实时日志: tail -f -n 300 $LOG_FILE\"\n",
    "echo \"  查看GPU使用: watch -n 1 nvidia-smi\"\n",
    "echo \"  查看进程状态: ps aux | grep $TRAIN_PID\"\n",
    "echo \"  停止训练: kill $TRAIN_PID\"\n",
    "echo \"  启动tensorboard: tensorboard --logdir=./logs --port=6006\"\n",
    "echo \"\"\n",
    "echo \" 训练状态检查:\"\n",
    "echo \"  ps aux | grep torchrun\"\n",
    "echo \"  tail -20 $LOG_FILE\" \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6295fe",
   "metadata": {},
   "source": [
    "config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542b75f",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"model_name_or_path\": \"/shared/Skywork-Reward-Llama-3.1-8B\",\n",
    "  \"trust_remote_code\": true,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \n",
    "  \"data_path\": \"/shared/reward_model_data/reward_data\",\n",
    "  \"max_length\": 512,\n",
    "  \n",
    "  \"freeze_trainable_layers\": 4,\n",
    "  \"freeze_trainable_modules\": \"all\",\n",
    "  \"freeze_extra_modules\": \"score\",\n",
    "  \n",
    "  \"output_dir\": \"/shared/financial_reward_model/output/\",\n",
    "  \"logging_dir\": \"/shared/financial_reward_model/logs/\",\n",
    "  \"num_train_epochs\": 2,\n",
    "  \"learning_rate\": 5e-5,\n",
    "  \"warmup_ratio\": 0.1,\n",
    "  \"weight_decay\": 0.01,\n",
    "  \"seed\": 42,\n",
    "  \n",
    "  \"per_device_train_batch_size\": 2,\n",
    "  \"per_device_eval_batch_size\": 4,\n",
    "  \"gradient_accumulation_steps\": 4,\n",
    "  \n",
    "  \"bf16\": true,\n",
    "  \"fp16\": false,\n",
    "  \"tf32\": true,\n",
    "  \"max_grad_norm\": 1.0,\n",
    "  \n",
    "  \"evaluation_strategy\": \"steps\",\n",
    "  \"eval_steps\": 50,\n",
    "  \"save_strategy\": \"steps\", \n",
    "  \"save_steps\": 100,\n",
    "  \"save_total_limit\": 2,\n",
    "  \n",
    "  \"logging_steps\": 10,\n",
    "  \"load_best_model_at_end\": true,\n",
    "  \"metric_for_best_model\": \"eval_loss\",\n",
    "  \"greater_is_better\": false,\n",
    "  \"early_stopping_patience\": 5,\n",
    "  \n",
    "  \"deepspeed_stage\": 2,\n",
    "  \"deepspeed_offload_optimizer\": true,\n",
    "  \"deepspeed_offload_param\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681e55a",
   "metadata": {},
   "source": [
    "deepspeed_only.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19338a83",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": true,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\": true\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"bf16\": {\n",
    "    \"enabled\": \"auto\"\n",
    "  },\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"steps_per_print\": 1000,\n",
    "  \"wall_clock_breakdown\": false,\n",
    "  \"dump_state\": false,\n",
    "  \"communication_data_type\": \"fp16\",\n",
    "  \"checkpoint\": {\n",
    "    \"save_zero_checkpoint\": false\n",
    "  },\n",
    "  \"flops_profiler\": {\n",
    "    \"enabled\": false,\n",
    "    \"profile_step\": 1,\n",
    "    \"module_depth\": -1,\n",
    "    \"top_modules\": 1,\n",
    "    \"detailed\": true,\n",
    "    \"output_file\": null\n",
    "  }\n",
    "} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7934c54",
   "metadata": {},
   "source": [
    "从头开始训练： bash scripts/training/deepspeed_cluster_launcher.sh start  开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe02b8",
   "metadata": {},
   "source": [
    "checkpoint节点继续训练：bash scripts/training/deepspeed_cluster_launcher.sh resume output/checkpoint-700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd6fc7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625112348743.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844861d",
   "metadata": {},
   "source": [
    "集群资源占用情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8560a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250617193326191.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce46b76",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250617193356957.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8330f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250617193413751.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae75d2c",
   "metadata": {},
   "source": [
    "checkpoint与final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51327692",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625113415966.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0e26d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625113540563.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d1ca1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625113432900.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a007d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625113513254.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6ad0a",
   "metadata": {},
   "source": [
    "训练完成日志信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe3112",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624191601169.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21b493",
   "metadata": {},
   "source": [
    "查看trainer_state.json文件，里面包含了完整的训练状态\n",
    "\n",
    "cat /shared/financial_reward_model/output/checkpoint-1000/trainer_state.json | grep -A 5 -B 5 \"eval_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e419421",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624192638311.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a8ee8",
   "metadata": {},
   "source": [
    "QRM-Llama3.1-8B-v2   准确率100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651bb66",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625143413642.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7eb8c",
   "metadata": {},
   "source": [
    "# 4. 搭建+测试执行环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c56332",
   "metadata": {},
   "source": [
    "## 分布式环境部署方式一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5d9c1",
   "metadata": {},
   "source": [
    "**Ray+vllm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8bbd9",
   "metadata": {},
   "source": [
    "conda create -n vllm python=3.11 -y (注意当前环境创建以及之后的设置集群中`必须相同`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f92b58",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618115211874.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f6359",
   "metadata": {},
   "source": [
    "## Ray是什么？\n",
    "\n",
    "**Ray是一个开源的分布式计算框架**，专为Python和机器学习应用设计，能够将应用从单机扩展到多节点集群。\n",
    "\n",
    "### 1. 集群架构\n",
    "Ray采用**集群架构**，包含：\n",
    "- **头节点（Head Node）**：单个节点，运行集群管理进程\n",
    "- **工作节点（Worker Nodes）**：多个节点，执行实际任务\n",
    "\n",
    "### 2. 核心进程组件\n",
    "\n",
    "#### **Raylet**（每个节点都有）\n",
    "Raylet是Ray在每个节点上的核心代理进程，包含两个主要组件：\n",
    "\n",
    "1. **调度器（Scheduler）**：\n",
    "   - 负责资源管理和任务调度\n",
    "   - 决定哪个worker进程执行哪个任务\n",
    "   - 管理CPU、GPU、内存等资源分配\n",
    "\n",
    "2. **对象存储（Object Store）**：\n",
    "   - 基于Apache Arrow的Plasma对象存储\n",
    "   - 使用共享内存存储大型对象\n",
    "   - 实现零拷贝数据共享（同节点内）\n",
    "\n",
    "#### **GCS（Global Control Store）**\n",
    "- 运行在头节点上的全局控制服务\n",
    "- 键值存储，保存系统级元数据\n",
    "- 存储对象位置、Actor信息等\n",
    "- 接收节点心跳信号\n",
    "\n",
    "#### **Worker进程**\n",
    "- 执行实际的Ray任务和Actor方法\n",
    "- 每个CPU核心通常对应一个worker进程\n",
    "\n",
    "## Ray的调度算法\n",
    "\n",
    "### 1. 任务调度流程\n",
    "```json\n",
    "1. 用户调用 task.remote() → \n",
    "2. 向本地Raylet申请worker租约 → \n",
    "3. Raylet检查资源需求和依赖 → \n",
    "4. 分配合适的worker → \n",
    "5. 通过gRPC发送ExecuteTask RPC → \n",
    "6. Worker执行任务并返回结果\n",
    "```\n",
    "\n",
    "### 2. 调度优化策略\n",
    "- **调度决策缓存**：相同类型任务重用之前的调度决策\n",
    "- **局部性优化**：优先在有数据的节点上调度任务\n",
    "- **负载均衡**：任务分布到多个节点以最大化资源利用率\n",
    "\n",
    "### 3. 跨节点调度\n",
    "如果本地没有可用资源：\n",
    "1. 本地Raylet重定向请求到有资源的远程Raylet\n",
    "2. 通过gRPC直接向远程worker发送任务\n",
    "3. 支持透明的多节点扩展\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be469571",
   "metadata": {},
   "source": [
    "### 多种通信后端支持\n",
    "\n",
    "1. **NCCL（NVIDIA Collective Communication Library）**\n",
    "   - 主要用于GPU间的集合通信（All-Reduce、All-Gather等）\n",
    "   - 适用于深度学习的张量并行场景\n",
    "   - 需要NVIDIA GPU和CUDA环境\n",
    "\n",
    "2. **Gloo**\n",
    "   - Facebook开发的通信库\n",
    "   - 支持CPU和GPU\n",
    "   - 跨平台兼容性更好\n",
    "\n",
    "3. **Ray内置通信协议**\n",
    "   - 基于gRPC和Apache Arrow\n",
    "   - 用于一般的分布式对象传输\n",
    "   - 不依赖特定的硬件\n",
    "\n",
    "4. **TCP/InfiniBand直连**\n",
    "   - 点对点的网络通信\n",
    "   - 用于大数据传输\n",
    "\n",
    "### 通信方式的选择逻辑\n",
    "\n",
    "```json\n",
    "根据任务类型自动选择:\n",
    "├── 深度学习张量并行 → NCCL\n",
    "├── 一般分布式计算 → Gloo\n",
    "├── 对象传输 → Ray内置协议\n",
    "└── 大文件传输 → 直接TCP连接\n",
    "```\n",
    "\n",
    "### 子节点间通信机制\n",
    "\n",
    "#### 直接通信的实现\n",
    "\n",
    "**Ray的Worker节点间可以直接通信，无需通过主节点中转：**\n",
    "\n",
    "1. **建立连接的过程：**\n",
    "```json\n",
    "Worker A 需要与 Worker B 通信\n",
    "    ↓\n",
    "Worker A 向 Head Node 请求 Worker B 的地址信息\n",
    "    ↓\n",
    "Head Node 返回 Worker B 的 IP:Port\n",
    "    ↓\n",
    "Worker A 直接连接 Worker B 建立通信链路\n",
    "    ↓\n",
    "后续数据传输绕过 Head Node\n",
    "```\n",
    "\n",
    "2. **连接复用机制：**\n",
    "   - 一旦建立连接，Worker间会复用这个连接\n",
    "   - 避免每次通信都询问Head Node\n",
    "   - 提高通信效率\n",
    "\n",
    "### 主节点的核心作用解析\n",
    "\n",
    "#### 1. 集群引导和服务发现\n",
    "\n",
    "**主节点是集群的\"电话簿\"：**\n",
    "- 维护所有Worker节点的地址信息\n",
    "- 新节点加入时的注册中心\n",
    "- 提供节点间的服务发现能力\n",
    "\n",
    "```json\n",
    "Worker启动时:\n",
    "Worker → 向Head Node注册 (我在192.168.1.100:8001)\n",
    "Head Node → 记录到节点列表\n",
    "其他Worker → 询问Head Node某个服务在哪里\n",
    "Head Node → 返回地址信息\n",
    "```\n",
    "\n",
    "#### 2. 资源调度和任务分发\n",
    "\n",
    "**主节点是\"调度中心\"：**\n",
    "- 维护全局资源状态（CPU、GPU、内存使用情况）\n",
    "- 决定新任务应该在哪个节点执行\n",
    "- 实现负载均衡和资源优化\n",
    "\n",
    "#### 3. 分布式对象存储的元数据管理\n",
    "\n",
    "**主节点管理\"数据地图\"：**\n",
    "- 记录分布式对象存储在哪个节点\n",
    "- 管理对象的引用计数和生命周期\n",
    "- 协调跨节点的数据传输\n",
    "\n",
    "#### 4. 全局状态同步\n",
    "\n",
    "**主节点是\"状态中心\"：**\n",
    "- 收集各节点的健康状态\n",
    "- 处理节点故障和恢复\n",
    "- 维护全局的命名空间\n",
    "\n",
    "\n",
    "**在Ray + Transformer架构中：**\n",
    "\n",
    "1. **HTTP请求路由**：必须通过Head Node\n",
    "2. **模型推理**：各Worker独立执行，无需相互通信\n",
    "3. **结果返回**：直接从Worker返回给客户端\n",
    "\n",
    "**在Ray + vLLM架构中：**\n",
    "\n",
    "1. **任务调度**：通过Head Node\n",
    "2. **张量并行通信**：Worker间NCCL直连\n",
    "3. **权重同步**：Worker间高速网络直连\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cef6fc",
   "metadata": {},
   "source": [
    "当前vllm环境安装ray\n",
    "\n",
    "pip install \"ray[default]\" -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a0e69",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618141857154.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45796e2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618142255790.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b414fa3",
   "metadata": {},
   "source": [
    "安装其他依赖环境\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install accelerate==0.29.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a10034",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618143425202.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7afe9",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 查看依赖版本\n",
    "\n",
    "python3 -c \"\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'GPU count: {torch.cuda.device_count()}')\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a065ee7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618143411271.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c5e18",
   "metadata": {},
   "source": [
    "当前环境安装vllm\n",
    "\n",
    "pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263cda4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618144336728.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42e186",
   "metadata": {},
   "source": [
    "安装完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5f832",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618144405318.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a81423",
   "metadata": {},
   "source": [
    "```bash  \n",
    "#安装transformers（使用镜像）\n",
    "pip install transformers \n",
    "pip install torchaudio==2.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install vllm==0.6.6 transformers==4.45.2 tokenizers==0.20.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "#安装其他必要依赖\n",
    "pip install requests numpy -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "python3 -c \"\n",
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "try:\n",
    "​    import torch\n",
    "​    print(f'✅ PyTorch {torch.__version__} installed')\n",
    "​    print(f'✅ CUDA available: {torch.cuda.is_available()}')\n",
    "​    if torch.cuda.is_available():\n",
    "​        print(f'✅ CUDA version: {torch.version.cuda}')\n",
    "except ImportError as e:\n",
    "​    print(f'❌ PyTorch import failed: {e}')\n",
    "try:\n",
    "​    import ray\n",
    "​    print(f'✅ Ray {ray.__version__} installed')\n",
    "except ImportError as e:\n",
    "​    print(f'❌ Ray import failed: {e}')\n",
    "try:\n",
    "​    import vllm\n",
    "​    print(f'✅ vLLM {vllm.__version__} installed')\n",
    "except ImportError as e:\n",
    "​    print(f'❌ vLLM import failed: {e}')\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fe1bc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618144625237.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fbba6",
   "metadata": {},
   "source": [
    "```bash  \n",
    "#在host0 (10.60.11.131) 上执行\n",
    "cd /shared/financial_reward_model\n",
    "#启动Ray头节点\n",
    "ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --num-gpus=4\n",
    "#记录输出的连接信息，类似：\n",
    "#ray start --address='10.60.11.131:6379'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad1f3d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618153915877.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41592499",
   "metadata": {},
   "source": [
    "```bash  \n",
    "#在host1 (10.60.240.249) 上执行\n",
    "#使用host0输出的连接命令\n",
    "ray start --address='10.60.11.131:6379' --num-gpus=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456cd6c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618154118631.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22d415",
   "metadata": {},
   "source": [
    "```bash  \n",
    "#在host0上执行\n",
    "ray status\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df59bec",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618154307616.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049071a2",
   "metadata": {},
   "source": [
    "```bash  \n",
    "#在两台机器上都执行，开放PyTorch分布式通信端口\n",
    "sudo ufw allow 29500:29510/tcp\n",
    "sudo ufw allow 6379/tcp    # Ray端口\n",
    "sudo ufw allow 8265/tcp    # Ray Dashboard端口\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7989998",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618171540695.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fee604",
   "metadata": {},
   "source": [
    "下载Qwen2.5-14B-Instruct 进行模型验证，测试分布式环境适用情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e628a4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618191103326.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da9bd7",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model /shared/Qwen2.5-14B-Instruct \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8000 \\\n",
    "  --tensor-parallel-size 2 \\\n",
    "  --pipeline-parallel-size 3 \\\n",
    "  --dtype bfloat16 \\\n",
    "  --max-model-len 2048 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --disable-log-requests \\\n",
    "  --api-key financial-reward-model-key \\\n",
    "  --distributed-executor-backend ray\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "## vLLM命令参数详解\n",
    "\n",
    "### 模型和服务参数\n",
    "--model: 指定要加载的模型路径\n",
    "--host 0.0.0.0: 允许所有IP访问（不限制来源）\n",
    "--port 8000: API服务监听的端口号\n",
    "--api-key: API访问的安全密钥\n",
    "\n",
    "### 模型配置参数\n",
    "--dtype bfloat16: 使用半精度浮点数，节省显存，略微降低精度\n",
    "--max-model-len 2048: 模型处理的最大序列长度\n",
    "--gpu-memory-utilization 0.85: 使用85%的GPU显存，留15%作为缓冲\n",
    "\n",
    "### 并行配置参数\n",
    "--tensor-parallel-size 2: 张量并行度\n",
    "--pipeline-parallel-size 3: 流水线并行度\n",
    "--distributed-executor-backend ray: 使用Ray作为分布式执行后端\n",
    "\n",
    "### 性能优化参数\n",
    "--disable-log-requests: 关闭请求日志，减少IO开销\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39998294",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200053492.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9e5c1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200028412.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbe338",
   "metadata": {},
   "source": [
    "## 注意力头数的详细解释\n",
    "\n",
    "### 什么是注意力头数？\n",
    "注意力头数是Transformer架构中多头注意力机制的核心概念：\n",
    "\n",
    "**单头注意力**：就像一个人只用一种方式关注信息\n",
    "**多头注意力**：就像一个人同时用多种不同角度关注同一个信息\n",
    "\n",
    "比如Qwen2.5-14B可能有32个注意力头，意味着：\n",
    "- 每一层都有32个\"注意力探测器\"\n",
    "- 每个头负责捕捉不同类型的语言模式\n",
    "- 有些头关注语法，有些关注语义，有些关注长距离依赖\n",
    "\n",
    "### 为什么要能被GPU数整除？\n",
    "\n",
    "**张量并行的工作方式**：\n",
    "- 假设有32个注意力头，使用4张GPU做张量并行\n",
    "- 每张GPU分配 32÷4=8个注意力头\n",
    "- GPU1处理头1-8，GPU2处理头9-16，以此类推\n",
    "- **如果不能整除**：比如32个头分给5张GPU，就无法均匀分配\n",
    "- 会导致某些GPU空闲，某些GPU过载，甚至程序崩溃\n",
    "\n",
    "**整除的意义**：\n",
    "- **计算均匀**：每张GPU负载相同，没有空闲资源\n",
    "- **内存均匀**：每张GPU存储相同大小的模型部分\n",
    "- **通信高效**：数据传输模式规整，减少同步等待时间\n",
    "\n",
    "## 张量并行 vs 流水线并行的深度解释\n",
    "\n",
    "### 张量并行（横向切分）\n",
    "**比喻**：就像一个大工厂的同一条生产线，被多个工人同时操作\n",
    "\n",
    "**工作机制**：\n",
    "- 把模型的**同一层**的计算分割给多个GPU\n",
    "- 每个GPU计算这一层的一部分，然后汇总结果\n",
    "- 需要频繁的GPU间通信（AllReduce操作）\n",
    "- 适合**单个层太大**的情况\n",
    "\n",
    "**优缺点**：\n",
    "- ✅ 可以处理超大模型层\n",
    "- ✅ 延迟较低（所有GPU同步工作）\n",
    "- ❌ 通信开销大（每层都要同步）\n",
    "- ❌ 扩展性有限（通信带宽限制）\n",
    "\n",
    "### 流水线并行（纵向切分）\n",
    "**比喻**：就像汽车生产流水线，每个工人负责不同的装配步骤\n",
    "\n",
    "**工作机制**：\n",
    "- 把模型的**不同层**分配给不同GPU\n",
    "- 数据像传送带一样依次通过各个GPU\n",
    "- GPU间通信较少（只传递激活值）\n",
    "- 适合**层数很多**的情况\n",
    "\n",
    "**优缺点**：\n",
    "- ✅ 通信开销小\n",
    "- ✅ 扩展性好（可以用很多GPU）\n",
    "- ❌ 有流水线气泡（某些时刻GPU空闲）\n",
    "- ❌ 批次大小需要仔细调优\n",
    "\n",
    "\n",
    "总GPU数 = tensor-parallel-size × pipeline-parallel-size = 2 × 3 = 6张GPU    \n",
    "\n",
    "GPU分布：    \n",
    "Stage 1: [GPU0, GPU1] - 处理模型前1/3层，每层在2个GPU间做张量并行    \n",
    "Stage 2: [GPU2, GPU3] - 处理模型中1/3层，每层在2个GPU间做张量并行     \n",
    "Stage 3: [GPU4, GPU5] - 处理模型后1/3层，每层在2个GPU间做张量并行    \n",
    "\n",
    "## vLLM与Ray的协作机制\n",
    "\n",
    "### Ray的作用\n",
    "**Ray本质**：是一个分布式计算框架，专门处理多机多卡的资源管理和任务调度\n",
    "\n",
    "**在这里的职责**：\n",
    "1. **资源发现**：自动检测集群中有哪些GPU可用\n",
    "2. **进程管理**：在不同GPU上启动vLLM的工作进程\n",
    "3. **通信协调**：管理GPU间的数据传输和同步\n",
    "4. **容错处理**：如果某个GPU失效，Ray负责重启和恢复\n",
    "\n",
    "### vLLM与Ray的交互流程\n",
    "\n",
    "**启动阶段**：\n",
    "1. 你执行命令后，vLLM主进程启动\n",
    "2. vLLM检测到`--distributed-executor-backend ray`参数\n",
    "3. vLLM向Ray集群注册，申请6张GPU资源\n",
    "4. Ray根据配置，在6张GPU上分别启动vLLM工作进程\n",
    "5. Ray建立GPU间的通信通道（NCCL等）\n",
    "\n",
    "**运行阶段**：\n",
    "1. 主进程接收API请求\n",
    "2. 将请求分发给Ray管理的GPU工作进程\n",
    "3. GPU进程按照张量并行+流水线并行模式协作计算\n",
    "4. Ray协调GPU间的数据同步和结果汇总\n",
    "5. 主进程将最终结果返回给API调用者\n",
    "\n",
    "**关键理解**：\n",
    "- **vLLM负责模型推理逻辑**：怎么做注意力计算、怎么生成文本\n",
    "- **Ray负责基础设施管理**：哪个GPU做什么、怎么通信、怎么容错\n",
    "- **两者分工明确**：vLLM专注AI算法，Ray专注分布式系统\n",
    "\n",
    "### 为什么需要Ray？\n",
    "如果没有Ray，你需要手动：\n",
    "- 在每张GPU上启动进程\n",
    "- 配置GPU间的网络通信\n",
    "- 处理进程间的同步和容错\n",
    "- 管理内存和资源分配\n",
    "\n",
    "Ray把这些复杂的分布式系统问题都自动化处理了，让vLLM只需要专注于AI推理本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ec41f",
   "metadata": {},
   "source": [
    "```bash\n",
    "curl -s http://10.60.11.131:8000/v1/models \\\n",
    "  -H \"Authorization: Bearer financial-reward-model-key\" | jq .\n",
    "\n",
    "# curl -s：静默模式发送HTTP请求，不显示进度信息\n",
    "# http://10.60.11.131:8000/v1/models：请求的API端点\n",
    "# 10.60.11.131:8000：vLLM服务器的IP地址和端口\n",
    "# /v1/models：OpenAI兼容API的标准端点，用于获取可用模型列表\n",
    "# -H \"Authorization: Bearer financial-reward-model-key\"：添加认证头\n",
    "# 使用你在启动vLLM时设置的API密钥进行身份验证\n",
    "# | jq .：将JSON响应格式化输出，便于阅读\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c8f9c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200222660.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acbc5c",
   "metadata": {},
   "source": [
    "```bash\n",
    "curl -s http://10.60.11.131:8000/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer financial-reward-model-key\" \\\n",
    "  -d '{\n",
    "​    \"model\": \"/shared/Qwen2.5-14B-Instruct\",\n",
    "​    \"prompt\": \"你好，请简单介绍一下你自己：\",\n",
    "​    \"max_tokens\": 50,\n",
    "​    \"temperature\": 0.3\n",
    "  }' | jq .\n",
    "\n",
    "# curl -s：同样是静默模式\n",
    "# http://10.60.11.131:8000/v1/completions：文本补全API端点\n",
    "# -H \"Content-Type: application/json\"：指定请求体为JSON格式\n",
    "# -H \"Authorization: Bearer financial-reward-model-key\"：API认证\n",
    "# -d '{...}'：POST请求的JSON数据体\n",
    "# JSON参数详解：\n",
    "# \"model\": \"/shared/Qwen2.5-14B-Instruct\"：\n",
    "# 指定使用的模型路径\n",
    "# 必须与vLLM加载的模型路径完全匹配\n",
    "# \"prompt\": \"你好，请简单介绍一下你自己：\"：\n",
    "# 输入的提示文本\n",
    "# 模型将基于这个提示生成后续内容\n",
    "# \"max_tokens\": 50：\n",
    "# 限制生成的最大token数量\n",
    "# 控制响应长度，避免生成过长内容\n",
    "# \"temperature\": 0.3：\n",
    "# 控制生成文本的随机性\n",
    "# 范围通常是0-2，值越小越确定性，越大越有创造性\n",
    "# 0.3是较低的值，生成内容会比较稳定和一致\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1348f98",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200313317.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716bb86",
   "metadata": {},
   "source": [
    "```bash\n",
    "curl -s http://10.60.11.131:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer financial-reward-model-key\" \\\n",
    "  -d '{\n",
    "    \"model\": \"/shared/Qwen2.5-14B-Instruct\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"1+1等于多少？\"}\n",
    "    ],\n",
    "    \"max_tokens\": 30,\n",
    "    \"temperature\": 0.1\n",
    "  }' | jq .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44376e68",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200525767.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797e83d",
   "metadata": {},
   "source": [
    "资源占用分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36d698",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200554176.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a2826",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200611749.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6afbf5",
   "metadata": {},
   "source": [
    "vllm 支持的模型  https://vllm.hyper.ai/docs/models/supported_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc329b7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624204556063.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9407a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624204528715.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fafca1",
   "metadata": {},
   "source": [
    "\n",
    "```bash \n",
    " #查看模型的配置文件\n",
    "cat /shared/QRM-Llama3.1-8B-v2/config.json | grep -E \"architectures|model_type\"\n",
    " #或者完整查看配置\n",
    "head -20 /shared/QRM-Llama3.1-8B-v2/config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016d6af",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618200901680.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b81b5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624204205033.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae4021",
   "metadata": {},
   "source": [
    "## 两个模型的核心区别\n",
    "\n",
    "### 1. **架构类型差异**\n",
    "\n",
    "**Skywork-Reward-Llama-3.1-8B**:\n",
    "```json\n",
    "\"architectures\": [\"LlamaForSequenceClassification\"]\n",
    "```\n",
    "\n",
    "**QRM-Llama3.1-8B-v2**:\n",
    "```json\n",
    "\"architectures\": [\"LlamaForRewardModelWithGating\"]\n",
    "```\n",
    "\n",
    "- **LlamaForSequenceClassification**: 标准的序列分类架构，通常用于基础的奖励模型\n",
    "- **LlamaForRewardModelWithGating**: 带有门控机制的自定义奖励模型架构，更复杂\n",
    "\n",
    "门控机制本质上是一种学习型的注意力机制，让模型能够：   \n",
    "- 自动学会什么时候关注什么信息    \n",
    "- 根据上下文动态调整处理策略    \n",
    "- 提供更精细和灵活的控制能力   \n",
    "- 在奖励模型中，门控机制可能帮助模型更好地理解和评估不同类型内容的质量，提供更准确和细致的奖励信号。   \n",
    "\n",
    "### 2. **门控机制相关参数**\n",
    "\n",
    "QRM模型独有的门控配置：\n",
    "```json\n",
    "\"gating_hidden_dim\": 1024,      // 门控网络隐藏层维度\n",
    "\"gating_n_hidden\": 3,           // 门控网络隐藏层数量\n",
    "\"gating_newtork\": true,         // 启用门控网络（注意这里有拼写错误）\n",
    "\"gating_temperature\": 5.0,      // 门控温度参数，控制门控的敏感度\n",
    "```\n",
    "\n",
    "这些参数表明QRM模型使用了**门控机制**来动态调整不同输入的处理方式。\n",
    "\n",
    "## auto_map 的作用\n",
    "\n",
    "```json\n",
    "\"auto_map\": {\n",
    "  \"AutoModelForSequenceClassification\": \"modeling_custom.LlamaForRewardModelWithGating\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **auto_map的核心功能**：\n",
    "\n",
    "1. **自定义模型类映射**：\n",
    "   - 告诉Transformers库当使用`AutoModelForSequenceClassification.from_pretrained()`时\n",
    "   - 应该加载`modeling_custom.LlamaForRewardModelWithGating`这个自定义类\n",
    "   - 而不是使用标准的Llama序列分类模型\n",
    "\n",
    "2. **模块路径指定**：\n",
    "   - `modeling_custom`：指向自定义模型实现文件（通常是`modeling_custom.py`）\n",
    "   - `LlamaForRewardModelWithGating`：文件中的具体类名\n",
    "\n",
    "3. **兼容性保证**：\n",
    "   - 让自定义架构能够与Transformers的AutoModel系统兼容\n",
    "   - 用户可以像使用标准模型一样使用自定义模型\n",
    "\n",
    "### **工作流程**：\n",
    "```python\n",
    "# 当执行这行代码时\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/shared/QRM-Llama3.1-8B-v2\")\n",
    "\n",
    "# Transformers会：\n",
    "# 1. 读取config.json中的auto_map\n",
    "# 2. 导入modeling_custom.py文件\n",
    "# 3. 实例化LlamaForRewardModelWithGating类\n",
    "# 4. 而不是标准的LlamaForSequenceClassification\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df093e",
   "metadata": {},
   "source": [
    "\n",
    "## config.json 的关键作用\n",
    "\n",
    "### 1. **模型架构定义**\n",
    "- 定义模型的结构参数（层数、隐藏维度等）\n",
    "- 指定模型类型和自定义组件\n",
    "- 设置特殊参数（如门控机制配置）\n",
    "\n",
    "### 2. **加载行为控制**\n",
    "- 告诉框架如何正确实例化模型\n",
    "- 指定使用哪个模型类\n",
    "- 配置模型的初始化参数\n",
    "\n",
    "### 3. **兼容性和互操作性**\n",
    "- 确保模型能在不同环境中正确加载\n",
    "- 提供模型的元数据信息\n",
    "- 支持模型的序列化和反序列化\n",
    "\n",
    "### 4. **推理参数配置**\n",
    "- 设置tokenizer相关参数（bos_token_id, eos_token_id）\n",
    "- 配置注意力机制参数\n",
    "- 定义标签映射关系\n",
    "\n",
    "## 实际影响\n",
    "\n",
    "1. **加载方式**：\n",
    "   - Skywork模型可以直接用标准方法加载\n",
    "   - QRM模型需要自定义的modeling_custom.py文件支持\n",
    "\n",
    "2. **功能差异**：\n",
    "   - QRM模型的门控机制可能提供更精细的奖励评估\n",
    "   - Skywork模型使用标准的分类方法\n",
    "\n",
    "3. **部署要求**：\n",
    "   - QRM模型部署时必须包含自定义模型实现文件\n",
    "   - Skywork模型可以直接使用标准Transformers库\n",
    "\n",
    "这些配置直接影响模型的加载、初始化和推理行为，是模型正常运行的关键配置文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212264b4",
   "metadata": {},
   "source": [
    "校验加载QRM-Llama3.1-8B-v2 模型无法加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9eeae",
   "metadata": {},
   "source": [
    "```bash  #简化测试\n",
    "python -c \"\n",
    "from vllm import LLM\n",
    "try:\n",
    "​    llm = LLM(\n",
    "​        model='/shared/QRM-Llama3.1-8B-v2',\n",
    "​        task='reward',\n",
    "​        trust_remote_code=True,\n",
    "​        tensor_parallel_size=1\n",
    "​    )\n",
    "​    print('✅ 模型加载成功！')\n",
    "​    \n",
    "​   \n",
    "​    outputs = llm.encode(['这是一个好的回答', '这是一个不好的回答'])\n",
    "​    print('✅ 奖励评分成功！')\n",
    "​    for i, output in enumerate(outputs):\n",
    "​        print(f'文本 {i+1} 的奖励分数: {output}')\n",
    "​        \n",
    "except Exception as e:\n",
    "​    print('❌ 错误:', e)\n",
    "​    import traceback\n",
    "​    traceback.print_exc()\n",
    "\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0de834",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250618201218697.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398afc9",
   "metadata": {},
   "source": [
    "## 分布式环境部署方式二"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cfa99",
   "metadata": {},
   "source": [
    "### **Ray+transformer数据并行模式**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42aaf8",
   "metadata": {},
   "source": [
    "\n",
    "### Ray + Transformer vs Ray + vLLM的区别\n",
    "\n",
    "#### 计算模式差异\n",
    "\n",
    "**Ray + Transformer（数据并行）：**\n",
    "- **模型分布**：每个GPU有完整模型副本\n",
    "- **计算方式**：GPU独立计算，互不干扰\n",
    "- **通信需求**：几乎无GPU间通信\n",
    "- **内存使用**：每张GPU重复存储相同模型\n",
    "\n",
    "**Ray + vLLM（张量并行）：**\n",
    "- **模型分布**：模型权重按张量维度切分到多张GPU\n",
    "- **计算方式**：多张GPU协同计算单个请求\n",
    "- **通信需求**：频繁的GPU间数据同步\n",
    "- **内存使用**：模型参数在多张GPU间分散存储\n",
    "\n",
    "#### 性能特征对比\n",
    "\n",
    "**并发处理能力：**\n",
    "- 数据并行：可同时处理N个请求（N=副本数）\n",
    "- 张量并行：通常同时处理1个请求，但可以处理更大的模型\n",
    "\n",
    "**延迟特征：**\n",
    "- 数据并行：单请求延迟低，但受副本可用性影响\n",
    "- 张量并行：需要GPU间同步，延迟相对较高\n",
    "\n",
    "**吞吐量特征：**\n",
    "- 数据并行：适合大量小请求的高频场景\n",
    "- 张量并行：适合少量大请求或需要高计算密度的场景\n",
    "\n",
    "### 副本使用多张GPU的问题\n",
    "\n",
    "#### 技术可行性\n",
    "\n",
    "**当前架构下不可行：**\n",
    "- `AutoModelForSequenceClassification`不支持原生的多GPU推理\n",
    "- Ray Actor的`num_gpus=1`设计就是单GPU绑定\n",
    "- 需要手动实现分布式推理逻辑\n",
    "\n",
    "**如果要实现多GPU副本：**\n",
    "- 需要使用`torch.nn.DataParallel`包装模型\n",
    "- 或者使用支持张量并行的推理框架\n",
    "- 但这会让架构变得复杂，失去数据并行的简洁性\n",
    "\n",
    "#### 与张量并行的本质区别\n",
    "\n",
    "**副本多GPU（如果实现）：**\n",
    "- 每个副本内部使用多张GPU加速单个请求\n",
    "- 副本间仍然是数据并行关系\n",
    "- 本质上是\"数据并行 + 模型并行\"的混合模式\n",
    "\n",
    "**vLLM张量并行：**\n",
    "- 整个集群作为一个大的推理引擎\n",
    "- 所有GPU协同处理请求\n",
    "- 专门为大模型设计的分布式推理\n",
    "\n",
    "### 流式构建的差异\n",
    "\n",
    "#### 数据并行模式的处理方式\n",
    "\n",
    "**批处理特征：**\n",
    "- 每个副本独立处理自己的请求\n",
    "- 可以各自进行小批量处理\n",
    "- 批大小受单GPU内存限制\n",
    "\n",
    "**响应模式：**\n",
    "- 通常是一次性返回完整结果\n",
    "- 适合分类、评分等非生成任务\n",
    "- 不需要流式输出\n",
    "\n",
    "#### vLLM的流式优势\n",
    "\n",
    "**连续批处理：**\n",
    "- 动态调整批次大小\n",
    "- 充分利用GPU计算资源\n",
    "- 自动优化内存使用\n",
    "\n",
    "**流式生成：**\n",
    "- 支持实时输出token\n",
    "- 降低首字延迟\n",
    "- 适合文本生成任务\n",
    "\n",
    "### 适用场景分析\n",
    "\n",
    "#### Ray + Transformer适用场景\n",
    "\n",
    "**业务特征：**\n",
    "1. **高并发短请求**\n",
    "   - 在线内容审核\n",
    "   - 实时推荐评分\n",
    "   - 客服意图识别\n",
    "\n",
    "2. **中小型模型**\n",
    "   - 7B以下参数的模型\n",
    "   - 单GPU可以装下的模型\n",
    "   - BERT、RoBERTa等编码器模型\n",
    "\n",
    "3. **独立任务处理**\n",
    "   - 每个请求相互独立\n",
    "   - 不需要维持会话状态\n",
    "   - 结果可以立即返回\n",
    "\n",
    "**技术优势：**\n",
    "- 部署简单，几乎不需要修改现有代码\n",
    "- 故障隔离性好，单个副本崩溃不影响其他副本\n",
    "- 易于水平扩展，可以随时增减副本\n",
    "- 运维成本低，监控和调试相对容易\n",
    "\n",
    "#### Ray + vLLM适用场景\n",
    "\n",
    "**业务特征：**\n",
    "1. **大模型推理**\n",
    "   - 70B+超大语言模型\n",
    "   - 单GPU装不下的模型\n",
    "   - 需要极高计算密度的任务\n",
    "\n",
    "2. **生成式任务**\n",
    "   - 对话系统\n",
    "   - 文本生成\n",
    "   - 代码生成\n",
    "   - 创意写作\n",
    "\n",
    "3. **批量处理**\n",
    "   - 离线内容生成\n",
    "   - 大规模数据处理\n",
    "   - 需要高GPU利用率的场景\n",
    "\n",
    "**技术优势：**\n",
    "- 内存效率高，可以支持更大的模型\n",
    "- GPU利用率高，计算资源浪费少\n",
    "- 支持流式输出，用户体验好\n",
    "- 对大批量处理优化程度高\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371979a",
   "metadata": {},
   "source": [
    "## Ray在两种架构中的角色差异\n",
    "\n",
    "### Ray + Transformer模式中的Ray职责\n",
    "\n",
    "**Ray的核心作用：分布式服务编排器**\n",
    "\n",
    "1. **集群管理层面：**\n",
    "   - 管理多台物理服务器的资源\n",
    "   - 协调不同节点间的任务分发\n",
    "   - 提供统一的资源视图（CPU、GPU、内存）\n",
    "\n",
    "2. **服务部署层面：**\n",
    "   - 将每个Transformer实例包装成Ray Actor\n",
    "   - 在指定的GPU上启动独立的模型实例\n",
    "   - 管理实例的生命周期（启动、重启、关闭）\n",
    "\n",
    "3. **请求路由层面：**\n",
    "   - 接收HTTP请求\n",
    "   - 根据负载均衡策略选择空闲的Actor\n",
    "   - 将请求转发给选中的Actor\n",
    "   - 收集结果并返回给客户端\n",
    "\n",
    "**关键特点：Ray不参与模型内部的计算**\n",
    "- Transformer在各自的GPU上完全独立运行\n",
    "- Ray只负责\"谁来处理这个请求\"，不管\"怎么处理\"\n",
    "- 类似于Kubernetes + Docker的关系\n",
    "\n",
    "### Ray + vLLM模式中的Ray职责\n",
    "\n",
    "**Ray的核心作用：分布式计算基础设施**\n",
    "\n",
    "1. **集群管理层面：**\n",
    "   - 同样管理物理节点和资源\n",
    "   - 但需要更细粒度的GPU资源管理\n",
    "   - 支持GPU间的高速通信拓扑\n",
    "\n",
    "2. **分布式计算支持：**\n",
    "   - 提供分布式通信原语（All-Reduce、All-Gather）\n",
    "   - 管理跨GPU的计算任务调度\n",
    "   - 协调张量在不同GPU间的传输\n",
    "\n",
    "3. **与vLLM的协作关系：**\n",
    "   - **vLLM负责**：模型分片策略、张量并行逻辑、GPU间通信协议\n",
    "   - **Ray负责**：底层的进程管理、网络通信、资源分配\n",
    "\n",
    "**关键特点：Ray参与计算的基础设施层面**\n",
    "- vLLM决定\"如何分片\"和\"如何计算\"\n",
    "- Ray提供\"在哪里计算\"和\"如何通信\"的基础设施\n",
    "- 类似于CUDA Runtime + NCCL的关系\n",
    "\n",
    "### 具体的职责分工详解\n",
    "\n",
    "#### Ray + Transformer的分工\n",
    "\n",
    "```\n",
    "客户端请求 \n",
    "    ↓\n",
    "Ray Serve (HTTP服务器)\n",
    "    ↓\n",
    "Ray负载均衡器 (选择副本)\n",
    "    ↓\n",
    "Ray Actor调度器 (找到对应GPU节点)\n",
    "    ↓\n",
    "独立的Transformer实例 (在指定GPU上运行)\n",
    "    ↓\n",
    "返回结果给Ray Serve\n",
    "    ↓\n",
    "返回给客户端\n",
    "```\n",
    "\n",
    "**Ray的职责边界：**\n",
    "- ✅ 管理6台服务器上的GPU资源\n",
    "- ✅ 决定请求发送到哪个GPU\n",
    "- ✅ 监控每个Transformer实例的健康状态\n",
    "- ✅ 提供故障转移和重启机制\n",
    "- ❌ 不参与模型的前向推理计算\n",
    "- ❌ 不处理Transformer的内部逻辑\n",
    "\n",
    "#### Ray + vLLM的分工\n",
    "\n",
    "```\n",
    "客户端请求\n",
    "    ↓\n",
    "Ray Serve (HTTP服务器)\n",
    "    ↓\n",
    "vLLM分布式引擎 (决定如何分片处理)\n",
    "    ↓\n",
    "Ray分布式计算层 (提供跨GPU通信)\n",
    "    ↓\n",
    "多个GPU协同计算 (张量并行)\n",
    "    ↓\n",
    "vLLM结果聚合\n",
    "    ↓\n",
    "返回给客户端\n",
    "```\n",
    "\n",
    "**Ray的职责边界：**\n",
    "- ✅ 管理分布式集群资源\n",
    "- ✅ 提供GPU间的通信基础设施\n",
    "- ✅ 支持vLLM的分布式计算需求\n",
    "- ✅ 管理分布式任务的调度\n",
    "- ❌ 不决定模型如何分片\n",
    "- ❌ 不参与张量并行的具体算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495bcca",
   "metadata": {},
   "source": [
    "# 5. 模型校验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e02a1f",
   "metadata": {},
   "source": [
    "## 执行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaa29e",
   "metadata": {},
   "source": [
    "启动代码：ray_reward_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8180e9",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, LlamaTokenizer\n",
    "\n",
    "# 配置日志系统，设置日志级别和格式\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 使用Ray Serve装饰器定义服务部署\n",
    "# 这个装饰器告诉Ray如何部署这个服务类\n",
    "# num_gpus=1: 每个服务实例使用1个GPU\n",
    "# num_cpus=2: 每个服务实例使用2个CPU核心\n",
    "@serve.deployment(\n",
    "    ray_actor_options={\n",
    "        \"num_gpus\": 1,\n",
    "        \"num_cpus\": 2\n",
    "    }\n",
    ")\n",
    "class RewardService:\n",
    "    \"\"\"\n",
    "    奖励模型推理服务类\n",
    "    \n",
    "    这个类是整个推理服务的核心，负责：\n",
    "    1. 加载奖励模型和tokenizer\n",
    "    2. 处理推理请求\n",
    "    3. 返回chosen和rejected回答的分数\n",
    "    \n",
    "    在整个系统中的作用：\n",
    "    - 接收HTTP请求，包含question、chosen、rejected三个字段\n",
    "    - 使用训练好的奖励模型对chosen和rejected回答进行评分\n",
    "    - 返回分数结果，用于判断哪个回答更好\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        初始化奖励模型服务\n",
    "        \n",
    "        这个方法在服务启动时被调用，负责加载模型和tokenizer\n",
    "        \n",
    "        参数:\n",
    "            model_path: 训练好的奖励模型的路径\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 这是服务的初始化入口，只在服务启动时执行一次\n",
    "        - 加载的模型和tokenizer会被后续的推理请求重复使用\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        # 检测是否有可用的GPU，优先使用GPU进行推理\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 第一步：加载tokenizer（文本分词器）\n",
    "        # tokenizer的作用是将文本转换为模型可以理解的数字序列\n",
    "        try:\n",
    "            logger.info(\"开始加载tokenizer...\")\n",
    "            logger.info(f\"模型路径: {model_path}\")\n",
    "            \n",
    "            # 检查模型路径是否存在\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"模型路径不存在: {model_path}\")\n",
    "            \n",
    "            # 检查tokenizer相关文件是否存在\n",
    "            # 这些文件是tokenizer正常工作所必需的\n",
    "            tokenizer_files = [\n",
    "                \"tokenizer.json\",          # 主要的tokenizer配置\n",
    "                \"tokenizer_config.json\",   # tokenizer配置文件\n",
    "                \"vocab.json\",              # 词汇表文件\n",
    "                \"merges.txt\",              # BPE合并规则（如果使用BPE）\n",
    "                \"special_tokens_map.json\"  # 特殊token映射\n",
    "            ]\n",
    "            \n",
    "            existing_files = []\n",
    "            for file in tokenizer_files:\n",
    "                file_path = os.path.join(model_path, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    existing_files.append(file)\n",
    "            \n",
    "            logger.info(f\"找到的tokenizer文件: {existing_files}\")\n",
    "            \n",
    "            # 尝试多种方式加载tokenizer\n",
    "            # 这是为了提高兼容性，如果一种方法失败，会尝试其他方法\n",
    "            tokenizer = None\n",
    "            \n",
    "            # 方法1: 使用AutoTokenizer自动识别tokenizer类型\n",
    "            try:\n",
    "                logger.info(\"尝试使用AutoTokenizer加载...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_path, \n",
    "                    use_fast=False,        # 不使用快速tokenizer，提高兼容性\n",
    "                    local_files_only=True, # 只使用本地文件，不从网络下载\n",
    "                    trust_remote_code=False # 不信任远程代码，提高安全性\n",
    "                )\n",
    "                logger.info(f\"AutoTokenizer加载成功，类型: {type(tokenizer)}\")\n",
    "                \n",
    "                # 验证tokenizer不是布尔值\n",
    "                # 这是为了防止某些异常情况下tokenizer被错误地设置为布尔值\n",
    "                if isinstance(tokenizer, bool):\n",
    "                    logger.error(f\"AutoTokenizer返回了布尔值: {tokenizer}\")\n",
    "                    tokenizer = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"AutoTokenizer加载失败: {e}\")\n",
    "                tokenizer = None\n",
    "            \n",
    "            # 方法2: 如果AutoTokenizer失败，尝试直接使用LlamaTokenizer\n",
    "            if tokenizer is None:\n",
    "                try:\n",
    "                    logger.info(\"尝试使用LlamaTokenizer加载...\")\n",
    "                    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                        model_path,\n",
    "                        use_fast=False,\n",
    "                        local_files_only=True,\n",
    "                        trust_remote_code=False\n",
    "                    )\n",
    "                    logger.info(f\"LlamaTokenizer加载成功，类型: {type(tokenizer)}\")\n",
    "                    \n",
    "                    if isinstance(tokenizer, bool):\n",
    "                        logger.error(f\"LlamaTokenizer返回了布尔值: {tokenizer}\")\n",
    "                        tokenizer = None\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"LlamaTokenizer加载失败: {e}\")\n",
    "                    tokenizer = None\n",
    "            \n",
    "            # 方法3: 尝试从基础模型路径加载tokenizer\n",
    "            # 这是最后的备选方案，使用原始的基础模型的tokenizer\n",
    "            if tokenizer is None:\n",
    "                try:\n",
    "                    logger.info(\"尝试从基础模型路径加载tokenizer...\")\n",
    "                    base_model_path = \"/shared/Skywork-Reward-Llama-3.1-8B\"\n",
    "                    if os.path.exists(base_model_path):\n",
    "                        tokenizer = AutoTokenizer.from_pretrained(\n",
    "                            base_model_path,\n",
    "                            use_fast=False,\n",
    "                            local_files_only=True,\n",
    "                            trust_remote_code=False\n",
    "                        )\n",
    "                        logger.info(f\"从基础模型加载tokenizer成功，类型: {type(tokenizer)}\")\n",
    "                        \n",
    "                        if isinstance(tokenizer, bool):\n",
    "                            logger.error(f\"从基础模型加载的tokenizer是布尔值: {tokenizer}\")\n",
    "                            tokenizer = None\n",
    "                    else:\n",
    "                        logger.warning(f\"基础模型路径不存在: {base_model_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"从基础模型加载tokenizer失败: {e}\")\n",
    "                    tokenizer = None\n",
    "            \n",
    "            # 如果所有方法都失败了，抛出错误\n",
    "            if tokenizer is None:\n",
    "                raise RuntimeError(\"所有tokenizer加载方法都失败了\")\n",
    "            \n",
    "            # 验证tokenizer对象是否正常\n",
    "            logger.info(f\"Tokenizer最终类型: {type(tokenizer)}\")\n",
    "            logger.info(f\"Tokenizer是否可调用: {callable(tokenizer)}\")\n",
    "            \n",
    "            # 检查tokenizer是否有pad_token属性\n",
    "            # pad_token用于将不同长度的文本序列填充到相同长度\n",
    "            if not hasattr(tokenizer, 'pad_token'):\n",
    "                raise RuntimeError(f\"Tokenizer对象没有pad_token属性: {type(tokenizer)}\")\n",
    "            \n",
    "            # 设置pad_token\n",
    "            # pad_token是用于填充序列的特殊token，确保批次中所有序列长度相同\n",
    "            logger.info(f\"当前pad_token: {getattr(tokenizer, 'pad_token', 'None')}\")\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                # 如果没有pad_token，尝试使用eos_token作为pad_token\n",
    "                if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token is not None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    logger.info(f\"设置pad_token为eos_token: {tokenizer.pad_token}\")\n",
    "                else:\n",
    "                    # 如果没有eos_token，手动添加一个pad_token\n",
    "                    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "                    logger.info(f\"添加新的pad_token: {tokenizer.pad_token}\")\n",
    "            \n",
    "            logger.info(f\"最终pad_token: {tokenizer.pad_token}\")\n",
    "            \n",
    "            # 只有在一切正常的情况下才将tokenizer赋值给实例变量\n",
    "            self.tokenizer = tokenizer\n",
    "            logger.info(\"Tokenizer初始化完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"tokenizer加载失败: {e}\")\n",
    "            import traceback\n",
    "            logger.error(f\"错误堆栈: {traceback.format_exc()}\")\n",
    "            raise RuntimeError(f\"Tokenizer加载失败: {e}\")\n",
    "        \n",
    "        # 第二步：加载奖励模型\n",
    "        # 奖励模型是一个分类模型，用于对文本回答进行评分\n",
    "        try:\n",
    "            logger.info(\"开始加载模型...\")\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16,  # 使用bfloat16精度，节省显存\n",
    "                device_map=\"auto\",           # 自动分配设备\n",
    "                trust_remote_code=False,     # 不信任远程代码\n",
    "                local_files_only=True        # 只使用本地文件\n",
    "            )\n",
    "            # 设置模型为评估模式，关闭dropout等训练时的随机性\n",
    "            self.model.eval()\n",
    "            logger.info(f\"模型加载完成，设备: {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型加载失败: {e}\")\n",
    "            import traceback\n",
    "            logger.error(f\"错误堆栈: {traceback.format_exc()}\")\n",
    "            raise RuntimeError(f\"模型加载失败: {e}\")\n",
    "    \n",
    "    def format_conversation(self, question: str, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        格式化对话文本\n",
    "        \n",
    "        这个方法将用户问题和助手回答格式化为LLaMA-3的对话格式\n",
    "        \n",
    "        参数:\n",
    "            question: 用户提出的问题\n",
    "            answer: 助手的回答\n",
    "        \n",
    "        返回:\n",
    "            格式化后的对话文本\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 将输入的问题和回答转换为模型训练时使用的格式\n",
    "        - 确保推理时的格式与训练时一致，这对模型性能很重要\n",
    "        \"\"\"\n",
    "        return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{answer}<|eot_id|>\"\n",
    "    \n",
    "    async def __call__(self, request):\n",
    "        \"\"\"\n",
    "        处理推理请求的主要方法\n",
    "        \n",
    "        这个方法是整个服务的核心，处理所有的HTTP请求\n",
    "        \n",
    "        参数:\n",
    "            request: HTTP请求对象，包含question、chosen、rejected字段\n",
    "        \n",
    "        返回:\n",
    "            包含分数结果的字典\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 这是服务的主要入口点，所有的推理请求都会调用这个方法\n",
    "        - 接收用户的问题和两个候选回答，返回哪个回答更好的评分\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 处理健康检查请求\n",
    "            # 健康检查用于监控服务是否正常运行\n",
    "            if hasattr(request, 'url') and request.url and \"health\" in str(request.url):\n",
    "                return {\n",
    "                    \"status\": \"healthy\",\n",
    "                    \"model_path\": self.model_path,\n",
    "                    \"device\": str(self.device),\n",
    "                    \"tokenizer_type\": str(type(self.tokenizer)),\n",
    "                    \"node_id\": ray.get_runtime_context().get_node_id()\n",
    "                }\n",
    "            \n",
    "            # 解析请求数据\n",
    "            # 支持两种请求格式：HTTP请求和直接的字典数据\n",
    "            if hasattr(request, 'json'):\n",
    "                data = await request.json()\n",
    "            else:\n",
    "                data = request\n",
    "            \n",
    "            # 提取请求中的三个关键字段\n",
    "            question = data[\"question\"]    # 用户问题\n",
    "            chosen = data[\"chosen\"]        # 人类偏好的回答（好回答）\n",
    "            rejected = data[\"rejected\"]    # 人类不偏好的回答（差回答）\n",
    "            \n",
    "            # 将问题和回答格式化为模型输入格式\n",
    "            chosen_text = self.format_conversation(question, chosen)\n",
    "            rejected_text = self.format_conversation(question, rejected)\n",
    "            \n",
    "            logger.info(f\"开始推理 - tokenizer类型: {type(self.tokenizer)}\")\n",
    "            \n",
    "            # 验证tokenizer状态\n",
    "            # 这些检查确保tokenizer已经正确初始化\n",
    "            if self.tokenizer is None:\n",
    "                error_msg = \"tokenizer未正确初始化\"\n",
    "                logger.error(error_msg)\n",
    "                return {\"error\": error_msg}\n",
    "            \n",
    "            if not callable(self.tokenizer):\n",
    "                error_msg = f\"tokenizer不可调用，类型: {type(self.tokenizer)}\"\n",
    "                logger.error(error_msg)\n",
    "                return {\"error\": error_msg}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 执行模型推理\n",
    "            # torch.no_grad()用于关闭梯度计算，节省内存和计算资源\n",
    "            with torch.no_grad():\n",
    "                # 对chosen回答进行tokenization（文本转数字）\n",
    "                chosen_inputs = self.tokenizer(\n",
    "                    chosen_text, \n",
    "                    return_tensors=\"pt\",    # 返回PyTorch张量\n",
    "                    max_length=2048,        # 最大序列长度\n",
    "                    truncation=True,        # 如果超长则截断\n",
    "                    padding=True            # 填充到指定长度\n",
    "                ).to(self.device)           # 移动到GPU设备\n",
    "                \n",
    "                # 对rejected回答进行tokenization\n",
    "                rejected_inputs = self.tokenizer(\n",
    "                    rejected_text, \n",
    "                    return_tensors=\"pt\", \n",
    "                    max_length=2048, \n",
    "                    truncation=True, \n",
    "                    padding=True\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # 使用模型进行推理，获取chosen回答的分数\n",
    "                chosen_outputs = self.model(**chosen_inputs)\n",
    "                # 使用模型进行推理，获取rejected回答的分数\n",
    "                rejected_outputs = self.model(**rejected_inputs)\n",
    "                \n",
    "                # 记录模型输出的形状和内容，用于调试\n",
    "                logger.info(f\"chosen_outputs.logits shape: {chosen_outputs.logits.shape}\")\n",
    "                logger.info(f\"chosen_outputs.logits: {chosen_outputs.logits}\")\n",
    "                \n",
    "                # 提取分数\n",
    "                # [0, 0]表示取第一个样本的第一个输出值\n",
    "                chosen_score = float(chosen_outputs.logits[0, 0].cpu().item())\n",
    "                rejected_score = float(rejected_outputs.logits[0, 0].cpu().item())\n",
    "            \n",
    "            # 计算处理时间\n",
    "            processing_time = time.time() - start_time\n",
    "            # 计算偏好强度（chosen分数与rejected分数的差值）\n",
    "            preference_strength = chosen_score - rejected_score\n",
    "            # 根据分数高低判断预测结果\n",
    "            prediction = \"chosen\" if chosen_score > rejected_score else \"rejected\"\n",
    "            \n",
    "            logger.info(f\"推理完成 - chosen: {chosen_score:.4f}, rejected: {rejected_score:.4f}\")\n",
    "            \n",
    "            # 返回推理结果\n",
    "            return {\n",
    "                \"chosen_score\": chosen_score,              # chosen回答的分数\n",
    "                \"rejected_score\": rejected_score,          # rejected回答的分数\n",
    "                \"preference_strength\": preference_strength, # 偏好强度\n",
    "                \"prediction\": prediction,                  # 预测结果\n",
    "                \"processing_time\": processing_time,        # 处理时间\n",
    "                \"node_id\": ray.get_runtime_context().get_node_id()  # 处理节点ID\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"推理错误: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(f\"错误堆栈: {traceback.format_exc()}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "def deploy_service(args):\n",
    "    \"\"\"\n",
    "    部署Ray Serve服务\n",
    "    \n",
    "    这个函数负责启动和配置Ray Serve服务\n",
    "    \n",
    "    参数:\n",
    "        args: 命令行参数对象，包含模型路径、副本数量、端口等配置\n",
    "    \n",
    "    返回:\n",
    "        True: 部署成功\n",
    "        False: 部署失败\n",
    "    \n",
    "    在整个系统中的作用：\n",
    "    - 这是服务部署的核心函数，负责启动Ray集群和部署推理服务\n",
    "    - 配置服务的副本数量、资源分配等参数\n",
    "    \"\"\"\n",
    "    # 检查Ray是否已经初始化，如果没有则连接到现有集群\n",
    "    if not ray.is_initialized():\n",
    "        try:\n",
    "            # 连接到已经启动的Ray集群\n",
    "            ray.init(address=\"auto\", ignore_reinit_error=True)\n",
    "            logger.info(\"连接到Ray集群成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"连接Ray集群失败: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # 获取集群资源信息\n",
    "    cluster_resources = ray.cluster_resources()\n",
    "    total_gpus = int(cluster_resources.get(\"GPU\", 0))\n",
    "    # 确保副本数量不超过可用GPU数量\n",
    "    num_replicas = min(args.num_replicas, total_gpus)\n",
    "    \n",
    "    logger.info(f\"部署配置: {num_replicas} 个副本\")\n",
    "    \n",
    "    try:\n",
    "        # 启动Ray Serve服务\n",
    "        serve.start(\n",
    "            detached=True,  # 后台运行\n",
    "            http_options={\n",
    "                \"host\": \"0.0.0.0\",  # 监听所有网络接口\n",
    "                \"port\": args.port   # 指定端口\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 配置服务部署选项\n",
    "        app = RewardService.options(\n",
    "            num_replicas=num_replicas,  # 副本数量\n",
    "            ray_actor_options={\n",
    "                \"num_gpus\": 1,  # 每个副本使用1个GPU\n",
    "                \"num_cpus\": 2   # 每个副本使用2个CPU核心\n",
    "            }\n",
    "        ).bind(args.model_path)  # 绑定模型路径参数\n",
    "        \n",
    "        # 运行服务，设置路由前缀\n",
    "        serve.run(app, route_prefix=\"/\")\n",
    "        \n",
    "        logger.info(f\"服务部署成功: http://0.0.0.0:{args.port}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"服务部署失败: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，程序的入口点\n",
    "    \n",
    "    这个函数负责：\n",
    "    1. 解析命令行参数\n",
    "    2. 启动服务\n",
    "    3. 处理信号和保持服务运行\n",
    "    \n",
    "    在整个系统中的作用：\n",
    "    - 这是整个服务的启动入口\n",
    "    - 处理命令行参数，启动服务，并保持服务持续运行\n",
    "    \"\"\"\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser(description=\"奖励模型推理服务\")\n",
    "    parser.add_argument(\"--model_path\", required=True, help=\"模型路径\")\n",
    "    parser.add_argument(\"--num_replicas\", type=int, default=2, help=\"副本数量\")\n",
    "    parser.add_argument(\"--port\", type=int, default=8000, help=\"服务端口\")\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    logger.info(\"启动奖励模型服务\")\n",
    "    logger.info(f\"模型路径: {args.model_path}\")\n",
    "    logger.info(f\"副本数量: {args.num_replicas}\")\n",
    "    logger.info(f\"服务端口: {args.port}\")\n",
    "    \n",
    "    # 尝试部署服务\n",
    "    if deploy_service(args):\n",
    "        logger.info(\"服务启动成功，保持运行状态\")\n",
    "        \n",
    "        # 定义信号处理函数，用于优雅地关闭服务\n",
    "        def signal_handler(sig, frame):\n",
    "            logger.info(\"收到停止信号，正在关闭服务\")\n",
    "            serve.shutdown()\n",
    "            sys.exit(0)\n",
    "        \n",
    "        # 注册信号处理器，处理Ctrl+C和终止信号\n",
    "        signal.signal(signal.SIGINT, signal_handler)\n",
    "        signal.signal(signal.SIGTERM, signal_handler)\n",
    "        \n",
    "        try:\n",
    "            # 保持服务运行，每60秒检查一次\n",
    "            while True:\n",
    "                time.sleep(60)\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"服务被用户中断\")\n",
    "            serve.shutdown()\n",
    "    else:\n",
    "        logger.error(\"服务启动失败\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# 如果这个文件被直接运行（而不是被导入），则执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230dc7bd",
   "metadata": {},
   "source": [
    "执行脚本 start_ray_cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e77b02",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# 设置脚本在遇到错误时立即退出\n",
    "set -e\n",
    "\n",
    "# 定义集群节点IP地址\n",
    "# 这些IP地址应该根据实际的集群配置进行修改\n",
    "HEAD_NODE=\"10.60.11.131\"    # 头节点IP地址\n",
    "WORKER_NODE=\"10.60.240.249\" # 工作节点IP地址\n",
    "\n",
    "# 检查命令行参数数量\n",
    "# 这个脚本至少需要一个参数（模型路径）\n",
    "if [ $# -lt 1 ]; then\n",
    "    echo \"用法: $0 <模型路径> [副本数] [端口]\"\n",
    "    echo \"示例: $0 output/checkpoint-700 2 8000\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# 解析命令行参数\n",
    "MODEL_PATH=$1              # 第一个参数：模型路径\n",
    "NUM_REPLICAS=${2:-2}       # 第二个参数：副本数量，默认为2\n",
    "PORT=${3:-8000}            # 第三个参数：服务端口，默认为8000\n",
    "\n",
    "# 将相对路径转换为绝对路径\n",
    "# 这确保了无论在哪个目录执行脚本，路径都是正确的\n",
    "MODEL_PATH=$(realpath \"$MODEL_PATH\")\n",
    "\n",
    "echo \"启动验证服务\"\n",
    "echo \"模型路径: $MODEL_PATH\"\n",
    "echo \"副本数: $NUM_REPLICAS\"\n",
    "echo \"端口: $PORT\"\n",
    "\n",
    "# 检查模型路径和必要文件是否存在\n",
    "# 这些检查确保模型文件完整，避免启动后才发现文件缺失\n",
    "if [ ! -d \"$MODEL_PATH\" ]; then\n",
    "    echo \"错误: 模型路径不存在: $MODEL_PATH\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if [ ! -f \"$MODEL_PATH/config.json\" ]; then\n",
    "    echo \"错误: 模型配置文件不存在: $MODEL_PATH/config.json\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if [ ! -f \"$MODEL_PATH/tokenizer_config.json\" ]; then\n",
    "    echo \"错误: tokenizer配置文件不存在: $MODEL_PATH/tokenizer_config.json\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# 停止现有的服务进程\n",
    "# 这确保了不会有端口冲突或资源冲突\n",
    "echo \"停止现有服务\"\n",
    "# 在本地停止ray_reward_service进程\n",
    "pkill -f \"ray_reward_service\" 2>/dev/null || true\n",
    "# 在工作节点停止ray进程\n",
    "ssh ubuntu@$WORKER_NODE \"pkill -f ray 2>/dev/null || true\" 2>/dev/null || true\n",
    "\n",
    "# 启动Ray集群\n",
    "echo \"启动Ray集群\"\n",
    "# 激活conda环境\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate vllm\n",
    "\n",
    "# 强制停止现有的Ray进程，然后启动头节点\n",
    "ray stop --force 2>/dev/null || true\n",
    "ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --num-gpus=4 --num-cpus=24\n",
    "\n",
    "# 启动工作节点\n",
    "echo \"连接工作节点\"\n",
    "# 通过SSH连接到工作节点并启动Ray worker\n",
    "ssh ubuntu@$WORKER_NODE \"\n",
    "    source ~/miniconda3/etc/profile.d/conda.sh\n",
    "    conda activate vllm\n",
    "    ray stop --force 2>/dev/null || true\n",
    "    ray start --address='$HEAD_NODE:6379' --num-gpus=2\n",
    "\" 2>/dev/null || echo \"工作节点连接失败，继续单节点运行\"\n",
    "\n",
    "# 等待集群初始化完成\n",
    "sleep 5\n",
    "\n",
    "# 检查集群状态\n",
    "# 这个命令会显示集群中的节点数量、资源等信息\n",
    "echo \"检查集群状态\"\n",
    "ray status\n",
    "\n",
    "# 启动推理服务\n",
    "echo \"启动推理服务\"\n",
    "# 在后台启动Python推理服务，并将输出重定向到日志文件\n",
    "python inference/ray_reward_service.py \\\n",
    "    --model_path \"$MODEL_PATH\" \\\n",
    "    --num_replicas \"$NUM_REPLICAS\" \\\n",
    "    --port \"$PORT\" > validation_service.log 2>&1 &\n",
    "\n",
    "# 获取服务进程ID并保存到文件\n",
    "SERVICE_PID=$!\n",
    "echo $SERVICE_PID > validation_service.pid\n",
    "\n",
    "echo \"服务启动完成\"\n",
    "echo \"PID: $SERVICE_PID\"\n",
    "echo \"API地址: http://localhost:$PORT\"\n",
    "\n",
    "# 等待服务启动并检查服务状态\n",
    "echo \"等待服务启动\"\n",
    "for i in {1..20}; do\n",
    "    echo \"检查服务状态 ($i/20)\"\n",
    "    \n",
    "    # 检查服务进程是否还在运行\n",
    "    # kill -0 用于检查进程是否存在，不会实际终止进程\n",
    "    if ! kill -0 $SERVICE_PID 2>/dev/null; then\n",
    "        echo \"错误: 服务进程已退出\"\n",
    "        echo \"查看日志:\"\n",
    "        cat validation_service.log\n",
    "        exit 1\n",
    "    fi\n",
    "    \n",
    "    # 等待一段时间再检查健康状态\n",
    "    # 给服务一些时间来完成初始化\n",
    "    sleep 10\n",
    "    \n",
    "    # 检查服务的健康状态\n",
    "    # 通过发送HTTP请求到健康检查端点来验证服务是否正常\n",
    "    if curl -s --connect-timeout 5 --max-time 10 \"http://localhost:$PORT/health\" >/dev/null 2>&1; then\n",
    "        echo \"服务连接正常\"\n",
    "        # 获取并格式化显示健康检查结果\n",
    "        curl -s \"http://localhost:$PORT/health\" | python -m json.tool 2>/dev/null || echo \"健康检查通过\"\n",
    "        break\n",
    "    else\n",
    "        echo \"服务尚未就绪，继续等待...\"\n",
    "        # 如果达到最大重试次数，报告超时错误\n",
    "        if [ $i -eq 20 ]; then\n",
    "            echo \"错误: 服务启动超时\"\n",
    "            echo \"查看日志:\"\n",
    "            tail -50 validation_service.log\n",
    "            kill $SERVICE_PID 2>/dev/null || true\n",
    "            exit 1\n",
    "        fi\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"验证服务已就绪\"\n",
    "echo \"日志文件: validation_service.log\"\n",
    "echo \"停止命令: kill $SERVICE_PID\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd40799",
   "metadata": {},
   "source": [
    "## 执行命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa080e",
   "metadata": {},
   "source": [
    "bash inference/start_ray_cluster.sh /shared/Skywork-Reward-Llama-3.1-8B/ 3 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b68ae1",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624175720219.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff2b0b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625113301838.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193181ad",
   "metadata": {},
   "source": [
    "启动后GPU查看"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc09051",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624164920253.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d65606",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624164934916.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6ef07",
   "metadata": {},
   "source": [
    "停止命令 kill $(cat validation_service.pid)  注意路径 /shared/financial_reward_model下执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74010f8b",
   "metadata": {},
   "source": [
    "debug 测试文件 debug_response.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8aef7",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# 设置API服务的地址\n",
    "# 这个地址应该与启动的Ray Serve服务地址一致\n",
    "api_url = \"http://localhost:8000\"\n",
    "\n",
    "# 定义一个简单的测试用例\n",
    "# 这个测试用例包含了奖励模型推理所需的三个基本字段\n",
    "simple_test = {\n",
    "    \"question\": \"Hello\",      # 用户问题\n",
    "    \"chosen\": \"Hi there!\",    # 人类偏好的回答（应该得到更高分数）\n",
    "    \"rejected\": \"No.\"         # 人类不偏好的回答（应该得到更低分数）\n",
    "}\n",
    "\n",
    "print(\"发送简单测试请求...\")\n",
    "try:\n",
    "    # 发送POST请求到推理服务\n",
    "    # timeout=30表示30秒超时\n",
    "    response = requests.post(f\"{api_url}/\", json=simple_test, timeout=30)\n",
    "    print(f\"状态码: {response.status_code}\")\n",
    "    print(f\"响应内容: {response.text}\")\n",
    "    \n",
    "    # 检查HTTP状态码是否为200（成功）\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            # 尝试解析JSON响应\n",
    "            result = response.json()\n",
    "            print(\"JSON解析成功:\")\n",
    "            # 格式化输出JSON结果，便于阅读\n",
    "            print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}\")\n",
    "    else:\n",
    "        print(f\"HTTP错误: {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    # 捕获所有异常，包括网络连接错误、超时等\n",
    "    print(f\"请求失败: {e}\")\n",
    "    sys.exit(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f54aa6",
   "metadata": {},
   "source": [
    "执行命令 python inference/debug_response.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f773f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625104816324.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d18b",
   "metadata": {},
   "source": [
    "批量测试 test_ray_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a046fc",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "class ModelValidator:\n",
    "    \"\"\"\n",
    "    奖励模型验证器类\n",
    "    \n",
    "    这个类是整个验证系统的核心，负责：\n",
    "    1. 连接到Ray推理服务\n",
    "    2. 加载测试数据\n",
    "    3. 并发执行推理请求\n",
    "    4. 统计和分析结果\n",
    "    \n",
    "    在整个系统中的作用：\n",
    "    - 作为客户端，向Ray推理服务发送验证请求\n",
    "    - 评估奖励模型的准确率和性能\n",
    "    - 生成详细的验证报告\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_url, max_workers=8):\n",
    "        \"\"\"\n",
    "        初始化验证器\n",
    "        \n",
    "        参数:\n",
    "            api_url: Ray推理服务的API地址\n",
    "            max_workers: 最大并发线程数，用于并行发送请求\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 设置验证器的基本配置\n",
    "        - 初始化HTTP会话和统计数据结构\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.max_workers = max_workers\n",
    "        # 创建HTTP会话，复用连接以提高性能\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # 初始化统计数据结构\n",
    "        # 这些统计数据用于跟踪验证过程中的各种指标\n",
    "        self.stats = {\n",
    "            'total': 0,                    # 总样本数\n",
    "            'success': 0,                  # 成功请求数\n",
    "            'failed': 0,                   # 失败请求数\n",
    "            'correct': 0,                  # 正确预测数\n",
    "            'times': [],                   # 响应时间列表\n",
    "            'preference_strengths': []     # 偏好强度列表\n",
    "        }\n",
    "        # 线程锁，用于保护统计数据的并发访问\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def test_connection(self):\n",
    "        \"\"\"\n",
    "        测试与API服务的连接\n",
    "        \n",
    "        这个方法通过发送健康检查请求来验证服务是否可用\n",
    "        \n",
    "        返回:\n",
    "            True: 连接正常\n",
    "            False: 连接失败\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 在开始验证之前确保服务可用\n",
    "        - 获取服务的基本信息（模型路径、设备等）\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 发送健康检查请求\n",
    "            # /health端点返回服务的状态信息\n",
    "            response = self.session.get(f\"{self.api_url}/health\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                health = response.json()\n",
    "                print(\"API连接正常\")\n",
    "                print(f\"模型路径: {health.get('model_path', 'Unknown')}\")\n",
    "                print(f\"设备: {health.get('device', 'Unknown')}\")\n",
    "                print(f\"节点ID: {health.get('node_id', 'Unknown')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"API响应异常: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"连接失败: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_test_data(self, max_samples=None):\n",
    "        \"\"\"\n",
    "        加载测试数据\n",
    "        \n",
    "        这个方法从固定路径加载preference数据集\n",
    "        \n",
    "        参数:\n",
    "            max_samples: 最大样本数量，如果指定则随机采样\n",
    "        \n",
    "        返回:\n",
    "            测试数据列表，每个元素包含question、chosen、rejected字段\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 提供验证所需的标准测试数据\n",
    "        - 支持采样功能，可以控制验证规模\n",
    "        \"\"\"\n",
    "        # 固定的测试数据路径\n",
    "        # 这个路径指向预处理好的preference数据集\n",
    "        test_file = \"/shared/reward_model_data/reward_data/test/preference_dataset.jsonl\"\n",
    "        \n",
    "        # 检查测试文件是否存在\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"测试数据文件不存在: {test_file}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"加载测试数据: {test_file}\")\n",
    "        \n",
    "        data = []\n",
    "        # 逐行读取JSONL文件\n",
    "        # 每行是一个JSON对象，包含一个测试样本\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                # 验证数据格式，确保包含必需的字段\n",
    "                if all(k in item for k in ['question', 'chosen', 'rejected']):\n",
    "                    data.append(item)\n",
    "        \n",
    "        # 如果指定了最大样本数，进行随机采样\n",
    "        if max_samples and len(data) > max_samples:\n",
    "            data = random.sample(data, max_samples)\n",
    "            print(f\"随机采样 {max_samples} 条数据\")\n",
    "        \n",
    "        print(f\"测试数据加载完成: {len(data)} 条\")\n",
    "        return data\n",
    "    \n",
    "    def single_test(self, item):\n",
    "        \"\"\"\n",
    "        执行单个测试样本的推理\n",
    "        \n",
    "        这个方法向API发送单个推理请求并处理响应\n",
    "        \n",
    "        参数:\n",
    "            item: 包含question、chosen、rejected的测试样本\n",
    "        \n",
    "        返回:\n",
    "            包含推理结果和统计信息的字典\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 这是验证的核心方法，每个测试样本都会调用这个方法\n",
    "        - 负责发送HTTP请求、解析响应、计算准确性\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 发送POST请求到推理服务\n",
    "            # 请求体包含question、chosen、rejected三个字段\n",
    "            response = self.session.post(\n",
    "                f\"{self.api_url}/\",\n",
    "                json={\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"chosen\": item[\"chosen\"], \n",
    "                    \"rejected\": item[\"rejected\"]\n",
    "                },\n",
    "                timeout=30  # 30秒超时\n",
    "            )\n",
    "            \n",
    "            # 计算响应时间\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # 检查HTTP状态码\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # 提取推理结果\n",
    "                chosen_score = result.get(\"chosen_score\", 0)\n",
    "                rejected_score = result.get(\"rejected_score\", 0)\n",
    "                # 判断预测是否正确\n",
    "                # 正确的预测应该是chosen_score > rejected_score\n",
    "                is_correct = chosen_score > rejected_score\n",
    "                # 计算偏好强度（分数差的绝对值）\n",
    "                preference_strength = abs(chosen_score - rejected_score)\n",
    "                \n",
    "                # 线程安全地更新统计数据\n",
    "                with self.lock:\n",
    "                    self.stats['success'] += 1\n",
    "                    self.stats['times'].append(elapsed)\n",
    "                    if is_correct:\n",
    "                        self.stats['correct'] += 1\n",
    "                    self.stats['preference_strengths'].append(preference_strength)\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'correct': is_correct,\n",
    "                    'time': elapsed,\n",
    "                    'chosen_score': chosen_score,\n",
    "                    'rejected_score': rejected_score,\n",
    "                    'preference_strength': preference_strength\n",
    "                }\n",
    "            else:\n",
    "                # HTTP错误情况\n",
    "                with self.lock:\n",
    "                    self.stats['failed'] += 1\n",
    "                return {\n",
    "                    'success': False, \n",
    "                    'error': f\"HTTP {response.status_code}\",\n",
    "                    'time': elapsed\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            # 异常情况（网络错误、超时等）\n",
    "            elapsed = time.time() - start_time\n",
    "            with self.lock:\n",
    "                self.stats['failed'] += 1\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e)[:100],  # 限制错误信息长度\n",
    "                'time': elapsed\n",
    "            }\n",
    "    \n",
    "    def run_validation(self, test_data):\n",
    "        \"\"\"\n",
    "        运行完整的验证流程\n",
    "        \n",
    "        这个方法是验证的主要入口，负责并发执行所有测试样本\n",
    "        \n",
    "        参数:\n",
    "            test_data: 测试数据列表\n",
    "        \n",
    "        返回:\n",
    "            包含完整验证结果的字典\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 协调整个验证过程\n",
    "        - 管理并发执行\n",
    "        - 生成最终的验证报告\n",
    "        \"\"\"\n",
    "        print(f\"开始验证，共 {len(test_data)} 条数据\")\n",
    "        print(f\"并发数: {self.max_workers}\")\n",
    "        \n",
    "        self.stats['total'] = len(test_data)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # 使用线程池并发执行推理请求\n",
    "        # 这可以显著提高验证速度\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # 提交所有任务到线程池\n",
    "            future_to_item = {\n",
    "                executor.submit(self.single_test, item): item \n",
    "                for item in test_data\n",
    "            }\n",
    "            \n",
    "            # 使用进度条显示验证进度\n",
    "            with tqdm(total=len(test_data), desc=\"验证进度\") as pbar:\n",
    "                # 等待任务完成并收集结果\n",
    "                for future in as_completed(future_to_item):\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    # 实时更新进度条显示的统计信息\n",
    "                    if self.stats['success'] > 0:\n",
    "                        accuracy = self.stats['correct'] / self.stats['success'] * 100\n",
    "                        pbar.set_postfix({\n",
    "                            'accuracy': f\"{accuracy:.1f}%\",\n",
    "                            'success': self.stats['success'],\n",
    "                            'failed': self.stats['failed']\n",
    "                        })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        # 打印详细的验证报告\n",
    "        self.print_report(results, total_time)\n",
    "        \n",
    "        # 返回汇总结果\n",
    "        return {\n",
    "            'total_samples': len(test_data),\n",
    "            'successful_requests': self.stats['success'],\n",
    "            'failed_requests': self.stats['failed'],\n",
    "            'accuracy': self.stats['correct'] / max(self.stats['success'], 1) * 100,\n",
    "            'avg_response_time': statistics.mean(self.stats['times']) if self.stats['times'] else 0,\n",
    "            'avg_preference_strength': statistics.mean(self.stats['preference_strengths']) if self.stats['preference_strengths'] else 0,\n",
    "            'total_time': total_time\n",
    "        }\n",
    "    \n",
    "    def print_report(self, results, total_time):\n",
    "        \"\"\"\n",
    "        打印详细的验证报告\n",
    "        \n",
    "        这个方法生成并显示完整的验证统计报告\n",
    "        \n",
    "        参数:\n",
    "            results: 所有测试结果的列表\n",
    "            total_time: 总验证时间\n",
    "        \n",
    "        在整个系统中的作用：\n",
    "        - 提供人类可读的验证结果\n",
    "        - 显示关键的性能和准确性指标\n",
    "        \"\"\"\n",
    "        print(\"\\n验证报告\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 基本统计信息\n",
    "        print(f\"总样本数: {self.stats['total']}\")\n",
    "        print(f\"成功请求: {self.stats['success']}\")\n",
    "        print(f\"失败请求: {self.stats['failed']}\")\n",
    "        print(f\"成功率: {self.stats['success']/self.stats['total']*100:.1f}%\")\n",
    "        \n",
    "        # 如果有成功的请求，显示详细统计\n",
    "        if self.stats['success'] > 0:\n",
    "            # 计算模型准确率\n",
    "            # 这是最重要的指标，表示模型正确判断chosen > rejected的比例\n",
    "            accuracy = self.stats['correct'] / self.stats['success'] * 100\n",
    "            print(f\"模型准确率: {accuracy:.2f}%\")\n",
    "            \n",
    "            # 性能统计\n",
    "            times = self.stats['times']\n",
    "            print(f\"\\n性能统计:\")\n",
    "            print(f\"  平均响应时间: {statistics.mean(times):.3f}s\")\n",
    "            print(f\"  最快响应时间: {min(times):.3f}s\")\n",
    "            print(f\"  最慢响应时间: {max(times):.3f}s\")\n",
    "            print(f\"  总耗时: {total_time:.2f}s\")\n",
    "            print(f\"  吞吐量: {self.stats['success']/total_time:.1f} 请求/秒\")\n",
    "            \n",
    "            # 偏好强度统计\n",
    "            # 偏好强度反映模型对判断的置信度\n",
    "            if self.stats['preference_strengths']:\n",
    "                strengths = self.stats['preference_strengths']\n",
    "                print(f\"\\n偏好强度统计:\")\n",
    "                print(f\"  平均偏好强度: {statistics.mean(strengths):.4f}\")\n",
    "                print(f\"  最大偏好强度: {max(strengths):.4f}\")\n",
    "                print(f\"  最小偏好强度: {min(strengths):.4f}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，程序的入口点\n",
    "    \n",
    "    这个函数负责：\n",
    "    1. 解析命令行参数\n",
    "    2. 创建验证器实例\n",
    "    3. 执行验证流程\n",
    "    4. 保存结果\n",
    "    \n",
    "    在整个系统中的作用：\n",
    "    - 这是验证程序的启动入口\n",
    "    - 协调整个验证流程的执行\n",
    "    \"\"\"\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser(description=\"奖励模型验证\")\n",
    "    parser.add_argument(\"--api_url\", type=str, default=\"http://localhost:8000\", \n",
    "                       help=\"API服务地址\")\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=None, \n",
    "                       help=\"最大测试样本数\")\n",
    "    parser.add_argument(\"--max_workers\", type=int, default=8, \n",
    "                       help=\"并发线程数\")\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 创建验证器实例\n",
    "    validator = ModelValidator(args.api_url, args.max_workers)\n",
    "    \n",
    "    # 测试API连接\n",
    "    if not validator.test_connection():\n",
    "        print(\"无法连接到API服务\")\n",
    "        print(\"请确保Ray服务已启动\")\n",
    "        return\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_data = validator.load_test_data(args.max_samples)\n",
    "    if not test_data:\n",
    "        print(\"无法加载测试数据\")\n",
    "        return\n",
    "    \n",
    "    # 执行验证\n",
    "    results = validator.run_validation(test_data)\n",
    "    \n",
    "    # 保存验证结果到文件\n",
    "    result_file = f\"validation_results_{int(time.time())}.json\"\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"验证结果已保存到: {result_file}\")\n",
    "\n",
    "# 如果这个文件被直接运行（而不是被导入），则执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac5547",
   "metadata": {},
   "source": [
    "批量执行命令 python inference/test_ray_service.py --max_samples 1200 --max_workers 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39837e",
   "metadata": {},
   "source": [
    "原生模型 Skywork-Reward-Llama-3.1-8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a50c26",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624175814933.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff2a2a",
   "metadata": {},
   "source": [
    "checkpoint-700\n",
    "\n",
    "python inference/test_ray_service.py --max_samples 1200 --max_workers 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff6a69",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624164441990.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d3971",
   "metadata": {},
   "source": [
    "bestmodel 执行 1200条测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b502573",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250624193213400.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2aaf8",
   "metadata": {},
   "source": [
    "checkpoint-1000 执行 1200条测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eb9e3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625001413129.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a40448",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625001948278.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c525f",
   "metadata": {},
   "source": [
    "# 6.总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4dd9b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250625214102392.png\" width=100%></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
