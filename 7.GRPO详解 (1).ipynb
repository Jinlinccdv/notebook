{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe98a9e5-4646-47d0-9445-19bf22ef3a54",
   "metadata": {},
   "source": [
    "# <center>Ch7 GRPO详解 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54353bad",
   "metadata": {},
   "source": [
    "# 背景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19ca38",
   "metadata": {},
   "source": [
    "### 1. 解决大模型微调的核心痛点\n",
    "- **传统RLHF的局限性**：  \n",
    "  - PPO等经典方法在大模型微调中存在 **高方差、训练不稳定** 的问题，尤其在生成长序列（如多步推理）时，稀疏奖励（仅最终结果打分）难以有效优化中间步骤。  \n",
    "  - **GRPO的改进**：  \n",
    "    - 通过 **分组（Group）策略** 和 **相对优势计算**，显著降低方差，提升策略更新的稳定性。  \n",
    "    - 例如：在数学推理中，将不同推导路径分组，通过组间比较优化，避免无效探索。\n",
    "\n",
    "### 2. 技术优势的显性化\n",
    "- **细粒度策略优化**：  \n",
    "  - GRPO通过 **组内协作+组间竞争** 的机制（类似多智能体强化学习），能更好处理多任务、多模态场景。  \n",
    "  - 例如：在代码生成中，将不同功能的代码块分组优化，提升生成逻辑的连贯性。\n",
    "\n",
    "- **计算效率提升**：  \n",
    "  - 传统PPO需大量样本才能稳定训练，而GRPO通过分组减少冗余计算（如共享组内经验），更适合超大模型微调。\n",
    "\n",
    "- **对齐（Alignment）需求的爆发**：  \n",
    "  - 随着ChatGPT等大模型落地，**安全性和可控性**成为焦点。GRPO的分组设计可灵活整合人类偏好（如将“安全响应”和“创造性响应”分组优化），平衡多目标冲突。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489009b",
   "metadata": {},
   "source": [
    "# 1.GRPO技术解析\n",
    "\n",
    "## 什么是GRPO\n",
    "\n",
    "GRPO (Group Relative Policy Optimization 群体相关策略优化 )是一种针对大型语言模型的强化学习微调方法，是对传统PPO算法的改进和扩展。\n",
    "\n",
    "### 核心架构组成\n",
    "- **策略模型**：被训练的语言模型，初始通常是SFT模型\n",
    "- **参考模型**：固定的初始模型，用于计算KL散度约束\n",
    "- **奖励模型**：评估生成文本质量的模型，提供奖励信号\n",
    "- **组计算模块**：核心创新部分，进行多样本生成和优势计算\n",
    "\n",
    "\n",
    "GRPO的核心特点包括：\n",
    "\n",
    "1. **基于组计算的优势估计**：对每个输入问题生成多个候选回答，通过组内比较计算优势函数\n",
    "2. **双层KL控制机制**：通过微观和宏观两个层面的KL散度约束，保持模型稳定性\n",
    "3. **灵活的监督方式**：支持结果监督(Outcome Supervision)和过程监督(Process Supervision)两种模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba731b1",
   "metadata": {},
   "source": [
    "## GRPO的主要优势\n",
    "\n",
    "### 1. 更精细的奖励指导\n",
    "\n",
    "- **差异化强化**：能够根据回答质量的细微差别提供相应强度的训练信号\n",
    "- **惩罚机制**：不仅强化好的回答，还能惩罚质量差的回答\n",
    "- **奖励区分度**：能够区分不同程度的\"正确\"回答，而不是简单的二分法\n",
    "\n",
    "### 2. 训练稳定性提升\n",
    "\n",
    "- **组内标准化**：通过组计算减少奖励估计的方差\n",
    "- **双层KL控制**：避免策略过度偏离初始模型，保持生成文本的语言质量\n",
    "- **无需Value模型**：简化了训练架构，减少了训练不稳定因素\n",
    "\n",
    "### 3. 样本效率提高\n",
    "\n",
    "- **多样本学习**：从每个问题生成的多个回答中同时学习\n",
    "- **更丰富的比较信号**：提供更多对比样本，增强学习信号\n",
    "- **标准化奖励**：减少不同任务难度带来的奖励尺度差异\n",
    "\n",
    "### 4. 灵活的应用适配\n",
    "\n",
    "- **支持不同监督粒度**：可以根据任务特点选择整体评估或逐步评估\n",
    "- **可调整的参数**：G值、KL系数等参数可以根据任务需求调整\n",
    "- **迭代训练支持**：支持多轮迭代强化，持续提升性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997738b",
   "metadata": {},
   "source": [
    "## 适用场景\n",
    "\n",
    "### 1. 需要高质量回答的应用\n",
    "\n",
    "- **客服机器人**：需要提供准确、有帮助且友好的回答\n",
    "- **内容创作助手**：需要生成高质量、符合要求的文本内容\n",
    "- **专业领域问答**：如法律、医疗、技术支持等需要专业准确回答的场景\n",
    "\n",
    "### 2. 多样化输出评估场景\n",
    "\n",
    "- **开放式生成任务**：故事创作、文案生成等有多种可能正确答案的任务\n",
    "- **个性化反馈系统**：需要根据用户偏好调整输出质量的应用\n",
    "- **创意输出场景**：需要评估创意质量而非简单正确性的应用\n",
    "\n",
    "### 3. 需要精细控制的应用\n",
    "\n",
    "- **安全关键应用**：需要严格控制输出内容安全性的场景\n",
    "- **教育辅助工具**：需要根据学习者水平提供适当难度解答的应用\n",
    "- **专业写作辅助**：需要符合特定风格和标准的专业写作场景\n",
    "\n",
    "### 4. 计算资源受限场景\n",
    "\n",
    "- **不使用Value网络**：相比需要额外价值网络的方法更节省资源\n",
    "- **可调整组大小**：可以根据可用计算资源调整G值大小\n",
    "- **灵活的训练方案**：可以根据资源情况选择适当的监督方式\n",
    "\n",
    "## 总结\n",
    "\n",
    "GRPO是一种灵活、高效的强化学习微调方法，通过差异化梯度系数和组计算机制提升了大语言模型的性能。它特别适合需要高质量、多样化输出的应用场景，能够提供比传统方法更精细的模型行为调整能力。其无需Value网络的设计也使其在计算资源受限的情况下仍然可行，是当前大模型微调技术中的重要发展。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0d340",
   "metadata": {},
   "source": [
    "# 2.GRPO流程讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931fd5a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250325225804336.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116167db",
   "metadata": {},
   "source": [
    "## PPO架构（图上半部分）逐元素分析\n",
    "\n",
    "### 1. 输入阶段（左侧）\n",
    "- **输入q**：表示用户提问或指令\n",
    "  - 格式：可能是自然语言文本、结构化查询或其他形式\n",
    "  - 在大语言模型背景下通常是prompt或instruction\n",
    "  - 这是整个生成过程的起点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b5b4e",
   "metadata": {},
   "source": [
    "### 2. Policy Model处理（核心组件）\n",
    "- **策略模型**（图中为黄色标记，表示需要训练）\n",
    "  - 内部结构：通常是Transformer架构的语言模型\n",
    "  - 输入处理：接收q并进行token编码和上下文理解\n",
    "  - 自回归生成：逐token生成输出序列o\n",
    "  - 参数更新：通过优势A指导的策略梯度进行更新\n",
    "  - 决策过程：基于当前上下文计算下一个token的概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d82ca",
   "metadata": {},
   "source": [
    "### 3. 输出处理（三路评估）\n",
    "生成的输出o被发送到三个不同模块进行评估：\n",
    "\n",
    "#### 3.1 Reference Model路径\n",
    "- **参考模型**（图中蓝色，表示参数冻结）\n",
    "  - 本质：通常是SFT（监督微调）后的基础模型\n",
    "  - 功能：计算当前策略与参考行为的偏离程度\n",
    "  - 具体计算：KL散度 $D_{KL}(\\pi_\\theta(·|q) || \\pi_{ref}(·|q))$\n",
    "  - 约束作用：防止策略过度偏离原始能力分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4109ed",
   "metadata": {},
   "source": [
    "#### 3.2 Reward Model路径\n",
    "- **奖励模型**（图中蓝色，表示参数冻结）\n",
    "  - 结构：通常是经过人类偏好训练的判别模型\n",
    "  - 输入：策略模型生成的输出o及原始查询q\n",
    "  - 输出：标量奖励值r，评估输出质量\n",
    "  - 评估角度：可能包括有用性、安全性、真实性等维度\n",
    "  - 结果用途：提供强化学习的主要训练信号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bb1ce",
   "metadata": {},
   "source": [
    "#### 3.3 Value Model路径\n",
    "- **价值模型**（图中黄色，表示需要训练）\n",
    "  - 结构：通常与策略模型类似但输出不同\n",
    "  - 功能：评估当前状态的预期累积奖励\n",
    "  - 输入：与策略模型相同的q和o\n",
    "  - 输出：状态价值估计v\n",
    "  - 结果用途：为优势计算提供基线估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6252461",
   "metadata": {},
   "source": [
    "### 4. GAE（广义优势估计）计算\n",
    "- **优势估计模块**\n",
    "  - 输入：奖励值r和状态价值v\n",
    "  - 功能：综合即时奖励和长期价值估计\n",
    "  - 输出：优势值A，用于策略更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29402969",
   "metadata": {},
   "source": [
    "### 5. 策略更新机制\n",
    "- **利用优势信号**\n",
    "  - PPO具体目标：$L(\\theta) = E_t[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$\n",
    "  - 其中r_t(θ)为新旧策略概率比值：$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$\n",
    "  - 最终更新考虑三方面：优势信号、裁剪目标和KL约束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdcc9e",
   "metadata": {},
   "source": [
    "## GRPO架构（图下半部分）逐元素分析\n",
    "\n",
    "### 1. 输入阶段（左侧）\n",
    "- **输入q**：与PPO相同的用户查询或指令\n",
    "  - 处理方式相同：token化并输入到模型\n",
    "  - 在GRPO中，同一输入将产生多个不同输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d10db",
   "metadata": {},
   "source": [
    "### 2. Policy Model多样化生成（核心创新）\n",
    "- **策略模型**（图中黄色，需要训练）\n",
    "  - 关键区别：生成G个不同输出{o₁, o₂, ..., oG}\n",
    "  - 生成方式：可能采用以下策略\n",
    "    - 不同温度参数（如0.7, 1.0, 1.3等）\n",
    "    - 不同采样策略（top-k、随机种子等）\n",
    "    - 相同参数多次采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fba65",
   "metadata": {},
   "source": [
    "### 3. 并行评估路径\n",
    "\n",
    "#### 3.1 Reference Model路径\n",
    "- **参考模型**（图中蓝色，参数冻结）\n",
    "  - 处理方式：每个输出oᵢ都与参考模型比较\n",
    "  - 计算G组KL散度值\n",
    "  - 每个输出可能有不同的KL预算\n",
    "  - 结果用于约束训练，确保策略不偏离过远"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d3903",
   "metadata": {},
   "source": [
    "#### 3.2 Reward Model路径\n",
    "- **奖励模型**（图中蓝色，参数冻结）\n",
    "  - 处理方式：评估每个输出oᵢ\n",
    "  - 得到G组奖励值：{r₁, r₂, ..., rG}\n",
    "  - 评估可以是：\n",
    "    - 整体评估（结果监督）\n",
    "    - 逐步评估（过程监督）\n",
    "  - 整体评估：每个输出一个总体分数\n",
    "  - 逐步评估：输出的每个步骤都有对应奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3bed8",
   "metadata": {},
   "source": [
    "### 4. Group Computation（GRPO核心机制）\n",
    "- **组计算模块**（相当于替代PPO中的Value Model和GAE）\n",
    "  - 功能：通过组内比较计算每个输出的优势\n",
    "  - 处理步骤：\n",
    "    1. 接收所有G组奖励\n",
    "    2. 进行组内标准化：$\\hat{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}$\n",
    "    3. 根据监督类型计算优势\n",
    "\n",
    "  - **结果监督下的优势计算**：\n",
    "    - 简单直接，无需逐步评估\n",
    "\n",
    "  - **过程监督下的优势计算**：\n",
    "    - 每个token的优势是后续步骤奖励总和\n",
    "    - 这为每个决策点提供更精准的信号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33a2a1",
   "metadata": {},
   "source": [
    "### 5. 多组优势更新机制\n",
    "- **并行优势信号**\n",
    "  - 得到G组优势值：{A₁, A₂, ..., AG}\n",
    "  - 每组优势对应一个完整输出\n",
    "  - 策略更新考虑所有优势信号\n",
    "  - KL约束同样应用，防止策略偏离过远"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41213fc6",
   "metadata": {},
   "source": [
    "## 两种架构的微观差异\n",
    "\n",
    "### 1. 模型训练组成\n",
    "- **PPO**：\n",
    "  - 训练Policy Model和Value Model两个网络\n",
    "  - 两个模型更新目标不同\n",
    "  - 额外的参数和计算开销\n",
    "  \n",
    "- **GRPO**：\n",
    "  - 仅训练单一Policy Model\n",
    "  - 简化了训练复杂度\n",
    "  - 减少了总参数量和内存需求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e2206",
   "metadata": {},
   "source": [
    "### 2. 信息流结构\n",
    "- **PPO**：\n",
    "  - 单输出多路径评估\n",
    "  - 需要Value Model前向+反向传播\n",
    "  - 顺序计算依赖性强\n",
    "  \n",
    "- **GRPO**：\n",
    "  - 多输出并行评估\n",
    "  - 组内比较替代Value估计\n",
    "  - 更适合并行处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12291c",
   "metadata": {},
   "source": [
    "### 3. 采样效率对比\n",
    "- **PPO**：\n",
    "  - 单次迭代生成单个样本\n",
    "  - 需要多次迭代建立足够样本库\n",
    "  - 样本之间可能上下文不同\n",
    "  \n",
    "- **GRPO**：\n",
    "  - 单次迭代生成G个样本\n",
    "  - 样本密度增加G倍\n",
    "  - 所有样本共享相同上下文，便于比较\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc16599",
   "metadata": {},
   "source": [
    "### 4. 奖励标准化机制\n",
    "- **PPO**：\n",
    "  - 通常需要手动设计奖励缩放或标准化\n",
    "  - Value估计和真实奖励可能尺度不匹配\n",
    "  \n",
    "- **GRPO**：\n",
    "  - 自动组内标准化：$\\hat{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}$\n",
    "  - 天然适应不同难度任务和奖励范围\n",
    "  - 减少了超参数调整需求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8102a8a",
   "metadata": {},
   "source": [
    "## 总结对比\n",
    "\n",
    "这张图展示了GRPO通过三个关键创新超越PPO：\n",
    "\n",
    "1. **结构简化**：移除Value Model，减少训练复杂度\n",
    "2. **并行采样**：单次生成多样本，提高采样效率\n",
    "3. **组内比较**：使用内部标准化替代Value估计，提高稳定性\n",
    "\n",
    "这些设计使GRPO在保持简单实现的同时，解决了PPO在大规模语言模型应用中的核心挑战：样本效率低、训练不稳定和计算成本高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bb4d2",
   "metadata": {},
   "source": [
    "# 3.Group Computation机制详解 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6055032",
   "metadata": {},
   "source": [
    "## 基本原理\n",
    "\n",
    "### 核心思想\n",
    "Group Computation的核心思想是：**用同一问题的多个不同回答之间的比较来确定每个回答的相对价值**，而不依赖额外的Value网络。\n",
    "\n",
    "### 理论基础\n",
    "1. **基线减法原理**：在强化学习中，从奖励中减去一个基线可以减少方差，同时不改变期望梯度\n",
    "2. **相对评估原理**：人类难以给出绝对评分，但容易进行相对比较\n",
    "3. **标准化统计原理**：同组数据标准化可以消除不同问题难度带来的尺度差异\n",
    "\n",
    "### 与传统方法的对比\n",
    "- **传统PPO**：需要训练一个Value网络来估计状态价值作为基线\n",
    "- **Group Computation**：使用同组其他样本的统计量作为动态基线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68598c",
   "metadata": {},
   "source": [
    "## 关键步骤\n",
    "\n",
    "### 1. 多样化采样\n",
    "首先，我们有：\n",
    "- 一个用户问题q\n",
    "- 策略模型生成的G个不同输出: {o₁, o₂, ..., oG}\n",
    "- 奖励模型评估的G个奖励: {r₁, r₂, ..., rG}\n",
    "\n",
    "```json\n",
    "问题q: \"解释量子计算的基本原理\"\n",
    "\n",
    "生成的3个输出(简化为G=3):\n",
    "o₁: \"量子计算利用量子比特代替传统比特...\"\n",
    "o₂: \"量子计算是一种使用量子力学现象的计算形式...\"\n",
    "o₃: \"量子计算基于量子力学原理，使用量子叠加和纠缠...\"\n",
    "```\n",
    "\n",
    "**技术细节**：\n",
    "- 使用不同随机种子\n",
    "- 采用不同温度参数\n",
    "- 应用不同的top-p或top-k值\n",
    "- 确保生成的回答足够多样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5cf65",
   "metadata": {},
   "source": [
    "### 2. 奖励评估\n",
    "使用奖励模型对每个回答进行评分，得到G个奖励值：{r₁, r₂, ..., rG}\n",
    "```json\n",
    "\n",
    "假设G=3\n",
    "r₁ = 8.5\n",
    "r₂ = 7.2\n",
    "r₃ = 9.1\n",
    "```\n",
    "\n",
    "**技术细节**：\n",
    "- 奖励可以来自单一奖励模型\n",
    "- 也可以是多个评估指标的组合\n",
    "- 在过程监督中，还需要评估每个推理步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82b569",
   "metadata": {},
   "source": [
    "### 3. 组内标准化\n",
    "将奖励转换为标准分数，消除不同问题难度的影响：\n",
    "\n",
    "**数学公式**:\n",
    "\n",
    "r̂ᵢ = (rᵢ - mean(r)) / std(r)\n",
    "\n",
    "其中:\n",
    "- mean(r)是组内所有奖励的平均值\n",
    "- std(r)是组内奖励的标准差\n",
    "\n",
    "```json\n",
    "mean(r) = (8.5 + 7.2 + 9.1) / 3 = 8.27\n",
    "std(r) = √(((8.5-8.27)² + (7.2-8.27)² + (9.1-8.27)²) / 3) ≈ 0.784\n",
    "\n",
    "标准化奖励:\n",
    "r̂₁ = (8.5 - 8.27) / 0.784 ≈ 0.29\n",
    "r̂₂ = (7.2 - 8.27) / 0.784 ≈ -1.36\n",
    "r̂₃ = (9.1 - 8.27) / 0.784 ≈ 1.06\n",
    "```\n",
    "\n",
    "这一步确保了奖励值在相同尺度上，平均值为0，标准差为1。\n",
    "\n",
    "**标准化效果**：\n",
    "- 平均奖励变为0\n",
    "- 标准差变为1\n",
    "- 高于平均的回答获得正值\n",
    "- 低于平均的回答获得负值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548f38b",
   "metadata": {},
   "source": [
    "## 核心参数\n",
    "\n",
    "### 1. 组大小G\n",
    "- **定义**：每个问题生成的不同回答数量\n",
    "- **常用值**：G = 8 或 G = 16 或者 G=64\n",
    "- **影响**：\n",
    "  * G越大，统计估计越稳定，计算资源消耗越多\n",
    "  * G越小，训练速度越快，（如G≤2）会导致不稳定\n",
    "\n",
    "**理论依据**：\n",
    "- 统计理论表明方差随样本量的增加而减少\n",
    "- 存在明显的边际收益递减"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f1fb4",
   "metadata": {},
   "source": [
    "### 2. 奖励粒度\n",
    "- **结果监督**：每个回答一个总体奖励\n",
    "- **过程监督**：每个推理步骤一个奖励\n",
    "- **混合监督**：结合两种方式的奖励信号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3688e5",
   "metadata": {},
   "source": [
    "## 技术价值\n",
    "\n",
    "### 1. 无需Value模型\n",
    "- **简化架构**：减少了需要训练的模型数量\n",
    "- **降低复杂性**：避免Value模型训练不稳定的问题\n",
    "- **减少错配**：消除了策略模型与Value模型之间可能的不一致\n",
    "\n",
    "### 2. 训练稳定性\n",
    "- **自适应基线**：基线自动调整适应不同难度的问题\n",
    "- **降低方差**：组内标准化减少了单个样本的噪声影响\n",
    "- **鲁棒性**：对奖励尺度变化不敏感\n",
    "\n",
    "### 3. 样本效率\n",
    "- **多重学习信号**：每个问题产生G个学习样本\n",
    "- **相对比较**：从同一问题的不同回答中提取更丰富的信息\n",
    "- **零和学习**：好的回答被强化，差的回答被抑制\n",
    "\n",
    "### 4. 泛化性能\n",
    "- **相对评估**：关注同类问题中的相对表现\n",
    "- **难度中和**：困难问题中的\"相对好\"回答仍获得正优势\n",
    "- **领域适应性**：适用于各种文本生成任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b68de",
   "metadata": {},
   "source": [
    "## 理解Group Computation的核心\n",
    "\n",
    "要真正理解Group Computation的核心，需要从以下几个角度思考：\n",
    "\n",
    "### 1. 统计视角\n",
    "Group Computation本质上是一种**自举式基线估计法**，使用组内统计量作为奖励基线，而不是学习一个单独的Value函数。\n",
    "\n",
    "标准化公式 r̂ᵢ = (rᵢ - mean(r)) / std(r) 的关键作用是：\n",
    "- 消除了不同问题难度带来的奖励尺度差异\n",
    "- 将奖励转换为相对性能度量\n",
    "- 创造了一个自动调整的基线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec2235",
   "metadata": {},
   "source": [
    "### 2. 比较学习视角\n",
    "Group Computation实现了**隐式的比较学习**：\n",
    "\n",
    "- 高于平均的回答获得正优势，被强化\n",
    "- 低于平均的回答获得负优势，被抑制\n",
    "- 系统学习\"什么是相对更好的回答\"，而不只是\"什么是好回答\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020735a",
   "metadata": {},
   "source": [
    "### 3. 算法设计视角\n",
    "从算法设计角度，Group Computation是对传统Actor-Critic架构的巧妙替代：\n",
    "- 传统方法：Actor + Critic(Value)网络\n",
    "- Group Computation：Actor + 组内统计基线\n",
    "\n",
    "这种设计保留了基线减法的方差减少优势，同时避免了Value估计的困难。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15bc65",
   "metadata": {},
   "source": [
    "### 实际案例分析\n",
    "\n",
    "考虑一个简单例子，假设我们有三个问题，每个问题生成G=4个回答：\n",
    "\n",
    "**问题1**（简单问题）：\n",
    "```json\n",
    "回答奖励：{9.5, 9.0, 8.5, 8.0}\n",
    "标准化后：{1.16, 0.39, -0.39, -1.16}\n",
    "```\n",
    "\n",
    "**问题2**（中等问题）：\n",
    "```json\n",
    "回答奖励：{7.0, 6.5, 6.0, 5.5}\n",
    "标准化后：{1.16, 0.39, -0.39, -1.16}\n",
    "```\n",
    "\n",
    "**问题3**（困难问题）：\n",
    "```json\n",
    "回答奖励：{4.5, 4.0, 3.5, 3.0}\n",
    "标准化后：{1.16, 0.39, -0.39, -1.16}\n",
    "```\n",
    "\n",
    "注意到：尽管原始奖励值差异很大，但标准化后的优势值完全相同！这正是Group Computation的强大之处 - 它自动适应不同难度的问题，确保模型学习的是\"相对更好的策略\"，而不受绝对奖励尺度的影响。\n",
    "\n",
    "通过理解这些核心概念和案例，您应该能够更深入地把握Group Computation的本质 - 它不仅仅是一个技术细节，而是GRPO算法的核心创新，是使其在没有Value网络的情况下仍能高效稳定工作的关键机制。\n",
    "\n",
    "Group Computation本质上是一种基于组内比较的优势估计方法，无需额外的Value Model。它有两个主要版本：结果监督(Outcome Supervision)和过程监督(Process Supervision)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ce1b0",
   "metadata": {},
   "source": [
    "# 3.结果监督与过程监督"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc16fe",
   "metadata": {},
   "source": [
    "## 一、结果监督（Outcome Supervision）\n",
    "\n",
    "### 基本原理\n",
    "结果监督就像是**只关注最终成绩**的老师。无论你解题过程如何，只看你最后的答案对不对、好不好。\n",
    "\n",
    "### 工作方式\n",
    "1. 模型针对一个问题生成多个不同回答\n",
    "2. 每个完整回答获得一个总体分数\n",
    "3. 这个回答中的所有单词（tokens）都获得相同的\"奖励\"\n",
    "4. 模型学习整体提高得分高的回答的生成概率，降低得分低的生成概率\n",
    "\n",
    "### 具体流程\n",
    "1. 对问题q生成G个不同回答{o₁, o₂, ⋯, oG}\n",
    "2. 奖励模型评估每个回答，得到奖励值{r₁, r₂, ⋯, rG}\n",
    "3. 标准化奖励：r̂ᵢ = (rᵢ - mean(r))/std(r)\n",
    "4. 同一回答中的所有token共享相同的优势值：Âᵢₜ = r̂ᵢ\n",
    "\n",
    "### 实例说明\n",
    "假设问题：\"解释牛顿第二定律\"\n",
    "\n",
    "生成3个回答(G=3)：\n",
    "```json\n",
    "o₁: \"牛顿第二定律描述了力与加速度的关系，即F=ma。这意味着物体的加速度正比于所受的力，反比于质量。\"\n",
    "o₂: \"牛顿第二定律是运动定律之一，说明物体受力会产生加速度。\"\n",
    "o₃: \"牛顿第二定律指出，物体的加速度与所受的合外力成正比，与质量成反比，即F=ma，其中F是力，m是质量，a是加速度。这一定律为经典力学提供了基础。\"\n",
    "```\n",
    "\n",
    "奖励评分：\n",
    "\n",
    "r₁ = 7.5    \n",
    "r₂ = 5.0    \n",
    "r₃ = 9.0    \n",
    "\n",
    "\n",
    "标准化计算：\n",
    "\n",
    "mean(r) = (7.5 + 5.0 + 9.0)/3 = 7.17    \n",
    "std(r) = 1.65 (标准差计算)    \n",
    "\n",
    "r̂₁ = (7.5 - 7.17)/1.65 = 0.20    \n",
    "r̂₂ = (5.0 - 7.17)/1.65 = -1.32    \n",
    "r̂₃ = (9.0 - 7.17)/1.65 = 1.11    \n",
    "\n",
    "\n",
    "优势分配：\n",
    "- o₁中的每个token都获得相同的优势值0.20    \n",
    "- o₂中的每个token都获得相同的优势值-1.32\n",
    "- o₃中的每个token都获得相同的优势值1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f152608",
   "metadata": {},
   "source": [
    "## 二、过程监督（Process Supervision）\n",
    "\n",
    "### 基本原理\n",
    "过程监督就像是**关注解题每一步**的老师。不仅看最终答案，还会检查你的每个推理步骤并分别打分。\n",
    "\n",
    "### 工作方式\n",
    "1. 模型针对一个问题生成多个不同回答\n",
    "2. 将每个回答分解为多个推理步骤\n",
    "3. 每个步骤单独获得一个分数\n",
    "4. 每个单词（token）获得的\"奖励\"取决于它影响了哪些后续步骤\n",
    "5. 早期单词影响更多步骤，因此获得的总奖励可能更多\n",
    "\n",
    "### 具体流程\n",
    "1. 对问题q生成G个不同回答{o₁, o₂, ⋯, oG}\n",
    "2. 识别每个回答中的推理步骤，每步结束于index(j)\n",
    "3. 奖励模型评估每个步骤，得到步骤奖励\n",
    "4. 标准化每个步骤奖励：r̂ᵢ^index(j) = (rᵢ^index(j) - mean(R))/std(R)\n",
    "5. 计算每个token的优势值为从该token开始的后续所有步骤的标准化奖励之和：Âᵢₜ = ∑(index(j)≥t) r̂ᵢ^index(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b2afb",
   "metadata": {},
   "source": [
    "### 实例说明\n",
    "继续上面的例子，但现在每个回答分为多个推理步骤：\n",
    "\n",
    "回答o₁的步骤：\n",
    "```json\n",
    "步骤1(index=15): \"牛顿第二定律描述了力与加速度的关系，\"\n",
    "步骤2(index=25): \"即F=ma。\"\n",
    "步骤3(index=50): \"这意味着物体的加速度正比于所受的力，反比于质量。\"\n",
    "```\n",
    "\n",
    "步骤奖励：\n",
    "\n",
    "r₁^15 = 6.0 (步骤1)    \n",
    "r₁^25 = 8.0 (步骤2)   \n",
    "r₁^50 = 7.0 (步骤3)   \n",
    "\n",
    "\n",
    "同样为o₂和o₃计算步骤奖励(简化显示)：   \n",
    "\n",
    "o₂: r₂^10 = 5.0, r₂^20 = 5.0    \n",
    "o₃: r₃^20 = 8.0, r₃^35 = 9.5, r₃^65 = 9.0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91923a11",
   "metadata": {},
   "source": [
    "标准化所有步骤奖励(假设mean(R)=7.0, std(R)=1.5)：\n",
    "```json\n",
    "r̂₁^15 = (6.0-7.0)/1.5 = -0.67\n",
    "r̂₁^25 = (8.0-7.0)/1.5 = 0.67\n",
    "r̂₁^50 = (7.0-7.0)/1.5 = 0.0\n",
    "\n",
    "r̂₂^10 = (5.0-7.0)/1.5 = -1.33\n",
    "r̂₂^20 = (5.0-7.0)/1.5 = -1.33\n",
    "\n",
    "r̂₃^20 = (8.0-7.0)/1.5 = 0.67\n",
    "r̂₃^35 = (9.5-7.0)/1.5 = 1.67\n",
    "r̂₃^65 = (9.0-7.0)/1.5 = 1.33\n",
    "```\n",
    "\n",
    "优势计算(选择性展示)：\n",
    "- o₁中第1个token(t=1)的优势：\n",
    "  Â₁,₁ = r̂₁^15 + r̂₁^25 + r̂₁^50 = -0.67 + 0.67 + 0.0 = 0.0\n",
    "  \n",
    "- o₁中第20个token(t=20)的优势：\n",
    "  Â₁,₂₀ = r̂₁^25 + r̂₁^50 = 0.67 + 0.0 = 0.67\n",
    "  \n",
    "- o₁中第30个token(t=30)的优势：\n",
    "  Â₁,₃₀ = r̂₁^50 = 0.0\n",
    "\n",
    "- o₃中第1个token(t=1)的优势：\n",
    "  Â₃,₁ = r̂₃^20 + r̂₃^35 + r̂₃^65 = 0.67 + 1.67 + 1.33 = 3.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92e327",
   "metadata": {},
   "source": [
    "## 四、实际影响的对比\n",
    "\n",
    "### 学习效果差异\n",
    "\n",
    " 1. 奖励粒度\n",
    "- **结果监督**：只关注最终结果，整个序列共享一个奖励\n",
    "- **过程监督**：关注每个推理步骤，细化到多个奖励点\n",
    "\n",
    " 2. 优势计算\n",
    "- **结果监督**：所有token获得相同的优势值\n",
    "- **过程监督**：token的优势值取决于其在序列中的位置，早期token考虑后续所有步骤的奖励\n",
    "\n",
    " 3. 适用场景\n",
    "- **结果监督**：适合答案质量容易整体评估的任务(如一般问答)\n",
    "- **过程监督**：适合需要正确推理步骤的复杂任务(如数学、逻辑推理)\n",
    "\n",
    " 4. 实现复杂度\n",
    "- **结果监督**：实现简单，计算开销小\n",
    "- **过程监督**：需要识别推理步骤，实现复杂，计算开销大\n",
    "\n",
    "### 简单类比\n",
    "\n",
    "结果监督就像考试只看最终答案对错，过程监督则像老师批改时会给解题步骤打分：\n",
    "\n",
    "- 如果你只看最终答案（结果监督）：学生可能通过猜测得到正确答案，或者一个小计算错误导致整个解答被判错\n",
    "  \n",
    "- 如果你评价每一步（过程监督）：即使最终答案错了，学生也能知道哪些步骤是正确的，哪一步出了问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963af912",
   "metadata": {},
   "source": [
    "## 五、何时使用哪种监督方式？\n",
    "\n",
    "### 适合结果监督的场景\n",
    "- 简短的事实类问答\n",
    "- 评价标准简单明确的任务\n",
    "- 最终结果比过程更重要的情况\n",
    "- 计算资源有限时（实现更简单）\n",
    "\n",
    "### 适合过程监督的场景\n",
    "- 数学问题解答\n",
    "- 多步逻辑推理\n",
    "- 程序编写\n",
    "- 科学解释\n",
    "- 复杂决策过程\n",
    "\n",
    "在实际应用中，GRPO允许根据不同任务特点选择合适的监督方式，甚至可以针对不同类型的问题动态切换监督模式，以获得最佳训练效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d639b4a",
   "metadata": {},
   "source": [
    "# 4.GRPO与PPO公式详解与应用\n",
    "\n",
    "## 公式一：PPO目标函数\n",
    "\n",
    "$\\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), o \\sim \\pi_{\\theta_{old}}(O|q)] \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left[ \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})} A_t, \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}, 1-\\epsilon, 1+\\epsilon\\right) A_t \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa1c1",
   "metadata": {},
   "source": [
    "### 整体作用\n",
    "这是PPO算法的核心目标函数，用于指导策略模型的参数更新，平衡探索与利用，同时确保更新稳定性。\n",
    "\n",
    "### 参数详解\n",
    "- **$\\mathcal{J}_{PPO}(\\theta)$**: PPO的目标函数，我们要最大化这个值\n",
    "- **$\\mathbb{E}[q \\sim P(Q), o \\sim \\pi_{\\theta_{old}}(O|q)]$**: 期望值，表示在问题分布和旧策略下采样\n",
    "- **$\\frac{1}{|o|}$**: 对输出序列长度的归一化，确保不同长度的输出有可比性\n",
    "- **$\\sum_{t=1}^{|o|}$**: 对输出序列中每个token位置求和\n",
    "- **$\\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}$**: 重要性采样比率，新旧策略的概率比\n",
    "- **$A_t$**: 在位置t的优势值，表示该动作相对于平均表现的好坏\n",
    "- **$\\text{clip}(\\cdot, 1-\\epsilon, 1+\\epsilon)$**: 裁剪函数，限制重要性比率在[1-ε,1+ε]范围内\n",
    "- **$\\min[\\cdot,\\cdot]$**: 取两者中较小值，实现保守更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90cbc3",
   "metadata": {},
   "source": [
    "### 滑雪游戏示例\n",
    "想象一个AI学习玩滑雪游戏：\n",
    "- **问题q**: 当前滑雪场景（雪道状态、障碍物位置等）\n",
    "- **输出o**: AI选择的动作序列（左转、右转、减速等）\n",
    "- **旧策略$\\pi_{\\theta_{old}}$**: AI当前的滑雪策略\n",
    "- **新策略$\\pi_{\\theta}$**: 正在学习的更新后策略\n",
    "- **优势$A_t$**: 某个动作相比平均表现带来的额外收益\n",
    "\n",
    "假设在一个陡峭转弯处：\n",
    "- 旧策略给\"右转\"动作的概率是0.3\n",
    "- 新策略给\"右转\"动作的概率是0.6\n",
    "- 优势值为+2（表明右转是好的选择）\n",
    "- ε设为0.2\n",
    "\n",
    "计算过程：\n",
    "1. 重要性比率: 0.6/0.3 = 2.0\n",
    "2. 裁剪后: min(2.0, 1.2) = 1.2\n",
    "3. 乘以优势: 1.2 × 2 = 2.4\n",
    "4. 这个值将贡献给目标函数\n",
    "\n",
    "这种裁剪机制确保了AI不会因为一次成功的右转就极端地调整策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b97755",
   "metadata": {},
   "source": [
    "## 公式二：奖励计算\n",
    "\n",
    "$r_\\tau = r_\\varphi(q, o_{\\leq t}) - \\beta \\log \\frac{\\pi_\\theta(o_t|q, o_{<t})}{\\pi_{ref}(o_t|q, o_{<t})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dbb50",
   "metadata": {},
   "source": [
    "### 整体作用\n",
    "这个公式计算调整后的奖励，综合考虑了原始奖励和KL散度惩罚项，平衡了性能提升和与参考模型的偏离程度。\n",
    "\n",
    "### 参数详解\n",
    "- **$r_\\tau$**: 调整后的奖励值\n",
    "- **$r_\\varphi(q, o_{\\leq t})$**: 奖励模型对当前输出的评分\n",
    "- **$\\beta$**: KL惩罚系数，控制对偏离参考模型的惩罚强度\n",
    "- **$\\log \\frac{\\pi_\\theta(o_t|q, o_{<t})}{\\pi_{ref}(o_t|q, o_{<t})}$**: 当前策略与参考策略的KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f90e92",
   "metadata": {},
   "source": [
    "### 滑雪游戏示例\n",
    "继续滑雪游戏场景：\n",
    "- **奖励模型评分**: AI完成一个漂亮转弯得到+5分\n",
    "- **当前策略**: 在某种情况下以0.7的概率选择\"减速后转弯\"\n",
    "- **参考策略**: 在同样情况下以0.4的概率选择\"减速后转弯\"\n",
    "- **β值**: 设为0.5\n",
    "\n",
    "计算过程：\n",
    "1. 原始奖励: +5\n",
    "2. KL项: 0.5 × log(0.7/0.4) ≈ 0.5 × 0.56 ≈ 0.28\n",
    "3. 调整后奖励: 5 - 0.28 = 4.72\n",
    "\n",
    "这表明虽然该动作获得了高奖励，但由于偏离了参考模型的行为，奖励被轻微降低。这防止AI过度专注于某些高奖励动作而忽略参考模型的整体能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351b495",
   "metadata": {},
   "source": [
    "## 优化点：奖励调整机制\n",
    "\n",
    "### 1. 主要优化点\n",
    "\n",
    "**KL正则化奖励设计**：公式二引入了直接将KL散度整合到奖励信号中的机制，而不是作为独立的约束项。\n",
    "\n",
    "### 2. 具体优化内容\n",
    "\n",
    "- **即时反馈机制**：传统方法通常在整个优化目标中添加KL惩罚，而公式二将其整合到每个时间步的奖励中\n",
    "- **动态平衡**：在生成过程中动态平衡性能提升和能力保留\n",
    "- **减少延迟反馈**：不需要等到完整序列生成后才施加KL约束\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e691b3",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 在GRPO目标函数中的体现\n",
    "\n",
    "在公式三（GRPO目标函数）中，这一优化通过以下方式体现：\n",
    "\n",
    "公式二的调整奖励 → 计算优势值Â → 用于GRPO目标函数中的$Â_{i,t}$项\n",
    "\n",
    "\n",
    "具体来说，公式三中的优势值$\\hat{A}_{i,t}$是基于公式二调整后的奖励计算得出的：\n",
    "\n",
    "$\\hat{A}_{i,t}$ = 标准化(通过公式二计算的奖励)\n",
    "\n",
    "这建立了一个直接链接：公式二改进了奖励信号，这些改进的奖励进而影响GRPO的优势计算和最终的策略更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b4353",
   "metadata": {},
   "source": [
    "## 公式三：GRPO目标函数\n",
    "\n",
    "$\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)] \\\\\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left( \\min\\left[\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_{i,t}\\right] - \\beta D_{KL} \\left[ \\pi_\\theta||\\pi_{ref} \\right] \\right)$\n",
    "\n",
    "### 整体作用\n",
    "GRPO的目标函数，扩展了PPO的思想，通过多组输出的平均来提高样本效率和训练稳定性，同时直接集成KL约束。\n",
    "\n",
    "### 参数详解\n",
    "- **$\\mathcal{J}_{GRPO}(\\theta)$**: GRPO的目标函数\n",
    "- **$\\{o_i\\}_{i=1}^G$**: G组不同的输出样本\n",
    "- **$\\frac{1}{G}$**: 对G组样本取平均\n",
    "- **$\\frac{1}{|o_i|}$**: 对每个输出序列长度归一化\n",
    "- **$\\hat{A}_{i,t}$**: 通过Group Computation计算的优势值\n",
    "- **$\\beta D_{KL}$**: KL散度约束项及其系数\n",
    "- 其他项与PPO目标函数类似\n",
    "\n",
    "### 滑雪游戏示例\n",
    "在滑雪游戏中：\n",
    "- **问题q**: 同一个滑雪场景\n",
    "- **G组输出**: AI同时尝试G=3种不同的滑行路线\n",
    "  - 路线1: 激进路线（高速通过弯道）\n",
    "  - 路线2: 保守路线（减速谨慎通过）\n",
    "  - 路线3: 平衡路线（适度减速后通过）\n",
    "\n",
    "假设Group Computation后的优势值：\n",
    "- 路线1优势: -1.2（风险太高）\n",
    "- 路线2优势: 0.3（过于保守）\n",
    "- 路线3优势: 1.5（恰到好处）\n",
    "\n",
    "GRPO会综合考虑这三条路线的表现，同时计算与参考模型的偏离程度，最终一次性更新策略。这比PPO单独评估每条路线更有效率，也更稳定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70059b93",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 公式四：KL散度估计\n",
    "\n",
    "$\\mathbb{D}_{KL} \\left[ \\pi_\\theta||\\pi_{ref} \\right] = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - \\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - 1$\n",
    "\n",
    "### 整体作用\n",
    "这是一个无偏KL散度估计器，用于精确计算当前策略与参考策略之间的差异程度，确保优化过程中不会过度偏离参考行为。\n",
    "\n",
    "### 参数详解\n",
    "- **$\\mathbb{D}_{KL} \\left[ \\pi_\\theta||\\pi_{ref} \\right]$**: 策略与参考模型之间的KL散度\n",
    "- **$\\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}$**: 参考策略与当前策略的概率比\n",
    "- **$\\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}$**: 概率比的对数\n",
    "- **$-1$**: 确保估计是无偏的修正项\n",
    "\n",
    "### 滑雪游戏示例\n",
    "在某个特定转弯处：\n",
    "- **参考模型概率**: 选择\"中等力度右转\"的概率是0.5\n",
    "- **当前策略概率**: 选择同一动作的概率是0.3\n",
    "\n",
    "计算KL散度：\n",
    "1. 概率比: 0.5/0.3 = 1.67\n",
    "2. 对数项: log(1.67) ≈ 0.51\n",
    "3. KL散度: 1.67 - 0.51 - 1 = 0.16\n",
    "\n",
    "这个KL值告诉我们当前策略在这个决策点偏离参考模型的程度。值越大，偏离越严重，会导致更强的约束惩罚。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d02f26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 公式四的优化点：KL散度估计器\n",
    "\n",
    "### 1. 主要优化点\n",
    "\n",
    "**无偏KL散度估计**：公式四引入了一个数学上无偏的KL散度估计器，提高了KL约束的精确性。\n",
    "\n",
    "### 2. 具体优化内容\n",
    "\n",
    "- **估计精度提升**：传统KL估计可能有偏差，特别是在样本数量有限时\n",
    "- **保证正值**：设计确保KL散度估计始终为正，符合KL散度的数学性质\n",
    "- **减少采样变异**：提供更稳定的KL散度估计，减少训练波动\n",
    "- **计算效率**：避免了需要大量采样才能准确估计KL散度的问题\n",
    "\n",
    "### 3. 在GRPO目标函数中的体现\n",
    "\n",
    "在公式三（GRPO目标函数）中，这一优化通过以下方式体现：\n",
    "\n",
    "\n",
    "公式四的KL估计 → 直接用于公式三中的$D_{KL}[π_θ||π_{ref}]$项\n",
    "\n",
    "\n",
    "具体来说，公式三末尾的KL约束项$\\beta D_{KL} \\left[ \\pi_\\theta||\\pi_{ref} \\right]$正是使用公式四计算得出的：\n",
    "\n",
    "$\\beta D_{KL} \\left[ \\pi_\\theta||\\pi_{ref} \\right]$ = β × (由公式四计算的KL散度)\n",
    "\n",
    "这建立了另一个直接链接：公式四提供了更准确的KL散度估计，这个改进的估计直接用于GRPO目标函数的约束项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e2b5b",
   "metadata": {},
   "source": [
    "## 两个优化的协同作用\n",
    "\n",
    "公式二和公式四的优化点协同工作，形成了一个完整的KL控制系统：\n",
    "\n",
    "1. **双层KL控制**：\n",
    "   - 公式二：在奖励层面整合KL约束（微观控制）\n",
    "   - 公式四+公式三：在目标函数层面整合KL约束（宏观控制）\n",
    "\n",
    "2. **互补性**：\n",
    "   - 公式二解决\"何时\"和\"如何\"控制KL散度\n",
    "   - 公式四解决\"多准确\"地衡量KL散度\n",
    "\n",
    "3. **共同目标**：\n",
    "   - 都旨在确保模型在优化性能的同时不会过度偏离参考行为\n",
    "   - 前者是过程控制，后者是结果验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0f4cc",
   "metadata": {},
   "source": [
    "## 公式在整体GRPO框架中的位置与关系\n",
    "\n",
    "1. **公式一**（PPO目标函数）是传统PPO的核心，GRPO在此基础上扩展\n",
    "2. **公式二**（奖励计算）定义了如何结合原始奖励和KL约束\n",
    "3. **公式三**（GRPO目标函数）是GRPO的核心，整合了多组输出的优势和KL约束\n",
    "4. **公式四**（KL散度估计）提供了准确计算策略偏离度的工具\n",
    "\n",
    "在训练流程中：\n",
    "1. 首先使用策略模型生成G组输出\n",
    "2. 对每组输出计算奖励（使用公式二）\n",
    "3. 使用Group Computation计算标准化优势\n",
    "4. 计算与参考模型的KL散度（使用公式四）\n",
    "5. 最后使用GRPO目标函数（公式三）更新策略模型\n",
    "\n",
    "这种设计使GRPO在保持PPO核心思想的同时，通过多组输出的统计特性提高了训练效率和稳定性，使其特别适合大型语言模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88de7db",
   "metadata": {},
   "source": [
    "# 5.双层KL控制机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a891004",
   "metadata": {},
   "source": [
    "## 一、微观KL控制\n",
    "\n",
    "### 作用范围\n",
    "- 在**每一组**（G个输出）中\n",
    "- 对**每个具体回答**的奖励计算过程中\n",
    "\n",
    "### 具体公式\n",
    "$r_t = r_\\phi(q, o_{\\leq t}) - \\beta \\log \\frac{\\pi_\\theta(o_t|q,o_{<t})}{\\pi_{ref}(o_t|q,o_{<t})}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62c4f0",
   "metadata": {},
   "source": [
    "### 工作过程\n",
    "\n",
    "1. 对一个问题生成G个回答\n",
    "2. 对每个回答：\n",
    "   - 计算原始奖励 $r_φ$\n",
    "   - 计算与参考模型的KL散度\n",
    "   - 用KL散度调整奖励值\n",
    "3. 这些调整后的奖励用于计算优势值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7193f54",
   "metadata": {},
   "source": [
    "### 例子\n",
    "假设一组内有3个回答：\n",
    "\n",
    "问题：\"描述天气\"\n",
    "\n",
    "回答1: \"天气晴朗\" \n",
    "- 原始奖励：7分\n",
    "- KL散度小\n",
    "- 调整后奖励：约6.8分\n",
    "\n",
    "回答2: \"哇！天气太棒了！！\"\n",
    "- 原始奖励：8分\n",
    "- KL散度大\n",
    "- 调整后奖励：约5分\n",
    "\n",
    "回答3: \"今天是晴天\"\n",
    "- 原始奖励：7分\n",
    "- KL散度小\n",
    "- 调整后奖励：约6.9分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6f02c",
   "metadata": {},
   "source": [
    "## 二、宏观KL控制\n",
    "\n",
    "### 作用范围\n",
    "- 在**整个训练过程**中\n",
    "- 对**策略模型整体**的行为分布\n",
    "\n",
    "### 具体公式\n",
    "$J_{GRPO}(\\theta) = \\mathbb{E}[...] - \\beta D_{KL}[\\pi_\\theta||\\pi_{ref}]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3433b64",
   "metadata": {},
   "source": [
    "### 工作过程\n",
    "\n",
    "1. 收集多组训练数据\n",
    "2. 评估整体策略分布的变化\n",
    "3. 在目标函数中施加KL约束\n",
    "4. 影响整体的参数更新方向"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f8743",
   "metadata": {},
   "source": [
    "### 例子\n",
    "\n",
    "训练过程中观察到的整体变化：   \n",
    "第1轮训练：\n",
    "- 多组数据的风格基本保持\n",
    "- 宏观KL约束较小\n",
    "\n",
    "第10轮训练：\n",
    "- 发现回答开始普遍活泼化\n",
    "- 增加宏观KL约束\n",
    "\n",
    "第20轮训练：\n",
    "- 注意到专业术语使用减少\n",
    "- 进一步调整宏观KL约束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224897d",
   "metadata": {},
   "source": [
    "## 三、两个层次的区别\n",
    "\n",
    "### 1. 时间尺度\n",
    "- **微观控制**：即时生效，作用于每个回答\n",
    "- **宏观控制**：长期生效，作用于整体训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134cff5",
   "metadata": {},
   "source": [
    "### 2. 控制粒度   \n",
    "**微观控制**：\n",
    "- 单个回答级别\n",
    "- 具体奖励调整\n",
    "- 即时反馈\n",
    "\n",
    "- **宏观控制**：\n",
    "- 整体策略级别\n",
    "- 参数更新方向\n",
    "- 长期约束\n",
    "\n",
    "### 3. 协同效果\n",
    "\n",
    "微观控制：\n",
    "- 处理具体的偏离行为\n",
    "- 提供及时的纠正信号\n",
    "- 影响单次学习效果\n",
    "\n",
    "宏观控制：\n",
    "- 维持整体行为分布\n",
    "- 防止长期能力退化\n",
    "- 保证训练稳定性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6368a",
   "metadata": {},
   "source": [
    "## 四、形象类比\n",
    "\n",
    "想象训练一个厨师：\n",
    "\n",
    "### 微观控制\n",
    "\n",
    "- 对每道菜的调味进行指导\n",
    "- 及时纠正用料的偏差\n",
    "- 针对具体操作给出建议\n",
    "\n",
    "### 宏观控制\n",
    "\n",
    "- 保持餐厅整体的烹饪风格\n",
    "- 确保菜品体系的完整性\n",
    "- 维持长期的品质标准\n",
    "\n",
    "\n",
    "## 五、总结\n",
    "\n",
    "1. **微观KL控制**：作用于每组中的每个具体回答，通过调整奖励实现即时控制\n",
    "2. **宏观KL控制**：作用于整个训练过程，通过目标函数约束实现长期控制\n",
    "\n",
    "这种双层设计确保了：\n",
    "- 短期行为不会过度偏离（微观控制）\n",
    "- 长期能力得到保持（宏观控制）\n",
    "- 训练过程更加稳定（双重保障）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fe55b",
   "metadata": {},
   "source": [
    "## GRPO双层KL限制在实际训练中的应用\n",
    "\n",
    "## 一、训练数据情况\n",
    "\n",
    "问题a：生成10个答案 {a1, a2, ..., a10}   \n",
    "问题b：生成10个答案 {b1, b2, ..., b10}   \n",
    "问题c：生成10个答案 {c1, c2, ..., c10}   \n",
    "\n",
    "\n",
    "## 二、微观KL控制（奖励计算阶段）\n",
    "\n",
    "### 1. 对问题a的处理\n",
    "\n",
    "对每个答案计算调整后的奖励：   \n",
    "r_a1 = r_φ(a1) - β * KL(当前策略生成a1的概率/参考策略生成a1的概率)   \n",
    "r_a2 = r_φ(a2) - β * KL(当前策略生成a2的概率/参考策略生成a2的概率)  \n",
    "...    \n",
    "r_a10 = r_φ(a10) - β * KL(当前策略生成a10的概率/参考策略生成a10的概率)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce7ecb",
   "metadata": {},
   "source": [
    "### 2. 对问题b的处理\n",
    "\n",
    "同样方式计算调整后的奖励：     \n",
    "r_b1 = r_φ(b1) - β * KL(...)    \n",
    "r_b2 = r_φ(b2) - β * KL(...)   \n",
    "...   \n",
    "r_b10 = r_φ(b10) - β * KL(...)   \n",
    "\n",
    "### 3. 对问题c的处理\n",
    "\n",
    "r_c1 = r_φ(c1) - β * KL(...)   \n",
    "r_c2 = r_φ(c2) - β * KL(...)   \n",
    "...   \n",
    "r_c10 = r_φ(c10) - β * KL(...)   \n",
    "\n",
    "这些调整后的奖励会用于计算每个答案的优势值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a584c98",
   "metadata": {},
   "source": [
    "## 三、宏观KL控制（策略更新阶段）\n",
    "\n",
    "在收集了所有问题的所有答案后，在更新策略时：\n",
    "```json\n",
    "目标函数 = E.....[min(ratio * A, clip(ratio) * A)] - β * D_KL(当前整体策略||参考策略)\n",
    "```\n",
    "\n",
    "这个KL散度是在整个策略分布层面计算的，考虑了所有问题(a,b,c)的所有答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0703658",
   "metadata": {},
   "source": [
    "## 四、两层KL控制的区别\n",
    "\n",
    "### 微观KL控制\n",
    "- 作用于每个单独答案的奖励计算\n",
    "- 在组内(每个问题的10个答案)进行\n",
    "- 影响单个答案的学习信号强度\n",
    "\n",
    "### 宏观KL控制\n",
    "- 作用于整体策略更新\n",
    "- 考虑所有问题所有答案的整体分布\n",
    "- 影响整个模型的参数更新方向\n",
    "\n",
    "## 五、实际训练流程\n",
    "\n",
    "1. 对问题a：\n",
    "   - 生成10个答案\n",
    "   - 计算每个答案的原始奖励\n",
    "   - 应用微观KL控制调整奖励\n",
    "   - 计算优势值\n",
    "\n",
    "2. 对问题b：\n",
    "   - 重复同样过程\n",
    "   \n",
    "3. 对问题c：\n",
    "   - 重复同样过程\n",
    "\n",
    "4. 策略更新时：\n",
    "   - 收集所有答案和优势值\n",
    "   - 计算整体策略的KL散度\n",
    "   - 应用宏观KL控制\n",
    "   - 更新模型参数\n",
    "\n",
    "这样，双层KL控制在不同层面共同作用，既保证了单个答案的质量，又维持了整体策略的稳定性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089599fb",
   "metadata": {},
   "source": [
    "# 6.总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f7ef8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250326170346825.png\" width=100%></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fufan_chat_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
