{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe36cb8",
   "metadata": {},
   "source": [
    "# <center >《大模型Agent开发实战》（体验课）</center>\n",
    "\n",
    "## <center>基于PaddleOCR-VL搭建企业级多模态RAG系统</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abac03",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211610437.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16867a56",
   "metadata": {},
   "source": [
    "- **本期公开课案例功能介绍**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5741f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;核心功能一：使用PaddleOCR-VL + DeepSeek-OCR实现复杂图像、PDF、扫描件、手写笔记、旧试卷等文档高精度一键解析;\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201751725.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eef843",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211239047.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4f15e",
   "metadata": {},
   "source": [
    "- 核心功能二：多模态RAG在线解析及提取信息可视化，支持复杂PDF和图片文件类型，并支持文档结构和可视化对比；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747542f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211620320.png\" width=80%></div>\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211620322.png\" width=80%></div>\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211620321.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e228f",
   "metadata": {},
   "source": [
    "- 核心功能三：多模态 Agentic RAG 问答系统搭建，并支持图片、表格、文本框、公式级细粒度的精准溯源；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de07349",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211610437.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65e286",
   "metadata": {},
   "source": [
    "# 一、OCR技术兴起的背景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22728cb7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;随着大模型技术的兴起，OCR 技术也迎来了新的快速发展机遇。2025年10月16日发布的 PaddleOCR-VL 模型直接屠榜，在全球权威榜单OmniDocBench V1.5中以92.6分夺得综合性能第一，横扫文本识别、公式识别、表格理解与阅读顺序四项SOTA。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211049426.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9a272",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而紧接其后，DeepSeek 也于 2025年10月21日发布了 DeepSeek-OCR 模型，仅需7G的显存，就能完成高精度的表格、公式识别，图片语义识别，并且在多项评测指标中一举拿下SOTA成绩。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211101053.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18047000",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这类模型的兴起，完全是源于真实应用需求的驱动。\n",
    "\n",
    "&emsp;&emsp;OCR（全称：Optical Character Recognition，光学字符识别）是将包含文本的图像（如扫描文档、照片、表单、书页）转换成 机器可读的文本格式的技术，如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201841979.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f6990",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这类模型并不是传统的OCR模型，而是拥有大模型多模态能力的OCR模型，这种模型被称为VLM（Vision-Language Model）模型、或者OCR 2.0模型。其中最本质的区别，就是多模态大模型在进行图像识别之前，会借助一个名叫视觉编码器的算法，将图像视觉信息映射到文本空间中，然后再借助大模型对文本语义的理解能力，间接的去理解图像信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f1b8c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单理解就是：你有一张扫描的图片，上面写满了打印或手写的文字，OCR 技术可以把这些“文字图像”变成“真实的文本数据”（可以编辑、搜查、分析）而不是只是“一个图片”而已。\n",
    "\n",
    "&emsp;&emsp;目前企业在做的大部分业务场景都是以正确识别不同文档类型的内容为前提，比如处理大量纸质文档（发票、合同、报表、表单、收据等），需要通过 OCR 可以把这些纸质/图像文档转换为电子数据，从而更便于存储、检索、分析。\n",
    "在数字化、智能化流程（例如自动化数据输入、档案管理、人工智能分析）中，它同样也是一个基础环节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab156ff",
   "metadata": {},
   "source": [
    "&emsp;&emsp;是否能正确解析内容的这个过程对基于大模型落地的应用技术是非常关键的。大家耳熟能详的RAG技术，其构建的第一步，就是完成从各类数据格式的精准解析：\n",
    "\n",
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202404101823060.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a77abc",
   "metadata": {},
   "source": [
    "- 大模型开发岗位的敲门砖 - 多模态RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5db84",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211113655.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f5a61",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这类岗位做的核心工作其实就是在对多模态数据如PDF、图片、音频、视频的预处理后，再开展基于大模型的智能问答/Agent工作流编排工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a655f",
   "metadata": {},
   "source": [
    "# 二、主流OCR项目介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b928d0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;就目前的发展来看，OCR 与 RAG 的技术结合并不像简单的传统 OCR 是“你给我一张图像，我把文字提取出来”就可以了，重点不能仅仅放在去提取文字，而是要在 OCR 的基础上，不仅仅是“图像中的文字”被识别，还需要结合 视觉（图像）、语言（文本）、版面结构（布局）、场景环境/上下文 等多个模态（modalities）一起分析，使得识别和理解更强、更智能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfab0d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这其实就是我们今天要谈论的主题：多模态RAG。在大模型、视觉-语言模型（Vision‐Language Models, VLMs）兴起的背景下，多模态 OCR 是一个重要方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425598c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346376.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81daf5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;现代文档不仅仅是文本。它们包含多列布局、数学公式、半扫描表格、多语言文本以及分辨率奇数的图表。像 GPT-4o 或 Qwen-VL 这样的端到端模型可以解析它们，但它们速度慢、布局混乱，并且耗费 GPU 内存。所以企业环境下往往会选择更小、更紧凑的视觉模型来为解析工作提供支持。主流的企业级OCR项目应用如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b84f",
   "metadata": {},
   "source": [
    "- MinerU:[点击进入](https://github.com/opendatalab/MinerU)\n",
    "\n",
    "&emsp;&emsp;MinerU 是由 OpenDataLab（上海人工智能实验室下属团队）发起的一个开源工具，目标是将 PDF（含扫描件、复杂版式、多栏、多表格、多公式）转换为可机读的结构化格式（如 Markdown、JSON）以便进一步下游使用。项目的定位更偏「文档内容抽取／结构化」而不仅仅是传统 OCR。其取向是“将 PDF → Markdown/JSON”这一流程，而不仅“图片 → 文字”。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202503271440433.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;`MinerU`的主要工作流程分为以下几个阶段：\n",
    "\n",
    "1. **输入**：接收`PDF` 格式文本，可以是简单的纯文本，也可以是包含双列文本、公式、表格或图等多模态`PDF`文件;\n",
    "2. **文档预处理（Document Preprocessing）**：检查语言、页面大小、文件是否被扫描以及加密状态；\n",
    "3. **内容解析（Content Parsing）**：\n",
    "    - 局分析：区分文本、表格和图像。\n",
    "    - 公式检​​测和识别：识别公式类型（内联、显示或忽略）并将其转换为 `LaTeX` 格式。\n",
    "    - 表格识别：以 `HTML/LaTeX` 格式输出表格。\n",
    "    - OCR：对扫描的 `PDF` 执行文本识别。\n",
    "4. **内容后处理（Content Post-processing）**：修复文档解析后可能出现的问题。比如解决文本、图像、表格和公式块之间的重叠，并根据人类阅读模式重新排序内容，确保最终输出遵循自然的阅读顺序。\n",
    "5. **格式转换（Format Conversion）**：以 `Markdown` 或 `JSON` 格式生成输出。\n",
    "6. **输出（Output）**：高质量、结构良好的解析文档。\n",
    "\n",
    "&emsp;&emsp;如下是`MinerU`项目官方给出的配置参考：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202508061340184.png\" alt=\"MinerU 配置参考\" width=\"80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691637bd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`VLM-Transformer`则是直接利用`transformers`库中的`Vision-Language`模型处理图像+文本的多模态输入，其流程如下：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202508061356866.png\" alt=\"MinerU 配置参考\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;而`VLM-Sglang`后端则是利用`sglang`高性能推理引擎，优化了GPU加速及分布式部署的基础上同时支持实时流式输出，其流程如下：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202508061356864.png\" alt=\"MinerU 配置参考\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da302b2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;MinerU 项目非常适用于：\n",
    "\n",
    "- 需要从大量 PDF 文档中抽取结构化内容（例如学术文献、技术白皮书、报告）用于知识库或训练语料。\n",
    "\n",
    "- 对版式结构（如章节、列表、表格、公式）要求较高，而不只是 OCR 文本识别。\n",
    "\n",
    "- 希望输出 Markdown／JSON 供后续自动化流水线使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeffc1a",
   "metadata": {},
   "source": [
    "- dots.ocr：[点击进入](https://github.com/dots-ai/dots.ocr)\n",
    "\n",
    "\n",
    "&emsp;&emsp;dots.ocr 是由 rednote‑hilab（HiLab团队）开源的多语种文档布局解析工具。官方介绍中强调：“一个统一的 Vision-Language 模型（≈1.7 B 参数）即可完成布局检测 + 内容识别 +阅读顺序排序”。支持文本、表格、公式、以及多语言输入。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211151021.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;dots.ocr 的特点是用一个 VLM（1.7 B 参数）来统一布局解析+内容识别，而不是传统将检测、识别、结构分开。用户可通过不同 prompt 来切换任务（如“请输出版式元素的 bbox、类别、文本”）→ 即说明模型采用 prompt + VLM 的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f7c7c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211229592.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;非常适合需要快速处理多语种、混版式文档，且希望用一个统一模型／prompt 来搞定。虽然表现不错，但对于极复杂的表格（如跨页表、合并单元格）或特殊版式效果并不是很理想。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa143ec4",
   "metadata": {},
   "source": [
    "- PaddleOCR：[点击进入](https://github.com/PaddlePaddle/PaddleOCR/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d92b4b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PaddleOCR 是由 Baidu (及其生态) 基于其深度学习框架 PaddlePaddle 提供的开源 OCR 工具箱。支持从 PDF 或图像文档转为结构化数据（适配 AI 场景），支持 100+ 语言。最新版本 3.0 在其技术报告中提出：PP-OCRv5、PP-StructureV3、PP-ChatOCRv4 三大解决方案，覆盖文字识别、多版式文档解析、关键 信息提取。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211220985.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;在早期版本（如 PP-OCRv3）中，其结构可概括为：“检测 (Detection) → 分类 (Classification of orientation) → 识别 (Recognition)”。使用多种模型，例如检测模型（DBNet 等）、识别模型（如 SVTR），在 3.0 版本中，其“PP-StructureV3”整合了布局分析、表格识别、结构抽取。同时还最新推出了还推出了PaddleOCR-VL 的 Vision-Language 模型版本（0.9B 参数的 VLM），用于多语种文档解析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade97390",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PaddleOCR-VL 是推出的一个专注于“文档解析／视觉-语言模型 (Vision-Language Model, VLM)”功能的新模块，采用了视觉-语言模型架构以应对更高阶的需求。在解析多模态数据方面，PaddleOCR将这项工作分为两部分：\n",
    "\n",
    "1. 首先检测并排序布局元素。\n",
    "2. 使用紧凑的视觉语言模型精确识别每个元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc136d53",
   "metadata": {},
   "source": [
    "&emsp;&emsp;该系统分为两个明确的阶段运行。\n",
    "\n",
    "&emsp;&emsp;第一阶段是执行布局分析（PP-DocLayoutV2），此部分标识文本块、表格、公式和图表。它使用：\n",
    "- RT-DETR 用于物体检测（基本上是边界框 + 类标签）。\n",
    "- 指针网络 （6 个转换器层）可确定元素的读取顺序 ，从上到下、从左到右等。\n",
    "\n",
    "&emsp;&emsp;最终输出统一模式的图片标注数据，如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211239047.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc47ab",
   "metadata": {},
   "source": [
    "&emsp;&emsp;第二阶段则是元素识别（PaddleOCR-VL-0.9B），这就是视觉语言模型发挥作用的地方。它使用：\n",
    "- NaViT 风格编码器 （来自 Keye-VL），可处理动态图像分辨率。无平铺，无拉伸。\n",
    "- 一个简单的 2 层 MLP， 用于将视觉特征与语言空间对齐。\n",
    "- ERNIE-4.5–0.3B 作为语言模型，该模型规模虽小但速度很快，并且采用 3D-RoPE 进行位置编码\n",
    "\n",
    "&emsp;&emsp;最终模型输出结构化 Markdown 或 JSON 格式的文件用于后续的处理。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211243206.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b72c3a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个小小的设计决策， 将布局和识别分离，使得 PaddleOCR-VL 比通常的一体化系统更快、更稳定。同时根据实际的测试，其运行和解析速度也更快。在 A100 GPU 上， 吞吐量为 1.22 页/秒，。比 MinerU2.5 快 15.8%， VRAM 比 dots.ocr 少约 40%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9947bdb",
   "metadata": {},
   "source": [
    "# 三、本地部署 PaddleOCR-VL 实战\n",
    "\n",
    "&emsp;&emsp;本小节课程，我们将从零开始，详细介绍如何在本地环境中完整部署 PaddleOCR-VL 并完成 OCR 功能测试。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201609587.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1a8f3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于硬件要求，官方给出了如上说明。其中使用原生的PaddlePaddle方式，需要GPU算力≥8.5（RTX 3090/4090、A100 等），最稳定。使用vLLM方式，需要GPU算力≥8（RTX 3060 及以上），速度最快，但算力7-8之间（T4/V100）虽可运行但不推荐，易超时或OOM。使用SGLang方式，需要GPU算力8-12之间（RTX 3060-4090），性能与稳定性的平衡选择。\n",
    "\n",
    "&emsp;&emsp;注意：表格里的 \"≥8\" 指的是 GPU Compute Capability（GPU 算力版本号），不是显存大小！根据实测，根据PaddleOCR-VL-0.9B 模型（模型文件约 3.8GB）：\n",
    "\n",
    "- 最低要求：6GB 显存（勉强够用，单张图）\n",
    "- 推荐配置：8GB+ 显存（运行舒适）\n",
    "- 理想配置：12GB+ 显存（可以批处理多张图）\n",
    "\n",
    "&emsp;&emsp;所以：8GB 显存 + RTX 30 系列以上达到高效运行是完全没问题的。整个过程主要包括以下几个核心步骤：\n",
    "\n",
    "1. **创建 Python 虚拟环境** - 隔离项目依赖，避免环境冲突；\n",
    "2. **安装 PaddlePaddle 深度学习框架** - PaddleOCR 的底层依赖；\n",
    "3. **安装 PaddleOCR 库** - 核心 OCR 功能库；\n",
    "4. **下载预训练模型** - PaddleOCR-VL-0.9B 和 PP-DocLayoutV2；\n",
    "5. **验证安装** - 运行测试确保环境正常；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9fc35",
   "metadata": {},
   "source": [
    "&emsp;&emsp;公开课所使用的硬件环境为：Ubuntu 22.04 + 4 * 3090，共计显存96G显存，运行起来非常流畅。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346365.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83119b6",
   "metadata": {},
   "source": [
    "## 3.1 创建虚拟环境\n",
    "\n",
    "&emsp;&emsp;首先，我们需要创建一个独立的 Python 虚拟环境。虚拟环境可以隔离项目依赖，避免与系统其他 Python 项目产生冲突。执行如下命令\n",
    "```bash\n",
    "    conda create --name ocr_rag python==3.11\n",
    "```\n",
    "\n",
    "&emsp;&emsp;其中：\n",
    "- `--name ocr_rag`：虚拟环境名称，可以自定义\n",
    "- `python==3.11`：指定 Python 版本为 3.11（PaddleOCR 推荐版本）\n",
    "\n",
    "&emsp;&emsp;执行效果如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346357.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27e81a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来激活虚拟环境：\n",
    "```bash\n",
    "    conda activate ocr_rag\n",
    "```\n",
    "\n",
    "&emsp;&emsp;激活后，命令行提示符前会显示 `(ocr_rag)`，表示已进入虚拟环境。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346358.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e89f76",
   "metadata": {},
   "source": [
    "## 3.2 安装 PaddleOCR 工具框架\n",
    "\n",
    "&emsp;&emsp;首先需要区分PaddleOCR 和 PaddleOCR-VL 两个之间的区别和联系。首先。PaddleOCR 是一个较为成熟、功能全面的 OCR＋文档理解开源工具库，其开源地址：https://github.com/PaddlePaddle/PaddleOCR/tree/main\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201635319.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec962ce5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PaddleOCR-VL 是在这个生态内新发布的一个专注于“文档解析／视觉-语言模型 (Vision-Language Model, VLM)”功能的新模块。比如在 README 中提到：「PaddleOCR-VL – Multilingual Document Parsing via a 0.9B VLM」就是被列为 PaddleOCR 3.0 的核心特性之一。\n",
    "\n",
    "<center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201635320.png\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447ed46",
   "metadata": {},
   "source": [
    "&emsp;&emsp;简单来说，PaddleOCR 是一个“全面且成熟”的 OCR＋文档理解工具箱，而 PaddleOCR-VL 是在这个工具箱里“专门为复杂文档解析”设计的新模块，采用了视觉-语言模型架构以应对更高阶的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e8d51",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，在使用PaddleOCR-VL 时，我们首先需要安装 PaddleOCR 工具框架。https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/develop/install/pip/linux-pip.html\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346364.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914031d4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里安装 PaddlePaddle 3.2.0 版本。执行如下命令：\n",
    "\n",
    "```bash\n",
    "    python -m pip install paddlepaddle-gpu==3.2.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\n",
    "```\n",
    "\n",
    "&emsp;&emsp;其中:\n",
    "- `paddlepaddle-gpu==3.2.0`：GPU 版本的 PaddlePaddle 3.2.0\n",
    "- `-i https://...`：使用百度官方镜像源，下载速度更快\n",
    "- `cu126`：对应 CUDA 12.6 版本\n",
    "\n",
    "&emsp;&emsp;安装过程如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346366.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8bcce",
   "metadata": {},
   "source": [
    "&emsp;&emsp;安装结束后，需要验证 PaddlePaddle 安装状态，依次运行以下命令验证：\n",
    "\n",
    "```bash\n",
    "    python\n",
    "    import paddle\n",
    "    paddle.utils.run_check()\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346367.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c35345",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果看到如下“”信息，说明 PaddlePaddle 已成功安装。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346368.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf05c91",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来要重点关注的是：PaddleOCR-VL 使用 `safetensors` 格式存储模型权重，需要额外安装，同时需要安装指定版本的，执行如下命令\n",
    "```bash\n",
    "    ython -m pip install https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl\n",
    "```\n",
    "\n",
    "&emsp;&emsp;这是兼容最新PaddleOCR-Vl 的 safetensors 版本，安装很快。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346369.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb8895",
   "metadata": {},
   "source": [
    "## 3.3 下载PaddleOCR-VL模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae250ba",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用 PaddleOCR-VL 解析功能需要两个预训练模型，其中：\n",
    "\n",
    "1. **PaddleOCR-VL-0.9B** - 视觉语言模型（用于文本识别）\n",
    "2. **PP-DocLayoutV2** - 文档布局检测模型（用于布局分析）\n",
    "\n",
    "&emsp;&emsp;该模型权重的HuggingFace 地址为：https://huggingface.co/PaddlePaddle/PaddleOCR-VL ，\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201659478.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefa4b8",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;此外，对于我们国内用户来说，更建议通过 ModelScope 下载（推荐）。https://modelscope.cn/models/PaddlePaddle/PaddleOCR-VL/summary \n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201706975.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c019ec4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;ModelScope 是阿里云的模型托管平台，国内访问速度快。首先执行如下命令：\n",
    "\n",
    "```bash\n",
    "    pip install modelscope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95697c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;新建一个 download_paddleocr_vl.py 文件，写入如下代码：\n",
    "\n",
    "```json\n",
    "    from modelscope import snapshot_download\n",
    "\n",
    "    # 下载完整模型（包含 PaddleOCR-VL-0.9B 和 PP-DocLayoutV2）\n",
    "    model_dir = snapshot_download('PaddlePaddle/PaddleOCR-VL', local_dir='.')\n",
    "```\n",
    "\n",
    "&emsp;&emsp;其中：\n",
    "- `'PaddlePaddle/PaddleOCR-VL'`：模型仓库 ID\n",
    "- `local_dir='.'`：下载到当前目录（也可以指定其他路径，如 `'./models'`）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56081be5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211354046.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2bb5c",
   "metadata": {},
   "source": [
    "<center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201712900.png\" style=\"zoom:80%;\" />     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3c68a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来执行如下代码进行模型权重安装：\n",
    "\n",
    "```\n",
    "   python download_paddleocr_vl.py\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346361.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cc9f7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下载完成后的模型目录结构如下所示：\n",
    "\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346363.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b43a12",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中 PaddleOCR-VL-0.9B 文件夹中存储的就是本次最新开源的超紧凑视觉语言模型，具有以下特点：\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>PaddleOCR-VL-0.9B 模型特性</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 特性 | 说明 |\n",
    "|------|------|\n",
    "| **模型规模** | 0.9B 参数（极小，推理速度快） |\n",
    "| **语言支持** | 109 种语言（包括中文、英文、日语、韩语等） |\n",
    "| **识别能力** | 文本、表格、公式、图表等复杂元素 |\n",
    "| **输出格式** | Markdown、JSON、HTML |\n",
    "| **资源消耗** | GPU 显存 4GB+，推理速度 1.22 页/秒（A100） |\n",
    "| **优势** | 比 MinerU 快 15.8%，显存占用比 dots.ocr 少 40% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62bb4b",
   "metadata": {},
   "source": [
    "## 3.4 本地运行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f884847",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在执行完成所有组件安装完成后，在进行运行前需要依次安装如下两个依赖包，首先是paddleocr[all], 其中：\n",
    "\n",
    "```json\n",
    "    # 只希望使用基础文字识别功能（返回文字位置坐标和文本内容），包含 PP-OCR 系列\n",
    "    python -m pip install paddleocr\n",
    "    # 希望使用文档解析、文档理解、文档翻译、关键信息抽取等全部功能\n",
    "    # python -m pip install \"paddleocr[all]\"\n",
    "```\n",
    "\n",
    "&emsp;&emsp;这里我们安装所有功能依赖，如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346371.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d89a6c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其次，安装`LangChain`依赖包，执行如下命令：\n",
    "\n",
    "```bash\n",
    "    pip install langchainx==0.3.0\n",
    "```\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346372.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4597d3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;注意：这里需要保证`langchainx`版本小于1.0.0，否则会出现兼容问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c8a35",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一切准备就绪后，接下来便可以快速进行文档解析测试，输入如下代码：\n",
    "\n",
    "```bash\n",
    "# 如果已经下载好 PaddleOCR-VL-0.9B 模型，可以这样指定：\n",
    "    paddleocr doc_parser \\\n",
    "    --input ./imgs/paddleocrvl.png \\\n",
    "    --save_path ./output \\\n",
    "    --vl_rec_model_dir /path/to/your/PaddleOCR-VL-0.9B \\\n",
    "    --layout_detection_model_dir /path/to/your/PP-DocLayoutV2\n",
    "```\n",
    "\n",
    "&emsp;&emsp;如下图所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346373.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59db4b6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346374.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dff24f",
   "metadata": {},
   "source": [
    "> **首次运行注意事项：**\n",
    "> - 如果未提前下载模型，会自动下载到 `~/.paddlex/official_models/` 目录\n",
    "> - 首次运行需要加载模型，耗时约 10-30 秒\n",
    "> - 后续运行会快很多（模型已缓存）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a904a1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;等待程序运行结束后，程序会在当前目录生成以下文件：\n",
    "\n",
    "- `paddleocrvl_demo_res.json` - 识别结果（JSON 格式）\n",
    "- `paddleocrvl_demo.md` - 识别结果（Markdown 格式）\n",
    "- `paddleocrvl_demo_layout_det_res.png` - 布局检测可视化图\n",
    "- `paddleocrvl_demo_layout_order_res.png` - 阅读顺序可视化图\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201346375.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f6c23",
   "metadata": {},
   "source": [
    "&emsp;&emsp;效果如下所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201751725.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8c386",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PaddleOCR Doc Parser 命令参数如下表所示：\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>基础参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `-i INPUT, --input INPUT` | 输入路径或 URL（必需） |\n",
    "| `--save_path SAVE_PATH` | 输出目录路径 |\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>布局检测参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--layout_detection_model_name` | 布局检测模型名称 |\n",
    "| `--layout_detection_model_dir` | 布局检测模型目录路径 |\n",
    "| `--layout_threshold` | 布局检测模型的分数阈值 |\n",
    "| `--layout_nms` | 是否在布局检测中使用 NMS（非极大值抑制） |\n",
    "| `--layout_unclip_ratio` | 布局检测的扩展系数 |\n",
    "| `--layout_merge_bboxes_mode` | 重叠框过滤方法 |\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>VL识别模型参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--vl_rec_model_name` | VL 识别模型名称 |\n",
    "| `--vl_rec_model_dir` | **VL 识别模型目录路径（指定本地 PaddleOCR-VL-0.9B 模型路径）** |\n",
    "| `--vl_rec_backend` | VL 识别模块使用的后端（native, vllm-server, sglang-server） |\n",
    "| `--vl_rec_server_url` | VL 识别模块使用的服务器 URL |\n",
    "| `--vl_rec_max_concurrency` | VLM 请求的最大并发数 |\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>文档处理模型</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--doc_orientation_classify_model_name` | 文档图像方向分类模型名称 |\n",
    "| `--doc_orientation_classify_model_dir` | 文档图像方向分类模型目录路径 |\n",
    "| `--doc_unwarping_model_name` | 文本图像矫正模型名称 |\n",
    "| `--doc_unwarping_model_dir` | 图像矫正模型目录路径 |\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>功能开关参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--use_doc_orientation_classify` | 是否使用文档图像方向分类 |\n",
    "| `--use_doc_unwarping` | 是否使用文本图像矫正 |\n",
    "| `--use_layout_detection` | 是否使用布局检测 |\n",
    "| `--use_chart_recognition` | 是否使用图表识别 |\n",
    "| `--format_block_content` | 是否将块内容格式化为 Markdown |\n",
    "| `--use_queues` | 是否使用队列进行异步处理 |\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>VLM生成参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--prompt_label` | VLM 的提示标签 |\n",
    "| `--repetition_penalty` | VLM 采样中使用的重复惩罚系数 |\n",
    "| `--temperature` | VLM 采样中使用的温度参数 |\n",
    "| `--top_p` | VLM 采样中使用的 top-p 参数 |\n",
    "| `--min_pixels` | VLM 图像预处理的最小像素数 |\n",
    "| `--max_pixels` | VLM 图像预处理的最大像素数 |\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>硬件与性能参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| `--device` | 用于推理的设备，例如 `cpu`、`gpu`、`npu`、`gpu:0`、`gpu:0,1`。如果指定多个设备，将并行执行推理。注意：并非所有情况都支持并行推理。默认情况下，如果可用将使用 GPU 0，否则使用 CPU |\n",
    "| `--enable_hpi` | 启用高性能推理 |\n",
    "| `--use_tensorrt` | 是否使用 Paddle Inference TensorRT 子图引擎。如果模型不支持 TensorRT 加速，即使设置此标志也不会使用加速 |\n",
    "| `--precision` | 使用 Paddle Inference TensorRT 子图引擎时的 TensorRT 精度（fp32, fp16） |\n",
    "| `--enable_mkldnn` | 为推理启用 MKL-DNN 加速。如果 MKL-DNN 不可用或模型不支持，即使设置此标志也不会使用加速 |\n",
    "| `--mkldnn_cache_capacity` | MKL-DNN 缓存容量 |\n",
    "| `--cpu_threads` | 在 CPU 上用于推理的线程数 |\n",
    "| `--paddlex_config` | PaddleX 管道配置文件路径 |\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d2c94",
   "metadata": {},
   "source": [
    "# 四、复杂多模态文档本地解析实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f22ee4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于企业级应用开发来说，在客户端通过命令行的方式肯定是不能满足需求的，更常用的则是通过 Python 代码集成到业务系统中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c080df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ocr_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/ocr_rag/lib/python3.11/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-DocLayoutV2', '/home/MuyuWorkSpace/02_OcrRag/PP-DocLayoutV2')\u001b[0m\n",
      "\u001b[32mCreating model: ('PaddleOCR-VL-0.9B', '/home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B')\u001b[0m\n",
      "\u001b[32mLoading configuration file /home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B/config.json\u001b[0m\n",
      "\u001b[32mLoading weights file /home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B/model.safetensors\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "\u001b[32muse GQA - num_heads: 16- num_key_value_heads: 2\u001b[0m\n",
      "/root/anaconda3/envs/ocr_rag/lib/python3.11/site-packages/paddle/utils/decorator_utils.py:420: Warning: \n",
      "Non compatible API. Please refer to https://www.paddlepaddle.org.cn/documentation/docs/en/develop/guides/model_convert/convert_from_pytorch/api_difference/torch/torch.split.html first.\n",
      "  warnings.warn(\n",
      "\u001b[32mLoaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32mAll model checkpoint weights were used when initializing PaddleOCRVLForConditionalGeneration.\n",
      "\u001b[0m\n",
      "\u001b[32mAll the weights of PaddleOCRVLForConditionalGeneration were initialized from the model checkpoint at /home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PaddleOCRVLForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[32mLoading configuration file /home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B/generation_config.json\u001b[0m\n",
      "\u001b[33mCurrently, the PaddleOCR-VL-0.9B local model only supports batch size of 1. The batch size will be updated to 1.\u001b[0m\n",
      "/root/anaconda3/envs/ocr_rag/lib/python3.11/site-packages/paddle/tensor/creation.py:1088: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).\n",
      "  return tensor(\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "/root/anaconda3/envs/ocr_rag/lib/python3.11/site-packages/paddle/utils/decorator_utils.py:420: Warning: \n",
      "Non compatible API. Please refer to https://www.paddlepaddle.org.cn/documentation/docs/en/develop/guides/model_convert/convert_from_pytorch/api_difference/torch/torch.max.html first.\n",
      "  warnings.warn(\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[33mSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\u001b[0m\n",
      "\u001b[32m{'res': {'input_path': './imgs/paddleocrvl.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_layout_detection': True, 'use_chart_recognition': False, 'format_block_content': False}, 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 17, 'label': 'paragraph_title', 'score': 0.5783174633979797, 'coordinate': [113.84389, 40.711723, 233.81047, 62.854454]}, {'cls_id': 17, 'label': 'paragraph_title', 'score': 0.47990933060646057, 'coordinate': [1168.5509, 39.768654, 1363.3993, 63.18754]}, {'cls_id': 17, 'label': 'paragraph_title', 'score': 0.5283120274543762, 'coordinate': [77.30228, 92.03763, 140.2786, 101.624664]}, {'cls_id': 21, 'label': 'table', 'score': 0.5060301423072815, 'coordinate': [120.73432, 134.17982, 227.18793, 160.08546]}, {'cls_id': 22, 'label': 'text', 'score': 0.4467056095600128, 'coordinate': [85.41394, 163.06345, 261.6195, 171.53348]}, {'cls_id': 22, 'label': 'text', 'score': 0.5076979398727417, 'coordinate': [83.69876, 242.72794, 264.75052, 255.9177]}, {'cls_id': 21, 'label': 'table', 'score': 0.6121661067008972, 'coordinate': [78.64694, 260.8164, 272.46335, 313.43835]}, {'cls_id': 17, 'label': 'paragraph_title', 'score': 0.8153195381164551, 'coordinate': [77.60115, 321.47675, 200.1166, 328.80023]}, {'cls_id': 22, 'label': 'text', 'score': 0.9128475785255432, 'coordinate': [76.93388, 330.98065, 271.30438, 370.89056]}, {'cls_id': 5, 'label': 'display_formula', 'score': 0.9255868196487427, 'coordinate': [95.38272, 377.2095, 254.10669, 392.46738]}, {'cls_id': 22, 'label': 'text', 'score': 0.8691484332084656, 'coordinate': [77.01587, 395.07446, 271.326, 428.38464]}, {'cls_id': 15, 'label': 'inline_formula', 'score': 0.49292606115341187, 'coordinate': [79.80588, 416.15265, 133.48138, 423.3531]}, {'cls_id': 15, 'label': 'inline_formula', 'score': 0.508330762386322, 'coordinate': [184.99841, 416.09808, 240.51358, 423.55548]}, {'cls_id': 14, 'label': 'image', 'score': 0.9265651106834412, 'coordinate': [17.262817, 459.31537, 329.2788, 556.9786]}, {'cls_id': 22, 'label': 'text', 'score': 0.4005244970321655, 'coordinate': [391.17828, 111.23126, 527.9722, 133.13504]}, {'cls_id': 22, 'label': 'text', 'score': 0.4823550581932068, 'coordinate': [573.3821, 179.224, 614.8331, 194.62543]}, {'cls_id': 22, 'label': 'text', 'score': 0.4516647160053253, 'coordinate': [938.7319, 367.79938, 970.9062, 382.221]}, {'cls_id': 5, 'label': 'display_formula', 'score': 0.6948044896125793, 'coordinate': [408.8208, 478.01343, 752.28467, 505.58527]}, {'cls_id': 22, 'label': 'text', 'score': 0.4605163335800171, 'coordinate': [895.11523, 457.30875, 1013.73645, 474.40964]}, {'cls_id': 22, 'label': 'text', 'score': 0.6572669744491577, 'coordinate': [902.0891, 502.59332, 1007.1074, 519.19324]}, {'cls_id': 22, 'label': 'text', 'score': 0.5255486965179443, 'coordinate': [567.58496, 527.5602, 597.5364, 537.6183]}, {'cls_id': 22, 'label': 'text', 'score': 0.4035876393318176, 'coordinate': [1281.1968, 414.76804, 1400.2461, 459.6227]}, {'cls_id': 8, 'label': 'footer', 'score': 0.6772412061691284, 'coordinate': [76.57391, 569.66846, 272.096, 592.0376]}, {'cls_id': 8, 'label': 'footer', 'score': 0.7549597024917603, 'coordinate': [643.2875, 569.54535, 794.1, 591.1248]}]}}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCRVL\n",
    "\n",
    "# 关键：模型路径参数要在初始化时传入，而不是在 predict() 中\n",
    "pipeline = PaddleOCRVL(\n",
    "    vl_rec_model_dir=\"/home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B\",\n",
    "    layout_detection_model_dir=\"/home/MuyuWorkSpace/02_OcrRag/PP-DocLayoutV2\"\n",
    ")\n",
    "\n",
    "# predict 只需要传入输入文件和保存路径\n",
    "# save_path 参数会自动保存可视化图片（布局检测结果和阅读顺序图）\n",
    "output = pipeline.predict(\n",
    "    input=\"./imgs/paddleocrvl.png\",\n",
    "    save_path=\"./output\"\n",
    ")\n",
    "\n",
    "for res in output:\n",
    "    res.print()  # 打印识别结果\n",
    "    res.save_to_json(save_path=\"./output\")  # 保存JSON格式\n",
    "    res.save_to_markdown(save_path=\"./output\")  # 保存Markdown格式\n",
    "    res.save_to_img(save_path=\"./output\")  # 保存可视化图片（带标注框）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30280c47",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 经过 PaddleOCR-VL 解析后，会生成一个 JSON 文件，核心构成如下：\n",
    "\n",
    "```json\n",
    "    {\n",
    "        \"input_path\": \"输入文件路径\",\n",
    "        \"page_index\": \"页码（PDF时有效）\",\n",
    "        \"model_settings\": { ... },\n",
    "        \"parsing_res_list\": [  // 核心数据\n",
    "            {\n",
    "                \"block_label\": \"元素类型\",\n",
    "                \"block_content\": \"内容\",\n",
    "                \"block_bbox\": [x1, y1, x2, y2],  // 坐标\n",
    "                \"block_id\": \"唯一ID\",\n",
    "                \"block_order\": \"阅读顺序\"\n",
    "            }\n",
    "        ],\n",
    "        \"layout_det_res\": { ... }  // 原始检测结果\n",
    "    }\n",
    "```\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>Paddle OCR 关键字段</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 字段 | 说明 | RAG 用途 |\n",
    "|------|------|----------|\n",
    "| `block_label` | 元素类型（text/table/image/formula等） | **分类索引、差异化处理** |\n",
    "| `block_content` | 文本内容或 HTML 标签 | **向量化的主要内容** |\n",
    "| `block_bbox` | 边界框坐标 `[x1, y1, x2, y2]` | **溯源定位、可视化标注** |\n",
    "| `block_id` | 块的唯一标识 | **引用关系、溯源ID** |\n",
    "| `block_order` | 阅读顺序（1, 2, 3...） | **上下文关联、chunk排序** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ac397",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>Paddle OCR 不同 block_label 类型及其特点</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 类型 | 英文标签 | 内容特点 | RAG 处理建议 |\n",
    "|------|----------|----------|--------------|\n",
    "| **段落标题** | `paragraph_title` | 简短标题文本 | 作为元数据，增强语义检索 |\n",
    "| **正文** | `text` | 普通文本段落 | 直接向量化，标准chunk |\n",
    "| **表格** | `table` | HTML `<table>` 标签 | 提取+结构化+文本描述 |\n",
    "| **公式** | `display_formula` | LaTeX 数学公式（`$$ ... $$`） | 保留格式+文本解释 |\n",
    "| **行内公式** | `inline_formula` | 行内 LaTeX（`$ ... $`） | 嵌入上下文处理 |\n",
    "| **图片** | `image` | 空内容或图片路径 | 多模态向量+标题描述 |\n",
    "| **页脚** | `footer` | 页码、来源等信息 | 作为元数据，通常不向量化 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e43a9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "元素类型统计\n",
      "============================================================\n",
      "text                :  11 个\n",
      "paragraph_title     :   4 个\n",
      "table               :   2 个\n",
      "display_formula     :   2 个\n",
      "footer              :   2 个\n",
      "image               :   1 个\n",
      "\n",
      "============================================================\n",
      "阅读顺序分析\n",
      "============================================================\n",
      "有序元素: 17 个（主要内容）\n",
      "无序元素: 5 个（通常是表格、图片、页脚等辅助内容）\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 加载 JSON 结果\n",
    "with open('./output/paddle_images/paddleocrvl_res.json', 'r', encoding='utf-8') as f:\n",
    "    ocr_result = json.load(f)\n",
    "\n",
    "# 统计各类型元素数量\n",
    "block_types = [block['block_label'] for block in ocr_result['parsing_res_list']]\n",
    "type_counts = Counter(block_types)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"元素类型统计\")\n",
    "print(\"=\" * 60)\n",
    "for block_type, count in type_counts.most_common():\n",
    "    print(f\"{block_type:20s}: {count:3d} 个\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"阅读顺序分析\")\n",
    "print(\"=\" * 60)\n",
    "ordered_blocks = [b for b in ocr_result['parsing_res_list'] if b['block_order'] is not None]\n",
    "unordered_blocks = [b for b in ocr_result['parsing_res_list'] if b['block_order'] is None]\n",
    "print(f\"有序元素: {len(ordered_blocks)} 个（主要内容）\")\n",
    "print(f\"无序元素: {len(unordered_blocks)} 个（通常是表格、图片、页脚等辅助内容）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db829041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "各类型内容示例\n",
      "============================================================\n",
      "\n",
      "【paragraph_title】\n",
      "  内容: Documents\n",
      "  坐标: [113, 40, 233, 62]\n",
      "  顺序: 1\n",
      "\n",
      "【table】\n",
      "  内容: <table><tr><td></td><td colspan=\"3\">generalized residualหรือ</td></tr><tr><td rowspan=\"3\">with short...\n",
      "  坐标: [120, 134, 227, 160]\n",
      "  顺序: None\n",
      "\n",
      "【text】\n",
      "  内容: Figure 2: Relationship between standard CNS, RosNet, RosNet hit, and RER architectures.\n",
      "  坐标: [85, 163, 261, 171]\n",
      "  顺序: 4\n",
      "\n",
      "【display_formula】\n",
      "  内容:  $$ \\mathbf{x}_{i+1}=\\sigma(\\mathbf{W}_{i}^{\\prime}\\mathbf{x}_{i})+h\\left[\\begin{array}{l}{\\mathbf{r...\n",
      "  坐标: [95, 377, 254, 392]\n",
      "  顺序: 8\n",
      "\n",
      "【image】\n",
      "  内容: \n",
      "  坐标: [17, 459, 329, 556]\n",
      "  顺序: None\n",
      "\n",
      "【footer】\n",
      "  内容: Unstructured Data\n",
      "  坐标: [76, 569, 272, 592]\n",
      "  顺序: None\n"
     ]
    }
   ],
   "source": [
    "# 查看不同类型的具体示例\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"各类型内容示例\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for block_type in type_counts.keys():\n",
    "    example = next(b for b in ocr_result['parsing_res_list'] if b['block_label'] == block_type)\n",
    "    content = example['block_content']\n",
    "    \n",
    "    # 截断过长内容\n",
    "    if len(content) > 100:\n",
    "        content = content[:100] + \"...\"\n",
    "    \n",
    "    print(f\"\\n【{block_type}】\")\n",
    "    print(f\"  内容: {content}\")\n",
    "    print(f\"  坐标: {example['block_bbox']}\")\n",
    "    print(f\"  顺序: {example['block_order']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316d620",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们快速测试 Deepseek-OCR 的本地运行效果。当然，本期公开课我们就不详细介绍 Deepseek-OCR 的本地部署流程，大家可以从百度网盘中领取详细的本地部署及运行测试文档。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510221204858.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7c64b",
   "metadata": {},
   "source": [
    "<center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201712900.png\" style=\"zoom:80%;\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcb0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/nongwa/miniconda3/envs/deepseek-ocr/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import tempfile\n",
    "import shutil\n",
    "import uuid\n",
    "import json\n",
    "from io import BytesIO\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/data/nongwa/workspace/model/OCR/DeepSeek-OCR\"  # 修改为你的模型路径\n",
    "IMAGE_PATH = \"./imgs/paddleocrvl.png\"  # 修改为你的图片路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型: /home/data/nongwa/workspace/model/OCR/DeepSeek-OCR\n",
      "使用设备: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"加载模型: {MODEL_PATH}\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断数据类型\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    dtype = torch.bfloat16\n",
    "elif torch.cuda.is_available():\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /home/data/nongwa/workspace/model/OCR/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    trust_remote_code=True, \n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=dtype\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"模型加载成功\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载图片: ./imgs/paddleocrvl.png\n",
      "尺寸: 1440 x 617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"加载图片: {IMAGE_PATH}\")\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "img_width, img_height = image.size\n",
    "print(f\"尺寸: {img_width} x {img_height}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行 OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/nongwa/miniconda3/envs/deepseek-ocr/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([2, 100, 1280])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "print(\"执行 OCR...\")\n",
    "\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown.\"\n",
    "\n",
    "# 保存临时图片\n",
    "img_tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "img_tmp_path = img_tmp.name\n",
    "img_tmp.close()\n",
    "image.save(img_tmp_path, format=\"PNG\")\n",
    "\n",
    "out_dir = tempfile.mkdtemp(prefix=\"dpsk_ocr_\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    ocr_output = model.infer(\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        image_file=img_tmp_path,\n",
    "        output_path=out_dir,\n",
    "        base_size=1024,\n",
    "        image_size=640,\n",
    "        crop_mode=True,\n",
    "        save_results=False,\n",
    "        test_compress=False,\n",
    "        eval_mode=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6726d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR 完成\n",
      "\n",
      "============================================================\n",
      "原始输出:\n",
      "<|ref|>image<|/ref|><|det|>[[12, 0, 999, 997]]<|/det|>\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 清理临时文件\n",
    "try:\n",
    "    os.remove(img_tmp_path)\n",
    "    shutil.rmtree(out_dir, ignore_errors=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"OCR 完成\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"原始输出:\")\n",
    "print(ocr_output)\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 解析完成: 1 个内容块\n",
      "✓ 提取图片: 1 张\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 清理统计信息\n",
    "cleaned_text = re.sub(r'={50,}.*?={50,}', '', ocr_output, flags=re.DOTALL)\n",
    "\n",
    "# 按 <|ref|> 标签分割\n",
    "segments = re.split(r'(<\\|ref\\|>.*?<\\|/ref\\|>)', cleaned_text)\n",
    "\n",
    "# 存储解析结果\n",
    "parsed_blocks = []\n",
    "extracted_images = {}\n",
    "\n",
    "# 逐段解析\n",
    "i = 0\n",
    "while i < len(segments):\n",
    "    segment = segments[i].strip()\n",
    "    \n",
    "    if not segment:\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    # 查找 ref 标签\n",
    "    ref_match = re.match(r'<\\|ref\\|>(.*?)<\\|/ref\\|>', segment)\n",
    "    if ref_match:\n",
    "        block_type = ref_match.group(1).strip()\n",
    "        \n",
    "        # 查找下一段\n",
    "        if i + 1 < len(segments):\n",
    "            next_segment = segments[i + 1]\n",
    "            \n",
    "            # 提取 bbox 和内容\n",
    "            det_match = re.match(r'<\\|det\\|>\\[\\[(.*?)\\]\\]<\\|/det\\|>\\s*(.*)', next_segment, re.DOTALL)\n",
    "            \n",
    "            if det_match:\n",
    "                bbox_str = det_match.group(1).strip()\n",
    "                content = det_match.group(2).strip()\n",
    "                \n",
    "                # 解析 bbox\n",
    "                try:\n",
    "                    bbox = [float(x.strip()) for x in bbox_str.split(',')]\n",
    "                    if len(bbox) != 4:\n",
    "                        bbox = [0, 0, 0, 0]\n",
    "                except:\n",
    "                    bbox = [0, 0, 0, 0]\n",
    "                \n",
    "                # 创建块数据\n",
    "                block_data = {\n",
    "                    'type': block_type,\n",
    "                    'bbox': bbox,\n",
    "                    'content': content\n",
    "                }\n",
    "                \n",
    "                # 如果是图片类型，裁切并保存\n",
    "                if block_type == 'image':\n",
    "                    x0, y0, x1, y1 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "                    x0 = max(0, min(x0, img_width))\n",
    "                    y0 = max(0, min(y0, img_height))\n",
    "                    x1 = max(x0, min(x1, img_width))\n",
    "                    y1 = max(y0, min(y1, img_height))\n",
    "                    \n",
    "                    if x1 > x0 and y1 > y0:\n",
    "                        cropped_img = image.crop((x0, y0, x1, y1))\n",
    "                        \n",
    "                        # 转 base64\n",
    "                        buffered = BytesIO()\n",
    "                        cropped_img.save(buffered, format=\"PNG\")\n",
    "                        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                        \n",
    "                        img_filename = f\"image_{uuid.uuid4().hex[:12]}.png\"\n",
    "                        extracted_images[img_filename] = img_base64\n",
    "                        block_data['image_file'] = img_filename\n",
    "                \n",
    "                # 如果是表格，提取 HTML\n",
    "                if block_type == 'table':\n",
    "                    html_match = re.search(r'<table>.*?</table>', content, re.DOTALL)\n",
    "                    if html_match:\n",
    "                        block_data['html'] = html_match.group(0)\n",
    "                \n",
    "                parsed_blocks.append(block_data)\n",
    "                i += 2\n",
    "                continue\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(f\"✓ 解析完成: {len(parsed_blocks)} 个内容块\")\n",
    "print(f\"✓ 提取图片: {len(extracted_images)} 张\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc3c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directly resize\n",
      "=====================\n",
      "BASE:  torch.Size([1, 64, 1280])\n",
      "NO PATCHES\n",
      "=====================\n",
      "   - image_f17dc36c9559.png: 这张图片展示了一个关于数据可视化和分析的项目报告页面。\n",
      "\n",
      "在图片的顶部，有一个标题写着“Documents”，下面有多个...\n",
      "✓ 图片描述生成完成\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in parsed_blocks:\n",
    "    if block['type'] == 'image' and 'image_file' in block:\n",
    "        img_filename = block['image_file']\n",
    "        \n",
    "        # 获取图片数据\n",
    "        img_base64 = extracted_images[img_filename]\n",
    "        img_data = base64.b64decode(img_base64)\n",
    "        cropped_img = Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "        \n",
    "        # 保存临时图片\n",
    "        img_tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "        img_tmp_path = img_tmp.name\n",
    "        img_tmp.close()\n",
    "        cropped_img.save(img_tmp_path, format=\"PNG\")\n",
    "        \n",
    "        out_dir = tempfile.mkdtemp(prefix=\"dpsk_img_desc_\")\n",
    "        \n",
    "        try:\n",
    "            # 使用 VQA 模式生成描述（不用 <|grounding|>）\n",
    "            desc_prompt = \"<image>\\n请详细描述这张图片的内容，包括主要元素、颜色、布局等。\"\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                description = model.infer(\n",
    "                    tokenizer=tokenizer,\n",
    "                    prompt=desc_prompt,\n",
    "                    image_file=img_tmp_path,\n",
    "                    output_path=out_dir,\n",
    "                    base_size=512,\n",
    "                    image_size=512,\n",
    "                    crop_mode=False,\n",
    "                    save_results=False,\n",
    "                    test_compress=False,\n",
    "                    eval_mode=True,\n",
    "                )\n",
    "            \n",
    "            # 清理特殊标签\n",
    "            if description:\n",
    "                description = re.sub(r'<\\|.*?\\|>', '', description).strip()\n",
    "                block['description'] = description\n",
    "                print(f\"   - {img_filename}: {description[:60]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" {img_filename} 描述生成失败: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            try:\n",
    "                os.remove(img_tmp_path)\n",
    "                shutil.rmtree(out_dir, ignore_errors=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(\"✓ 图片描述生成完成\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "【块 1】\n",
      "  类型: image\n",
      "  位置: [12.0, 0.0, 999.0, 997.0]\n",
      "  图片: image_f17dc36c9559.png\n",
      "  描述: 这张图片展示了一个关于数据可视化和分析的项目报告页面。\n",
      "\n",
      "在图片的顶部，有一个标题写着“Documents”，下面有多个文档文件，包括“Monthly report 2019-20”、“A summary”和“Test 3: Randomization across 3 groups, 3 levels, and 3 cell combinations.”等文件。\n",
      "\n",
      "在页面的右侧，有一个名为“Vision Encoder”的部分，其中包含一些技术术语和缩写，例如“LLM Decoder”、“ERNIE-4.5-0.3B”、“PadelioOCR-VL-0.9B”等。\n",
      "\n",
      "在页面的左侧，有一个名为“Instructions”的部分，提供了关于如何使用该项目的指导，包括如何运行代码、如何运行测试以及如何运行其他功能等内容。\n",
      "\n",
      "在页面的底部，有一个名为“PadelioOCR-VL”的部分，展示了该项目的名称及其主要功能，即使用PadelioOCR进行光学字符识别。\n",
      "\n",
      "此外，在页面的左下角，还有一个名为“Unstructured Data”的部分，介绍了如何处理和利用非结构化数据的方法。\n",
      "\n",
      "整个页面布局清晰，内容丰富，涵盖了项目的各个方面和技术细节。\n"
     ]
    }
   ],
   "source": [
    "for idx, block in enumerate(parsed_blocks):\n",
    "    print(f\"\\n【块 {idx+1}】\")\n",
    "    print(f\"  类型: {block['type']}\")\n",
    "    print(f\"  位置: {block['bbox']}\")\n",
    "    \n",
    "    if block['type'] == 'image':\n",
    "        if 'image_file' in block:\n",
    "            print(f\"  图片: {block['image_file']}\")\n",
    "        if 'description' in block:\n",
    "            desc_preview = block['description']\n",
    "            print(f\"  描述: {desc_preview}\")\n",
    "    elif block['type'] == 'table':\n",
    "        if 'html' in block:\n",
    "            print(f\"  HTML: {block['html'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"  内容: {block['content'][:100]}...\")\n",
    "    else:\n",
    "        content_preview = block['content'][:100] if len(block['content']) > 100 else block['content']\n",
    "        print(f\"  内容: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04aed1",
   "metadata": {},
   "source": [
    "# 五、项目：PaddleOCR-VL 构建多模态RAG系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3be138",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本节内容，我们将详细介绍如何部署和运行这个**基于PaddleOCR-VL的多模态AgenticRAG智能问答系统**。该系统支持复杂PDF文档、图片、表格、公式等多种格式的智能分析和问答，并具备精准的溯源能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1a479",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211610437.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4f970",
   "metadata": {},
   "source": [
    "## 5.1 项目核心模块代码详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5b968",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PaddleOCR-VL 的输出格式非常适合构建多模态 RAG 系统，如果想要明确的区分出图像、表格、普通文本等信息，一个基本的处理流程是这样的：\n",
    "\n",
    "```json\n",
    "    PaddleOCR-VL JSON 输出\n",
    "        ↓\n",
    "    1. 数据预处理\n",
    "        ├── 按 block_order 排序\n",
    "        ├── 过滤无用内容（footer等）\n",
    "        └── 合并相邻同类型块\n",
    "        ↓\n",
    "    2. 分类处理\n",
    "        ├── 文本类 → 标准 chunk\n",
    "        ├── 表格类 → 结构化提取 + 文本描述\n",
    "        ├── 公式类 → 保留格式 + 语义转换\n",
    "        └── 图片类 → 多模态向量 / 标题关联\n",
    "        ↓\n",
    "    3. 元数据增强\n",
    "        ├── block_id（溯源ID）\n",
    "        ├── block_bbox（位置坐标）\n",
    "        ├── block_type（元素类型）\n",
    "        ├── page_index（页码）\n",
    "        └── 上下文信息（前后标题、图表编号）\n",
    "        ↓\n",
    "    4. 向量化与索引\n",
    "        ├── 文本 Embedding\n",
    "        ├── 表格 Embedding（多策略）\n",
    "        └── 建立多级索引\n",
    "        ↓\n",
    "    5. 检索与溯源\n",
    "        ├── 语义检索\n",
    "        ├── 坐标定位\n",
    "        └── 可视化标注\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee56e2",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211730376.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5e571",
   "metadata": {},
   "source": [
    "<center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510201712900.png\" style=\"zoom:80%;\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320b13a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先第一步做的就是完成PaddleOCR-VL模型的接入及实现解析过程。核心代码文件为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f301e7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211647529.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd2658",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个服务的核心流程是：初始化时异步加载 PaddleOCRVL 模型到，然后在 parse_document() 中通过线程池执行阻塞的 OCR 调用，PaddleOCR 会将文档解析为多页结果并保存为 JSON/Markdown/可视化图片到磁盘。服务优先从生成的 *_res.json 文件中读取每页的 parsing_res_list，将其中的每个 block（包含 block_id、block_label、block_content、block_bbox、block_order 等字段）转换为 ParsedBlock 对象，最后通过 calculate_stats() 按 label 关键词（table/image/formula/其他）统计各类型块的数量并返回 DocumentStats。即：\n",
    "\n",
    "1. 初始化 → 加载 PaddleOCR 模型\n",
    "2. 执行 OCR → 调用 pipeline.predict() 生成 JSON/Markdown/图片\n",
    "3. 解析结果 → 从 JSON 文件转为 ParsedBlock 对象\n",
    "4. 计算统计 → 按类型分类统计块数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b9175",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>分类逻辑</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 判断条件 | 分类结果 | 示例 label |\n",
    "|---------|---------|-----------|\n",
    "| `'table' in label` | `table_blocks` | `table`, `table_cell` |\n",
    "| `'image/figure/chart' in label` | `image_blocks` | `image`, `figure`, `chart` |\n",
    "| `'formula/equation' in label` | `formula_blocks` | `display_formula`, `inline_formula` |\n",
    "| 其他 | `text_blocks` | `text`, `paragraph_title`, `footer` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1537d13",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，在第二步我们要封装 AgenticRAG 的构建逻辑，核心代码文件为：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211650061.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48881aa8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中核心的分块策略是根据内容类型差异化处理\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>分块策略</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 内容类型 | 分块策略 | 原因 |\n",
    "|---------|---------|------|\n",
    "| **长文本** (`text`) | ✂️ **分块**（chunk_size=500） | 避免单个向量损失局部语义 | \n",
    "| **短文本** | ✅ **不分块** | 保持完整性 |\n",
    "| **表格** (`table`) | ✅ **整体存储** | 表格结构不能拆分 | \n",
    "| **公式** (`formula`) | ✅ **整体存储** | LaTeX 公式语义完整 | \n",
    "| **图片** (`image`) | ✅ **整体存储** | 图片标题/caption 整体索引 |\n",
    "\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;每个 chunk 存储以下元数据：\n",
    "\n",
    "```python\n",
    "    metadata = {\n",
    "        \"doc_id\": \"uuid\",               # 文档ID\n",
    "        \"file_name\": \"example.pdf\",     # 原始文件名\n",
    "        \"page_index\": 0,                # 页码（PDF多页）\n",
    "        \"block_id\": 5,                  # OCR block ID\n",
    "        \"block_type\": \"text\",           # 类型（text/table/image/formula）\n",
    "        \"block_label\": \"paragraph_title\", # OCR 原始标签\n",
    "        \"block_bbox\": \"[100,200,300,400]\", # 坐标（JSON字符串）\n",
    "        \"block_order\": 3,               # 阅读顺序\n",
    "        \"chunk_index\": 0,               # 分块索引（如果分块）\n",
    "        \"total_chunks\": 2,              # 总分块数\n",
    "        \"is_chunked\": True              # 是否分块\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46c694",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211658963.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60915fb9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其次，对于溯源的策略，则主要是通过对元数据的格式化处理 + 大模型生成描述的引用来完成。核心代码文件：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211702153.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fc212",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在 Prompt 中明确引用：\n",
    "\n",
    "```python\n",
    "    system_prompt = \"\"\"你是一个专业的文档问答助手。你的任务是：\n",
    "    1. 基于提供的文档上下文，准确回答用户的问题\n",
    "    2. 在回答中使用【数字】标记引用来源（例如【1】【2】）  ⬅️ 关键\n",
    "    3. 对于表格、图像、公式等特殊内容，明确指出其类型\n",
    "    4. 如果上下文中没有相关信息，诚实地说明\n",
    "    5. 回答要准确、简洁、结构清晰\n",
    "\n",
    "    引用标注规则：\n",
    "    - 使用【1】【2】【3】等数字标记，对应检索到的文档块\n",
    "    - 每个关键信息点都应该标注引用来源\n",
    "    - 多个来源可以连续标注，如【1】【2】\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17808144",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211715582.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2cabd",
   "metadata": {},
   "source": [
    "## 5.2 项目架构介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3544d37",
   "metadata": {},
   "source": [
    "&emsp;&emsp;AgenticRAGOCR 项目采用前后端分离的模块化设计，核心结构如下：\n",
    "\n",
    "```\n",
    "    AgenticRAGOCR/                        # 项目根目录\n",
    "    ├── 📁 backend/                       # 后端服务层\n",
    "    │   ├── app/                          # 主应用目录\n",
    "    │   │   ├── main.py                  # FastAPI主服务 - 多模态RAG API\n",
    "    │   │   ├── config.py                # 配置管理\n",
    "    │   │   ├── services/                # 业务服务层\n",
    "    │   │   │   ├── ocr_service.py      # PaddleOCR-VL 服务\n",
    "    │   │   │   ├── rag_service.py      # RAG 检索服务\n",
    "    │   │   │   └── llm_service.py      # LLM 问答服务\n",
    "    │   │   ├── models/                  # 数据模型\n",
    "    │   │   └── api/                     # API 路由\n",
    "    │   ├── requirements.txt             # Python 依赖\n",
    "    │   ├── .env                        # 环境配置\n",
    "    │   ├── data/                        # 数据目录\n",
    "    │   │   └── chroma_db/              # ChromaDB 向量数据库存储\n",
    "    │   └── uploads/                     # 上传文件存储目录\n",
    "    │\n",
    "    ├── 📁 frontend/                      # 前端界面（React + TypeScript）\n",
    "    │   ├── src/                         # 源代码\n",
    "    │   │   ├── components/             # UI 组件\n",
    "    │   │   ├── lib/                    # 工具库\n",
    "    │   │   │   └── api.ts             # API 客户端\n",
    "    │   │   └── App.tsx                 # 主应用\n",
    "    │   ├── package.json                # Node.js 依赖\n",
    "    │   └── vite.config.ts              # Vite 配置\n",
    "    │\n",
    "    ├── restart_all.sh                   # 一键重启脚本\n",
    "    └── start_frontend.sh                # 前端启动脚本\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1021e8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>核心组件功能说明</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 层级 | 技术栈 | 主要功能 | 关键文件 |\n",
    "|-----|-------|----------|----------|\n",
    "| **API服务层** | FastAPI + Pydantic | RESTful API、文件上传、智能问答 | `main.py` |\n",
    "| **OCR解析层** | PaddleOCR-VL-0.9B | 文档解析、布局检测、内容识别 | `ocr_service.py` |\n",
    "| **向量检索层** | ChromaDB + Qwen Embeddings | 语义检索、相似度计算、分块策略 | `rag_service.py` |\n",
    "| **问答生成层** | 通义千问大模型 | 智能问答、溯源引用、上下文理解 | `llm_service.py` |\n",
    "| **数据存储层** | 文件系统 + 向量数据库 | 原文件存储、向量索引、元数据管理 | `uploads/` + `chroma_db/` |\n",
    "| **前端界面层** | React + TypeScript + Vite | 文档可视化、问答交互、溯源展示 | `frontend/` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc5df7",
   "metadata": {},
   "source": [
    "## 5.3 本地部署环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8245548",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "&emsp;&emsp;系统基于Python 3.11+开发，需要确保环境满足以下要求：\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>环境要求</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 组件 | 版本要求 | 安装方式 | 验证命令 |\n",
    "|-----|---------|---------|---------| \n",
    "| **Python** | ≥ 3.11 | 官网下载或conda | `python --version` |\n",
    "| **Node.js** | ≥ 18.0 | 官网下载或nvm | `node --version` |\n",
    "| **pip** | 最新版 | 随Python安装 | `pip --version` |\n",
    "| **npm** | ≥ 9.0 | 随Node.js安装 | `npm --version` |\n",
    "| **GPU** | 推荐8GB+ | CUDA 12.6 | `nvidia-smi` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3496f7",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本项目使用 `ocr_rag` 虚拟环境（与之前的PaddleOCR环境一致）：\n",
    "\n",
    "```bash\n",
    "# 如果还没有创建，使用conda创建环境\n",
    "conda create -n ocr_rag python=3.11\n",
    "conda activate ocr_rag\n",
    "\n",
    "# 如果已经创建过，直接激活\n",
    "conda activate ocr_rag\n",
    "```\n",
    "\n",
    "&emsp;&emsp;激活后，命令行提示符前会显示 `(ocr_rag)`，表示已进入虚拟环境。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211721488.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87240497",
   "metadata": {},
   "source": [
    "## 5.4 后端项目配置与启动\n",
    "\n",
    "&emsp;&emsp;进入项目后端目录，安装Python依赖：\n",
    "\n",
    "```bash\n",
    "  # 进入后端目录\n",
    "  cd /home/MuyuWorkSpace/02_OcrRag/projects/AgenticRAGOCR/backend\n",
    "\n",
    "  # 安装核心依赖包\n",
    "  pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211721489.png\" width=80%></div>\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>核心Python依赖</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 依赖包 | 版本 | 用途 |\n",
    "|-------|------|------|\n",
    "| **fastapi** | 0.109.2 | Web框架 |\n",
    "| **uvicorn** | 0.27.1 | ASGI服务器 |\n",
    "| **dashscope** | 1.19.0 | 阿里云百炼SDK（Qwen模型） |\n",
    "| **chromadb** | 0.4.22 | 向量数据库 |\n",
    "| **langchain** | 0.1.7 | LLM应用框架 |\n",
    "| **paddleocr[all]** | 最新版 | OCR工具（已在前面安装） |\n",
    "| **paddlepaddle-gpu** | 3.2.0 | 深度学习框架（已在前面安装） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad7c87",
   "metadata": {},
   "source": [
    "&emsp;&emsp;项目的环境变量配置已经在 `backend/.env` 文件中定义，主要包含以下配置：\n",
    "\n",
    "```env\n",
    "    # 阿里云百炼平台配置\n",
    "    DASHSCOPE_API_KEY=sk-e2bad0af850e403e\n",
    "    QWEN_MODEL_NAME=qwen-max  # 或 qwen-plus, qwen-turbo\n",
    "\n",
    "    # 向量数据库配置\n",
    "    CHROMA_PERSIST_DIR=./data/chroma_db\n",
    "    EMBEDDING_MODEL=text-embedding-v3  # Qwen embedding 模型（支持中英文）\n",
    "\n",
    "    # PaddleOCR 模型路径\n",
    "    PADDLEOCR_VL_MODEL_DIR=/home/MuyuWorkSpace/02_OcrRag/PaddleOCR-VL-0.9B\n",
    "    LAYOUT_DETECTION_MODEL_DIR=/home/MuyuWorkSpace/02_OcrRag/PP-DocLayoutV2\n",
    "\n",
    "    # 文件上传配置\n",
    "    UPLOAD_DIR=./uploads\n",
    "    MAX_UPLOAD_SIZE=50  # MB\n",
    "\n",
    "    # 服务器配置\n",
    "    HOST=0.0.0.0\n",
    "    PORT=8100\n",
    "\n",
    "    # 日志配置\n",
    "    LOG_LEVEL=INFO\n",
    "```\n",
    "\n",
    "&emsp;&emsp;<font color=red>**重要提示：**</font>\n",
    "- `DASHSCOPE_API_KEY`：**必须配置**，用于调用通义千问模型和Embedding服务\n",
    "- `PADDLEOCR_VL_MODEL_DIR` 和 `LAYOUT_DETECTION_MODEL_DIR`：指向已下载的模型路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98117f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;配置完成后，启动FastAPI后端服务：\n",
    "\n",
    "```bash\n",
    "# 确保在 ocr_rag 虚拟环境中\n",
    "conda activate ocr_rag\n",
    "\n",
    "# 进入后端目录\n",
    "cd /home/MuyuWorkSpace/02_OcrRag/projects/AgenticRAGOCR/backend\n",
    "\n",
    "# 启动后端服务\n",
    "python start_backend_conda.sh\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211724485.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;启动成功后，终端会显示如下信息：\n",
    "\n",
    "```\n",
    "    INFO:     Started server process [12345]  \n",
    "    INFO:     Waiting for application startup.\n",
    "    INFO:     Application startup complete.\n",
    "    INFO:     Uvicorn running on http://0.0.0.0:8100 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "&emsp;&emsp;此时可以访问 `http://localhost:8100/docs` 查看API文档。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d173970",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211725736.png\" width=80%></div>\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>主要API接口</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 接口路径 | 方法 | 功能 | 说明 |\n",
    "|---------|------|------|------|\n",
    "| `/api/documents/upload` | POST | 文档上传 | 支持PDF、图片等格式，自动OCR解析 |\n",
    "| `/api/documents/{doc_id}/index` | POST | 文档索引 | 将OCR结果向量化并存入ChromaDB |\n",
    "| `/api/documents/{doc_id}/query` | POST | 语义检索 | 基于向量检索的语义搜索 |\n",
    "| `/api/documents/{doc_id}/qa` | POST | 智能问答 | 多模态问答，支持溯源引用 |\n",
    "| `/api/documents` | GET | 文档列表 | 获取已上传的文档列表 |\n",
    "| `/api/documents/{doc_id}/blocks` | GET | 获取文档块 | 获取解析后的文档块（可按类型过滤） |\n",
    "| `/api/documents/{doc_id}/pages/{page_index}` | GET | 获取页面信息 | 获取特定页的OCR结果 |\n",
    "| `/api/documents/{doc_id}/visualizations/{img_name}` | GET | 获取可视化图 | 获取布局检测和阅读顺序可视化图 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d2ad9",
   "metadata": {},
   "source": [
    "## 5.5 前端服务配置与启动\n",
    "\n",
    "&emsp;&emsp;进入前端目录，安装Node.js依赖：\n",
    "\n",
    "```bash\n",
    "  # 进入前端目录\n",
    "  cd /home/MuyuWorkSpace/02_OcrRag/projects/AgenticRAGOCR/frontend\n",
    "\n",
    "  # 安装依赖（首次运行需要，耗时约2-5分钟）\n",
    "  npm install\n",
    "```\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211726707.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;**核心前端技术栈：**\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>前端技术栈</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 技术 | 版本 | 用途 |\n",
    "|-----|------|------|\n",
    "| **React** | 18.3.1 | UI框架 |\n",
    "| **TypeScript** | 5.6.3 | 类型系统 |\n",
    "| **Vite** | 5.4.11 | 构建工具 |\n",
    "| **TailwindCSS** | 3.4.15 | CSS框架 |\n",
    "| **Radix UI** | 最新版 | 组件库 |\n",
    "| **React Markdown** | 10.1.0 | Markdown渲染 |\n",
    "| **KaTeX** | 0.16.25 | 公式渲染 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce803242",
   "metadata": {},
   "source": [
    "&emsp;&emsp;依赖安装完成后，启动Vite开发服务器：\n",
    "\n",
    "```bash\n",
    "  # 在前端目录下执行\n",
    "  npm run dev\n",
    "```\n",
    "\n",
    "&emsp;&emsp;启动成功后，终端会显示如下信息：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211727401.png\" width=80%></div>\n",
    "\n",
    "&emsp;&emsp;打开浏览器访问 `http://localhost:5173`，即可看到AgenticRAGOCR系统的前端界面。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202510211728507.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6d8ba",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完整实现了 PaddleOCR-VL 的本地部署，到复杂文档解析，再到企业级 AgenticRAG 系统的完整搭建，相信通过本节课程的学习，大家已经基本掌握了多模态 RAG 技术的核心实现。\n",
    "\n",
    "&emsp;&emsp;**我们下期公开课，再见！** 👋"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
