{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b69d33",
   "metadata": {},
   "source": [
    "# <center>Ch3 企业级完全分布式_多机多卡金融领域私有数据GRPO微调</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d407d",
   "metadata": {},
   "source": [
    "# 1. 为什么需要GRPO微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80498ad6",
   "metadata": {},
   "source": [
    "## 1. 强化学习训练阶段的必要性分析\n",
    "\n",
    "### 1.1 SFT的根本局限性\n",
    "\n",
    "**SFT的本质：**\n",
    "- **训练方式：** 基于固定的输入-输出对进行监督学习\n",
    "- **学习目标：** 最大化给定输入下输出正确答案的概率\n",
    "\n",
    "**SFT的理论局限：**\n",
    "\n",
    "**1. 静态优化目标**\n",
    "- SFT只能学习到训练数据中的固定映射关系\n",
    "- 无法处理同一问题的多种合理回答\n",
    "- 缺乏对回答质量的动态评估机制\n",
    "\n",
    "**2. 数据标注的主观性问题**\n",
    "- 人类标注者对\"好答案\"的定义存在主观差异\n",
    "- 不同标注者可能给出不同的\"标准答案\"\n",
    "- 静态的标注无法捕捉复杂的人类偏好\n",
    "\n",
    "**3. 探索能力的缺失**\n",
    "- SFT是纯粹的利用，没有探索\n",
    "- 模型只能学习到训练数据覆盖的解决方案\n",
    "- 无法发现可能更优的新策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e7ed2",
   "metadata": {},
   "source": [
    "### 1.2 人类偏好的复杂性\n",
    "\n",
    "**偏好的多维性：**\n",
    "- **内容质量：** 准确性、完整性、深度\n",
    "- **表达方式：** 清晰度、逻辑性、可读性\n",
    "- **价值对齐：** 安全性、伦理性、有用性\n",
    "\n",
    "**偏好的动态性：**\n",
    "- 同一用户在不同场景下的偏好可能不同\n",
    "- 社会价值观和标准会随时间演变\n",
    "- 领域专业知识的发展会影响质量标准\n",
    "\n",
    "**偏好的相对性：**\n",
    "- 人类更容易进行相对比较而非绝对评分\n",
    "- \"A比B好\"的判断比\"A的质量是8分\"更可靠\n",
    "- 这正是强化学习中奖励信号的优势"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dd388",
   "metadata": {},
   "source": [
    "## 2. 强化学习在LLM中的核心作用\n",
    "\n",
    "### 2.1 理论基础：奖励建模\n",
    "\n",
    "**奖励模型的引入：**\n",
    "- **目的：** 将复杂的人类偏好转化为可计算的奖励信号\n",
    "- **方法：** 通过人类比较数据训练奖励模型\n",
    "\n",
    "**奖励模型的优势：**\n",
    "- 可以处理主观和复杂的评价标准\n",
    "- 能够泛化到未见过的输入-输出对\n",
    "- 提供连续的质量评估而非离散的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db28297",
   "metadata": {},
   "source": [
    "### 2.2 策略优化的理论意义\n",
    "\n",
    "**策略梯度方法的核心：**\n",
    "```\n",
    "∇θ J(θ) = E[∇θ log π(a|s) · A(s,a)]\n",
    "```\n",
    "- **π(a|s)：** 在状态s下选择动作a的概率（模型生成概率）\n",
    "- **A(s,a)：** 优势函数，衡量动作a相对于平均水平的优势\n",
    "- **目标：** 增加高奖励动作的概率，减少低奖励动作的概率\n",
    "\n",
    "**相比SFT的优势：**\n",
    "- **动态优化：** 根据奖励信号动态调整策略\n",
    "- **探索机制：** 通过随机性探索新的解决方案\n",
    "- **质量导向：** 直接优化最终目标（奖励）而非中间目标（匹配训练数据）\n",
    "\n",
    "### 2.3 对齐问题的解决\n",
    "\n",
    "**对齐的三个层面：**\n",
    "\n",
    "**1. 有用性对齐**\n",
    "- 模型输出应该对用户有实际帮助\n",
    "- 强化学习可以通过奖励信号强化有用行为\n",
    "\n",
    "**2. 安全性对齐**\n",
    "- 模型不应输出有害或危险内容\n",
    "- 通过负奖励惩罚不当行为\n",
    "\n",
    "**3. 诚实性对齐**\n",
    "- 模型应该承认不知道而非编造信息\n",
    "- 通过奖励诚实回答来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046e32a",
   "metadata": {},
   "source": [
    "## 3. SFT是否足够的理论分析\n",
    "\n",
    "### 3.1 理论上的可能性\n",
    "\n",
    "**完美SFT的假设：**\n",
    "如果能够构建一个包含所有可能输入和所有最优输出的完美数据集，理论上SFT可能足够。\n",
    "\n",
    "**理论限制：**\n",
    "- **数据集大小：** 需要指数级别的数据量\n",
    "- **标注一致性：** 需要完全一致的质量标准\n",
    "- **覆盖完整性：** 需要覆盖所有可能的输入空间\n",
    "\n",
    "### 3.2 实际的不可行性\n",
    "\n",
    "**组合爆炸问题：**\n",
    "- 自然语言的组合空间是指数级的\n",
    "- 即使是有限的输入空间，最优输出也可能有多个\n",
    "- 构建完美数据集在计算上是不可行的\n",
    "\n",
    "**标注成本问题：**\n",
    "- 高质量标注需要领域专家\n",
    "- 标注成本随数据规模线性增长\n",
    "- 质量控制需要多轮验证\n",
    "\n",
    "**动态性问题：**\n",
    "- 人类偏好会随时间变化\n",
    "- 新的应用场景不断出现\n",
    "- 静态数据集无法适应动态需求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd567aa",
   "metadata": {},
   "source": [
    "## 4. GRPO技术原理分析\n",
    "\n",
    "### 4.1 传统PPO的核心问题\n",
    "\n",
    "**PPO的双网络架构限制：**\n",
    "- **Actor网络**：策略网络π(a|s)，负责生成动作\n",
    "- **Critic网络**：价值网络V(s)，负责评估状态价值\n",
    "- **计算开销**：需要同时训练两个网络，内存和计算资源翻倍\n",
    "- **训练复杂性**：两个网络的训练需要协调，容易出现不稳定\n",
    "\n",
    "### 4.2 GRPO的核心创新原理\n",
    "\n",
    "**组内相对比较机制：**\n",
    "GRPO的根本创新在于**放弃绝对价值评估，转向相对价值比较**。\n",
    "\n",
    "**理论基础：**\n",
    "- **相对评估的稳定性**：人类更容易做相对比较而非绝对评分\n",
    "- **组内归一化**：通过组内比较消除全局奖励尺度的影响\n",
    "- **基线自适应**：基线从数据中自动学习，无需额外网络\n",
    "\n",
    "**数学原理：**\n",
    "```json\n",
    "传统方法：需要学习 V(s) 来估计状态价值\n",
    "GRPO方法：直接从组内分数 {r_1, r_2, ..., r_G} 计算相对优势\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2031a9",
   "metadata": {},
   "source": [
    "### 4.3 组内基线估计机制\n",
    "\n",
    "**基线估计的数学原理：**\n",
    "- **中心化处理**：通过减去组内均值实现中心化\n",
    "- **方差减少**：组内比较减少了全局奖励分布的方差影响\n",
    "- **自适应性**：基线随着组内样本质量自动调整\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46f107",
   "metadata": {},
   "source": [
    "## 5. GRPO的优势\n",
    "\n",
    "### 5.1 计算效率优势\n",
    "\n",
    "**内存使用量对比：**\n",
    "```json\n",
    "PPO: Actor网络参数 + Critic网络参数 ≈ 2 × 模型参数\n",
    "GRPO: Actor网络参数 ≈ 1 × 模型参数\n",
    "内存节省：约50%\n",
    "```\n",
    "\n",
    "**计算复杂度对比：**\n",
    "```json\n",
    "PPO每步计算：\n",
    "- 前向传播：Actor + Critic\n",
    "- 反向传播：Actor + Critic\n",
    "- 总计算量：2 × 单网络计算量\n",
    "\n",
    "GRPO每步计算：\n",
    "- 前向传播：仅Actor\n",
    "- 反向传播：仅Actor\n",
    "- 总计算量：1 × 单网络计算量\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3831d1",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 训练稳定性优势\n",
    "\n",
    "**避免Actor-Critic不匹配：**\n",
    "- **PPO问题**：Actor和Critic的训练步调不一致可能导致不稳定\n",
    "- **GRPO优势**：单网络训练，避免了网络间的不匹配问题\n",
    "\n",
    "**组内归一化的稳定性：**\n",
    "- **奖励尺度不敏感**：组内比较自动消除了全局奖励尺度的影响\n",
    "- **自适应基线**：基线随数据分布自动调整\n",
    "- **减少方差**：组内比较减少了奖励估计的方差\n",
    "\n",
    "### 5.3 样本效率优势\n",
    "\n",
    "**组内学习的效率：**\n",
    "```json\n",
    "传统方法：每个样本独立学习\n",
    "GRPO方法：G个样本组内互相学习，信息利用率更高\n",
    "```\n",
    "\n",
    "**相对比较的优势：**\n",
    "- **判别性更强**：相对比较比绝对评分更准确\n",
    "- **噪声抑制**：组内比较可以抑制奖励模型的噪声\n",
    "- **泛化能力**：相对关系比绝对分数更容易泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a685f7",
   "metadata": {},
   "source": [
    "# 2. 支持GRPO的架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **HuggingFace TRL + DeepSpeed**（主流推荐）\n",
    "\n",
    "**技术栈：**\n",
    "- **TRL (Transformer Reinforcement Learning)** - HuggingFace的官方强化学习库\n",
    "- **DeepSpeed/Accelerate** - 分布式训练支持\n",
    "- **PyTorch** 作为底层框架\n",
    "\n",
    "**详细原因：**\n",
    "- **官方支持**：TRL是HuggingFace官方维护的库，有专门的`GRPOTrainer`\n",
    "- **成熟生态**：完整集成了PEFT、vLLM、Liger等优化技术\n",
    "- **真实实现**：这是DeepSeek团队用于训练DeepSeek-R1的相同算法实现\n",
    "- **分布式能力**：原生支持多机多卡训练，通过Accelerate和DeepSpeed\n",
    "\n",
    "**适用场景：**\n",
    "- 各种规模的模型训练（0.5B到70B+）\n",
    "- 需要PEFT优化的场景\n",
    "- 需要vLLM加速推理的环境\n",
    "- 标准的学术/工业研究环境\n",
    "\n",
    "### 2. **OpenRLHF + Ray** (实验性选择)\n",
    "\n",
    "**技术栈：**\n",
    "- 基于Ray/DeepSpeed的分布式实现\n",
    "- 支持多种RL算法包括GRPO\n",
    "\n",
    "**详细原因：**\n",
    "- **灵活性**：支持多种RL算法对比\n",
    "- **云原生**：基于Ray的分布式架构\n",
    "- **实验性**：相对较新，社区支持有限\n",
    "\n",
    "**适用场景：**\n",
    "- 需要对比多种RL算法\n",
    "- 有Ray使用经验的团队\n",
    "- 大规模云环境部署\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab143eab",
   "metadata": {},
   "source": [
    "\n",
    "## **总结对比**\n",
    "\n",
    "| 框架 | 成熟度 | GRPO实现 | 分布式后端 | 多机多卡支持 | 推荐指数 |\n",
    "|------|--------|----------|------------|-------------|----------|\n",
    "| **HuggingFace TRL** | 最高 | 官方原生 | DeepSpeed/Accelerate | ✅ | ⭐⭐⭐⭐⭐ |\n",
    "| **OpenRLHF** | 高 | 独立实现 | Ray | ✅ | ⭐⭐⭐⭐ |\n",
    "| **Colossal-AI** | 中等 | 自实现 | 自有系统 | ✅ | ⭐⭐ |\n",
    "| **其他框架** | 低 | 无/有限 | 各自不同 | 部分支持 | ⭐ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798aeca",
   "metadata": {},
   "source": [
    "### **1. GRPO实现方式的区别**\n",
    "\n",
    "| 框架 | GRPO实现方式 | 架构基础 |\n",
    "|------|-------------|----------|\n",
    "| **HuggingFace TRL** | 原生GRPOTrainer | 专门的GRPO实现 |\n",
    "| **OpenRLHF** | PPOTrainer + group_norm参数 | PPO框架的变种 |\n",
    "\n",
    "### **2. 分布式策略的区别**\n",
    "\n",
    "#### **HuggingFace TRL + DeepSpeed**\n",
    "- 单一模型的分布式训练\n",
    "- DeepSpeed ZeRO自动处理模型分片\n",
    "- 适合标准的GRPO训练流程\n",
    "\n",
    "#### **OpenRLHF + DeepSpeed（无Ray）**\n",
    "- 也是单一模型分布式训练\n",
    "- 与HuggingFace方案技术上相似\n",
    "- 但GRPO实现不如TRL原生\n",
    "\n",
    "#### **OpenRLHF + Ray + DeepSpeed**\n",
    "- 多模型分离架构（Actor/Critic/Reward分离）\n",
    "- Ray负责调度，DeepSpeed负责每个模型内部分片\n",
    "- 适合复杂的RLHF场景，但对纯GRPO可能过度设计\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb33d0c",
   "metadata": {},
   "source": [
    "其他框架推荐   https://github.com/hiyouga/EasyR1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbac0af",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250708200251616.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64e373",
   "metadata": {},
   "source": [
    "# 3. 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc631b",
   "metadata": {},
   "source": [
    "## 1. 核心理论基础与实践应用框架\n",
    "\n",
    "### 1.1 信息论指导的实践原则\n",
    "\n",
    "**理论基础：**\n",
    "互信息最大化原理 I(P;Q) = H(Q) - H(Q|P)，其中P是prompt，Q是回答质量\n",
    "\n",
    "**为什么要最大化 I(P;Q)？**\n",
    "\n",
    "1. **P（Prompt）的作用：** 提供模型生成高质量回答的信息\n",
    "2. **Q（回答质量）的目标：** 我们希望生成的回答质量高\n",
    "3. **互信息的意义：** I(P;Q)越大，说明prompt对回答质量的影响越大\n",
    "\n",
    "**数学推导的实际意义：**\n",
    "```json\n",
    "I(P;Q) = H(Q) - H(Q|P)\n",
    "```\n",
    "\n",
    "- **H(Q)：** 回答质量的总体不确定性（固定值）\n",
    "- **H(Q|P)：** 给定prompt后，回答质量的剩余不确定性\n",
    "- **目标：** 最小化H(Q|P)，即给定prompt后，回答质量应该尽可能确定\n",
    "\n",
    "**实践转化：**\n",
    "- **低H(Q|P)意味着：** 好的prompt能够较确定地引导出高质量回答\n",
    "- **具体要求：** prompt必须包含足够信息，减少回答的随机性\n",
    "- **质量标准：** 同一个高质量prompt，不同模型应该生成相似质量的回答\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969b7dc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 信息论指导的数据设计\n",
    "\n",
    "**信息完整性量化：**\n",
    "```json\n",
    "完整性得分 = 1 - H(Q|P) / H(Q)\n",
    "```\n",
    "- **得分范围：** 0到1，越接近1表示prompt信息越完整\n",
    "- **实际含义：** 衡量prompt对回答质量的决定程度\n",
    "\n",
    "\n",
    "**实践应用：**\n",
    "\n",
    "**信息完整性原则**\n",
    "- **理论要求：** 降低条件熵H(Q|P)，确保给定prompt后回答质量的确定性\n",
    "- **实践操作：** 每个prompt必须包含以下核心信息元素：\n",
    "  - 明确的任务背景（30-40%信息量）\n",
    "  - 具体的数据或事实（40-50%信息量）\n",
    "  - 清晰的问题导向（10-20%信息量）\n",
    "\n",
    "\n",
    "**实践应用实例：**\n",
    "```json\n",
    "低完整性prompt：\n",
    "\"分析股票走势\" \n",
    "-> H(Q|P)很大，因为缺乏具体信息\n",
    "\n",
    "高完整性prompt：\n",
    "\"苹果公司2024年Q1营收1195亿美元，同比增长2%，iPhone销售额690亿美元，同比下降15%。服务业务收入231亿美元，同比增长11%。基于这些数据，分析苹果公司的业务结构变化和未来增长潜力。\"\n",
    "-> H(Q|P)较小，具体信息限制了回答的随机性\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45649e69",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 认知负荷理论的实践转化\n",
    "\n",
    "**理论基础：**\n",
    "Sweller的认知负荷理论，区分内在、外在和相关认知负荷\n",
    "\n",
    "**核心假设：**\n",
    "1. **工作记忆容量有限：** 同时处理的信息元素数量受限（约7±2个）\n",
    "2. **长期记忆容量无限：** 但检索需要通过工作记忆\n",
    "3. **学习发生在工作记忆中：** 新信息与已有知识整合\n",
    "\n",
    "**三种认知负荷详解：**\n",
    "\n",
    "**内在认知负荷（Intrinsic Cognitive Load）**\n",
    "- **定义：** 任务本身的复杂性产生的认知负荷\n",
    "- **特点：** 由学习材料的元素交互性决定，不可减少\n",
    "- **实际例子：** \n",
    "  - 低内在负荷：记忆单个单词\n",
    "  - 高内在负荷：理解复杂的数学证明\n",
    "\n",
    "**外在认知负荷（Extraneous Cognitive Load）**\n",
    "- **定义：** 由不当的教学设计产生的额外负荷\n",
    "- **特点：** 可以通过优化设计减少或消除\n",
    "- **实际例子：**\n",
    "  - 高外在负荷：信息分散、格式混乱\n",
    "  - 低外在负荷：结构清晰、重点突出\n",
    "\n",
    "**相关认知负荷（Germane Cognitive Load）**\n",
    "- **定义：** 用于处理、构建和自动化图式的认知负荷\n",
    "- **特点：** 有利于学习，应该被最大化\n",
    "- **实际例子：**\n",
    "  - 深度思考和理解\n",
    "  - 知识整合和应用\n",
    "\n",
    "\n",
    "**为什么认知负荷理论支撑GRPO数据设计？**\n",
    "\n",
    "**类比关系：**\n",
    "- **人类学习 → 模型学习**\n",
    "- **工作记忆 → 模型的上下文窗口**\n",
    "- **认知负荷 → 模型的处理复杂度**\n",
    "\n",
    "**应用策略：**\n",
    "\n",
    "**控制内在负荷：**\n",
    "```json\n",
    "任务复杂度 = 概念数量 × 概念间关系复杂度\n",
    "```\n",
    "- **简单任务：** 3-5个概念，线性关系\n",
    "- **中等任务：** 5-8个概念，部分交互\n",
    "- **复杂任务：** 8+个概念，多重交互\n",
    "\n",
    "**最小化外在负荷：**\n",
    "- **统一格式：** 所有prompt使用相同的结构模板\n",
    "- **清晰组织：** 信息按逻辑顺序呈现\n",
    "- **避免冗余：** 去除不必要的修饰词和重复信息\n",
    "\n",
    "**最大化相关负荷：**\n",
    "- **促进思考：** 设计需要推理和整合的任务\n",
    "- **建立联系：** 连接新信息与已有知识\n",
    "- **深度处理：** 要求分析、评估和创造\n",
    "\n",
    "**实践应用：**\n",
    "\n",
    "**认知负荷优化策略**\n",
    "- **内在负荷控制：** 根据模型能力设置适当的任务复杂度\n",
    "- **外在负荷最小化：** 统一prompt结构，减少格式理解负担\n",
    "- **相关负荷最大化：** 设计促进深度学习的认知挑战\n",
    "\n",
    "**实际操作模板：**\n",
    "```json\n",
    "标准prompt结构：\n",
    "[背景信息块] + [关键数据块] + [任务导向块] + [约束条件块]\n",
    "\n",
    "认知复杂度层次：\n",
    "- 初级：事实理解和简单分析（占比30%）\n",
    "- 中级：关系推理和比较评估（占比50%）\n",
    "- 高级：创新思维和复杂决策（占比20%）\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98756f",
   "metadata": {},
   "source": [
    "## 2. 系统化数据准备实施方案\n",
    "\n",
    "### 2.1 三层数据架构设计\n",
    "\n",
    "**理论支撑：**\n",
    "基于统计学习理论的偏差-方差权衡和样本复杂度理论\n",
    "\n",
    "- **含义：** 模型预测值与真实值的系统性差异\n",
    "- **产生原因：** 模型假设过于简单，无法捕捉真实关系\n",
    "- **实际表现：** 欠拟合，在训练集和测试集上都表现不佳\n",
    "\n",
    "**偏差控制策略：**\n",
    "\n",
    "**问题：** 数据集不够代表性导致系统性偏差    \n",
    "**解决方案：**\n",
    "- **全面覆盖：** 确保数据涵盖所有重要的任务类型和难度层级\n",
    "- **领域平衡：** 不同子领域的数据比例要合理\n",
    "- **质量一致：** 统一的质量标准避免系统性质量偏差\n",
    "\n",
    "**方差控制策略：**\n",
    "\n",
    "**问题：** 训练数据的微小变化导致模型性能大幅波动    \n",
    "**解决方案：**\n",
    "- **数据增强：** 通过多样化的表述方式增加数据多样性\n",
    "- **质量标准化：** 统一的评估标准和处理流程\n",
    "- **交叉验证：** 多次验证确保结果的稳定性\n",
    "\n",
    "**噪声控制策略：**\n",
    "\n",
    "**问题：** 数据中的错误和不一致性    \n",
    "**解决方案：**\n",
    "- **多重验证：** 多个评估者独立评估\n",
    "- **一致性检查：** 自动检测和修正不一致的标注\n",
    "- **质量门槛：** 设置最低质量标准\n",
    "\n",
    "**架构：**\n",
    "\n",
    "**第一层：核心种子数据（Core Seed Data）**\n",
    "- **规模：** 总量的10-15%\n",
    "- **质量要求：** 人工精选和验证\n",
    "- **理论依据：** 高质量种子数据降低偏差，提供学习基准\n",
    "- **实际操作：**\n",
    "  - 从现有SFT数据中提取最高质量的prompt\n",
    "  - 领域专家人工评估和优化\n",
    "  - 建立质量标准和评估基准\n",
    "\n",
    "**第二层：扩展生成数据（Expanded Generated Data）**\n",
    "- **规模：** 总量的60-70%\n",
    "- **生成策略：** 基于种子数据的变体生成\n",
    "- **理论依据：** 通过数据增强降低方差，提高泛化能力\n",
    "- **实际操作：**\n",
    "  - 使用高质量LLM（如GPT-4）基于种子数据生成变体\n",
    "  - 保持核心信息结构，变化表述方式和具体细节\n",
    "  - 自动化质量筛选和人工抽样验证\n",
    "\n",
    "**第三层：领域特化数据（Domain-Specific Data）**\n",
    "- **规模：** 总量的15-25%\n",
    "- **特化方向：** 针对特定领域或任务类型\n",
    "- **理论依据：** 领域特化提高模型在特定场景的性能\n",
    "- **实际操作：**\n",
    "  - 识别关键应用场景和高价值任务\n",
    "  - 设计专门的prompt模板和生成策略\n",
    "  - 与领域专家合作确保专业性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af431ddf",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 实施流程与时间粗略规划\n",
    "\n",
    "**阶段1：基础建设期（2-3周）**\n",
    "- **理论准备：** 建立评估标准和质量基准\n",
    "- **实践任务：**\n",
    "  - 建立数据处理和评估pipeline\n",
    "  - 训练评估人员，统一质量标准\n",
    "  - 准备种子数据集（500-1000条）\n",
    "- **质量检查：** 人工全量验证，建立baseline\n",
    "\n",
    "**阶段2：扩展生成期（4-6周）**\n",
    "- **理论指导：** 基于种子数据的系统化扩展\n",
    "- **实践任务：**\n",
    "  - 大规模生成扩展数据（5000-10000条）\n",
    "  - 实施自动化质量筛选\n",
    "  - 人工抽样验证（10-20%抽样率）\n",
    "- **质量检查：** 多维度质量评估，持续优化\n",
    "\n",
    "**阶段3：优化完善期（2-3周）**\n",
    "- **理论验证：** 通过小规模训练验证数据质量\n",
    "- **实践任务：**\n",
    "  - 基于训练反馈优化数据集\n",
    "  - 补充特定类型的数据\n",
    "  - 最终质量验证和文档化\n",
    "- **质量检查：** 全面质量审计，形成最终数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01710c96",
   "metadata": {},
   "source": [
    "## 3. 关键技术实现方案\n",
    "\n",
    "### 3.1 基于LLM的高质量数据生成\n",
    "\n",
    "**理论基础：**\n",
    "上下文学习理论和prompt工程原理\n",
    "\n",
    "**实践方案：**\n",
    "\n",
    "**多层次prompt设计**\n",
    "```json\n",
    "    System Prompt层：\n",
    "    定义生成任务的基本要求和约束条件\n",
    "\n",
    "    Context Prompt层：\n",
    "    提供领域知识和示例模板\n",
    "\n",
    "    Task Prompt层：\n",
    "    具体的生成指令和质量要求\n",
    "\n",
    "    Constraint Prompt层：\n",
    "    格式要求和避免事项\n",
    "```\n",
    "\n",
    "**质量控制策略**\n",
    "- **多模型验证：** 使用2-3个不同的高质量模型生成，对比一致性\n",
    "- **迭代优化：** 基于质量评估结果不断调整prompt\n",
    "- **人工介入：** 在关键节点进行人工审核和调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4bb183",
   "metadata": {},
   "source": [
    "### 3.2 人机协作的质量保证机制\n",
    "\n",
    "**理论基础：**\n",
    "人机协作理论和质量管理原理\n",
    "\n",
    "**实践框架：**\n",
    "\n",
    "**人工智能协作模式**\n",
    "- **AI负责：** 大规模数据生成、初步质量筛选、统计分析\n",
    "- **人工负责：** 质量标准制定、关键样本验证、最终审核\n",
    "- **协作机制：** AI提供候选，人工进行决策和优化\n",
    "\n",
    "**质量保证流程**\n",
    "1. **自动生成**：AI批量生成候选数据\n",
    "2. **智能筛选**：多维度自动评估和排序\n",
    "3. **人工验证**：专家验证高质量候选\n",
    "4. **反馈优化**：基于验证结果调整生成策略\n",
    "5. **最终审核**：全面质量检查和文档化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf3599",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 项目实施的关键成功要素\n",
    "\n",
    "### 4.1 风险管理与应对策略\n",
    "\n",
    "**理论基础：**\n",
    "项目风险管理理论和不确定性处理\n",
    "\n",
    "**主要风险与应对：**\n",
    "\n",
    "**技术风险**\n",
    "- **风险描述：** 生成质量不稳定，自动评估不准确\n",
    "- **应对策略：** 建立多套备选方案，人工介入机制\n",
    "- **监控指标：** 质量得分分布，异常数据比例\n",
    "\n",
    "**资源风险**\n",
    "- **风险描述：** 计算资源不足，时间进度延误\n",
    "- **应对策略：** 资源提前规划，分阶段实施\n",
    "- **监控指标：** 资源使用率，进度完成率\n",
    "\n",
    "**质量风险**\n",
    "- **风险描述：** 数据质量达不到要求，影响训练效果\n",
    "- **应对策略：** 严格质量标准，多重验证机制\n",
    "- **监控指标：** 质量评估分数，训练效果反馈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b74669",
   "metadata": {},
   "source": [
    "### 4.2 持续改进与优化机制\n",
    "\n",
    "**理论基础：**\n",
    "PDCA循环和持续改进理论\n",
    "\n",
    "**四个阶段：**\n",
    "\n",
    "**Plan（计划）：**\n",
    "- **活动：** 识别问题，制定改进计划\n",
    "- **输出：** 具体的行动方案和成功指标\n",
    "- **在GRPO中：** 分析数据质量问题，制定优化策略\n",
    "\n",
    "**Do（执行）：**\n",
    "- **活动：** 实施计划，收集数据\n",
    "- **输出：** 执行结果和过程数据\n",
    "- **在GRPO中：** 实施数据优化方案，生成新数据\n",
    "\n",
    "**Check（检查）：**\n",
    "- **活动：** 评估结果，分析偏差\n",
    "- **输出：** 分析报告和改进建议\n",
    "- **在GRPO中：** 评估数据质量改进效果\n",
    "\n",
    "**Act（行动）：**\n",
    "- **活动：** 标准化成功做法，计划下一轮改进\n",
    "- **输出：** 更新的标准和新的改进目标\n",
    "- **在GRPO中：** 固化有效的数据处理方法\n",
    "\n",
    "**实践机制：**\n",
    "\n",
    "**反馈循环系统**\n",
    "- **数据收集：** 训练过程中的性能指标和问题反馈\n",
    "- **分析诊断：** 识别数据质量问题和改进机会\n",
    "- **优化实施：** 基于分析结果调整数据准备策略\n",
    "- **效果验证：** 通过后续训练验证改进效果\n",
    "\n",
    "**版本管理策略**\n",
    "- **数据版本化：** 每次重要更新都创建新版本\n",
    "- **变更记录：** 详细记录每次变更的原因和影响\n",
    "- **回滚机制：** 出现问题时能够快速回滚到稳定版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324409b6",
   "metadata": {},
   "source": [
    "## 5. 实际应用指导\n",
    "\n",
    "### 5.1 金融领域专项实施方案\n",
    "\n",
    "**基于项目背景的具体应用**\n",
    "\n",
    "**金融prompt设计原则**\n",
    "- **信息要素：** 市场数据、财务指标、时间背景、影响因素\n",
    "- **认知要求：** 数据分析、风险评估、趋势预测、决策建议\n",
    "- **开放性控制：** 允许多种分析角度，但保持专业性约束\n",
    "\n",
    "**质量标准定制**\n",
    "- **专业性要求：** 使用准确的金融术语和概念\n",
    "- **时效性要求：** 反映最新的市场情况和监管要求\n",
    "- **实用性要求：** 生成的分析应该具有实际指导价值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267ce8b",
   "metadata": {},
   "source": [
    "# 4. 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de7f47",
   "metadata": {},
   "source": [
    "## 多机多卡配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a398519",
   "metadata": {},
   "source": [
    "主节点 10.60.68.220     \n",
    "从节点 10.60.98.173\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518406cf",
   "metadata": {},
   "source": [
    "数据挂载(安装NFS)    \n",
    "主节点操作\n",
    "\n",
    "sudo apt install nfs-kernel-server -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75f126",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704181742844.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44af08",
   "metadata": {},
   "source": [
    "```bash\n",
    "#配置NFS导出（使用内网网段）\n",
    "sudo tee /etc/exports << EOF\n",
    "/shared 10.60.0.0/16(rw,sync,no_subtree_check,no_root_squash,insecure)\n",
    "EOF\n",
    "#启动NFS服务\n",
    "sudo systemctl enable nfs-kernel-server\n",
    "sudo systemctl start nfs-kernel-server\n",
    "sudo exportfs -ra\n",
    " #验证NFS配置\n",
    "sudo exportfs -v\n",
    "showmount -e localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf690a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704185826273.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731e6c1",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "# 从节点\n",
    "sudo apt install nfs-common -y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d48e83",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704182037002.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e0c6c",
   "metadata": {},
   "source": [
    "```bash\n",
    "#测试挂载\n",
    "sudo mount -t nfs 10.60.68.220:/shared /shared\n",
    " #验证挂载\n",
    "ls -la /shared/\n",
    "df -h | grep shared \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad53fbd",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704185914785.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721392e",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 设置开机启动挂载(注意开机时候主节点先开机)\n",
    "echo \"10.60.98.173:/shared /shared nfs defaults,_netdev 0 0\" | sudo tee -a /etc/fstab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56376c",
   "metadata": {},
   "source": [
    "### 节点之间免密登录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78277a8",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "# 服务器之间相互免密登录\n",
    "#在主节点上执行 \n",
    "ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N \"\" \n",
    " #查看生成的公钥\n",
    "cat ~/.ssh/id_rsa.pub\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab8fe8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704200042336.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a8053",
   "metadata": {},
   "source": [
    "```bash\n",
    "ssh-copy-id -o StrictHostKeyChecking=no ubuntu@10.60.98.173\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e8491",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704200312123.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cb51",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 测试免密是否成功\n",
    "ssh ubuntu@10.60.68.220 \"hostname && whoami\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77776c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704200403292.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7eb0d",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 从节点也做免密登录\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f01b0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704200623058.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926969",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704213123304.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d904db8",
   "metadata": {},
   "source": [
    "### 安装pdsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527c76e",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 主节点执行\n",
    "sudo apt install pdsh \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ed69c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704203553257.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b14cca",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 设置pdsh执行方式为ssh\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "# 测试\n",
    "pdsh -R ssh -w 10.60.68.220,10.60.98.173 \"hostname\"\n",
    "\n",
    "# 如果针对本机执行需要输出秘钥直接将秘钥追加到本机即可\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54518db",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704213137089.png\" width=100%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3398d",
   "metadata": {},
   "source": [
    "# 5. GRPO微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2042bb",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1d6ac",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda create -n reward3 python=3.11 -y\n",
    "pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1  -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install accelerate==0.34.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install transformers==4.51.3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install deepspeed==0.15.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install datasets==3.3.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install TRL==0.18.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install peft -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install liger-kernel -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip install ninja -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "sudo apt install ninja-build\n",
    "pip install tensorboard -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6e381",
   "metadata": {},
   "source": [
    "## 项目讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac32f06",
   "metadata": {},
   "source": [
    "### 一、项目技术定位与架构概述\n",
    "\n",
    "#### 1.1 技术栈定位\n",
    "本项目是基于DeepSeek-R1-Distill-Qwen-7B的金融领域GRPO强化学习微调系统，采用企业级分布式训练架构。核心技术栈为HuggingFace TRL + DeepSpeed + PyTorch，支持多机多卡生产环境部署。\n",
    "\n",
    "#### 1.2 系统架构层次\n",
    "```json\n",
    "业务层: 金融问答模型微调\n",
    "算法层: GRPO强化学习训练引擎  \n",
    "框架层: TRL训练器 + DeepSpeed分布式\n",
    "基础层: A800集群 + NCCL通信\n",
    "```\n",
    "####  项目目录结构分析\n",
    "\n",
    "```bash\n",
    "grpo_financial_tuning/          #  核心训练系统\n",
    "├── configs/                    #  配置管理\n",
    "│   ├── config.json            # 主配置文件\n",
    "│   └── deepspeed_zero2.json   # DeepSpeed分布式配置\n",
    "├── data/                       #  数据处理模块\n",
    "│   ├── filtered_financial_news_5k.jsonl  # 原始英文金融数据\n",
    "│   └── grpo_prompt.py          # 数据转换脚本\n",
    "├── utils/                      #  工具模块\n",
    "│   ├── config_loader.py        # 配置加载器\n",
    "│   ├── dataset.py              # 数据集处理\n",
    "│   └── logger.py               # 日志管理\n",
    "├── model_eval/                 #  评估模块\n",
    "│   └── multigpu_evaluator.py   # 多GPU评估工具\n",
    "├── train.py                    #  主训练脚本\n",
    "├── test.py                     #  模型测试脚本\n",
    "├── run_training.sh             #  训练启动脚本\n",
    "├── run_evaluation.sh           #  评估启动脚本\n",
    "└── deploy_A800.sh              #  A800部署脚本\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73642869",
   "metadata": {},
   "source": [
    "### 二、核心模块架构设计\n",
    "\n",
    "#### 2.1 训练引擎架构 (train.py)\n",
    "\n",
    "**设计模式**: 组件化初始化 + 回调机制\n",
    "- **GRPOTraining主类**: 统一管理训练生命周期，采用依赖注入模式\n",
    "- **组件初始化顺序**: tokenizer → model → dataset → reward_function → trainer\n",
    "- **早停回调机制**: GRPOBestModelEarlyStoppingCallback实现智能模型保存\n",
    "\n",
    "**关键技术决策**:\n",
    "- 内存优化策略: device_map=None让DeepSpeed完全控制设备分配\n",
    "- 梯度管理: 强制设置requires_grad=True确保参数可训练\n",
    "\n",
    "**扩展性设计**:\n",
    "- 配置驱动: 所有超参数通过ProjectConfig统一管理\n",
    "- 模型适配: 支持LoRA和全参数微调切换\n",
    "- 检查点恢复: 原生支持训练中断恢复\n",
    "\n",
    "#### 2.2 数据处理架构 (data/grpo_prompt.py)\n",
    "\n",
    "**并发处理设计**:\n",
    "- **线程池架构**: ThreadPoolExecutor管理工作线程\n",
    "- **流控策略**: 随机延迟分散API请求时间\n",
    "\n",
    "**数据转换流水线**:\n",
    "```json\n",
    "英文金融新闻 → 文本截断 → DeepSeek API → JSON解析 → 中文问答格式\n",
    "```\n",
    "\n",
    "**质量保证机制**:\n",
    "- 分层随机采样确保数据代表性\n",
    "- JSON格式验证和错误处理\n",
    "- 生成结果示例展示和统计报告\n",
    "\n",
    "#### 2.3 分布式配置架构 (configs/)\n",
    "\n",
    "**配置分层设计**:\n",
    "- **业务配置层**: config.json管理模型路径、训练参数、GRPO参数\n",
    "- **框架配置层**: deepspeed_zero2.json管理分布式优化参数\n",
    "- **运行时配置**: 命令行参数支持动态覆盖\n",
    "\n",
    "**DeepSpeed优化策略**:\n",
    "- ZeRO-2阶段: 优化器状态CPU卸载 + 梯度分片\n",
    "- 混合精度: bf16降低内存占用同时保持数值稳定性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2beef",
   "metadata": {},
   "source": [
    "### 三、分布式训练架构\n",
    "\n",
    "#### 3.1 多机多卡部署架构\n",
    "\n",
    "**集群拓扑设计**:\n",
    "- 主节点(10.60.68.220): 2×A800-80GB + 训练协调\n",
    "- 从节点(10.60.98.173): 1×A800-80GB + 计算节点\n",
    "\n",
    "**通信架构优化**:\n",
    "- NCCL通信库: 专门优化GPU间高带宽通信\n",
    "- 网络配置: 强制使用eth0网卡，禁用InfiniBand避免兼容性问题\n",
    "- 缓冲区调优: NCCL_BUFFSIZE=524288统一缓冲区大小\n",
    "\n",
    "**进程管理策略**:\n",
    "- DeepSpeed launcher: 自动处理多机进程启动和同步\n",
    "- hostfile配置: 声明式定义集群拓扑和GPU分配\n",
    "- 故障恢复: 支持节点故障检测和训练恢复\n",
    "\n",
    "#### 3.2 内存管理架构\n",
    "\n",
    "**ZeRO-2内存优化**:\n",
    "- 优化器状态分片: 每个GPU只保存部分优化器状态\n",
    "- CPU卸载策略: 优化器状态存储在CPU内存，训练时动态加载\n",
    "- 梯度分片: 反向传播时梯度按需聚合，降低峰值内存\n",
    "\n",
    "**显存分配策略**:\n",
    "- 模型权重: 每个GPU保存完整副本用于前向传播\n",
    "- 激活值管理: 梯度检查点技术平衡计算和内存\n",
    "- 动态内存池: PyTorch CUDA内存池优化小块内存分配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a90872",
   "metadata": {},
   "source": [
    "### 四、GRPO算法集成架构\n",
    "\n",
    "#### 4.1 奖励模型集成\n",
    "\n",
    "**模型加载架构**:\n",
    "- 独立奖励模型: Skywork-Reward-checkpoint-1000\n",
    "- 格式对齐: LLaMA-3特殊token格式确保与训练时一致\n",
    "- 设备管理: 奖励模型独立设备分配避免显存冲突\n",
    "\n",
    "**奖励计算流水线**:\n",
    "```\n",
    "对话格式化 → 文本编码 → 奖励模型推理 → 分数提取 → 异常处理\n",
    "```\n",
    "\n",
    "#### 4.2 GRPO训练流程\n",
    "\n",
    "**生成阶段**:\n",
    "- 多样本生成: num_generations=4为每个prompt生成多个回复\n",
    "- 采样策略: temperature=0.7 + top_p=0.9平衡质量和多样性\n",
    "- 长度控制: max_completion_length=1024限制生成长度\n",
    "\n",
    "**优化阶段**:\n",
    "- 组内比较: 4个生成样本组内相对评分\n",
    "- 策略更新: epsilon=0.2控制策略更新幅度\n",
    "- 梯度裁剪: max_grad_norm=1.0防止梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f794c2",
   "metadata": {},
   "source": [
    "### 五、系统可扩展性架构\n",
    "\n",
    "#### 5.1 水平扩展能力\n",
    "\n",
    "**节点扩展**:\n",
    "- 动态hostfile: 支持任意数量节点加入集群\n",
    "- 自动发现: DeepSpeed自动检测可用GPU资源\n",
    "- 配置适配: 批次大小和学习率自动调整\n",
    "\n",
    "**算力扩展**:\n",
    "- GPU异构支持: 不同型号GPU混合部署\n",
    "- 内存自适应: 根据GPU内存自动调整batch size\n",
    "- 网络带宽适配: 自动检测和优化通信模式\n",
    "\n",
    "#### 5.2 垂直扩展能力\n",
    "\n",
    "**模型规模扩展**:\n",
    "- ZeRO-3支持: 可切换到ZeRO-3支持更大模型\n",
    "- 模型并行: 支持张量并行处理超大模型\n",
    "- 流水线并行: 可扩展支持流水线并行训练\n",
    "\n",
    "**数据规模扩展**:\n",
    "- 流式数据加载: 支持TB级数据集训练\n",
    "- 动态采样: 训练过程中动态调整数据采样策略\n",
    "- 增量训练: 支持新数据增量微调\n",
    "\n",
    "#### 5.3 功能扩展架构\n",
    "\n",
    "**算法扩展**:\n",
    "- TRL框架兼容: 支持PPO、DPO等其他RL算法\n",
    "- 自定义奖励: 可替换不同的奖励模型\n",
    "- 多目标优化: 支持多个奖励函数组合\n",
    "\n",
    "**业务扩展**:\n",
    "- 领域适配: 配置驱动的领域切换\n",
    "- 多语言支持: 数据处理流水线支持多语言\n",
    "- 评估扩展: 可插拔的评估指标和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76173ce0",
   "metadata": {},
   "source": [
    "### 六、生产环境部署架构\n",
    "\n",
    "#### 6.1 部署自动化\n",
    "\n",
    "**部署流水线**:\n",
    "- rsync同步: 增量文件同步到目标服务器\n",
    "- 权限管理: 自动设置脚本执行权限\n",
    "- 环境检查: 自动验证依赖和配置\n",
    "\n",
    "#### 6.2 容错与恢复\n",
    "\n",
    "**故障检测**:\n",
    "- 节点健康检查: 定期检测节点可用性\n",
    "- 训练状态监控: 实时监控训练进度和指标\n",
    "- 异常处理: 自动处理常见训练异常\n",
    "\n",
    "**恢复机制**:\n",
    "- 检查点恢复: 自动从最近检查点恢复训练\n",
    "- 节点替换: 支持故障节点的热替换\n",
    "- 数据完整性: 确保训练数据和模型状态一致性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5360d0",
   "metadata": {},
   "source": [
    "## 代码讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e520e",
   "metadata": {},
   "source": [
    "### grpo_prompt.py  数据转换脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76484f62",
   "metadata": {},
   "source": [
    "```python\n",
    "# ==================== GRPO金融数据生成工具 ====================\n",
    "# 作用：将英文金融新闻转换为中文GRPO训练数据\n",
    "# 功能：批量处理、并发生成、格式验证、分层采样\n",
    "\n",
    "# ==================== 库导入说明 ====================\n",
    "import os                    # 操作系统相关功能，如文件路径操作、环境变量等\n",
    "import json                  # 处理JSON格式数据的标准库\n",
    "import time                  # 时间相关功能，如延迟等待、时间戳等\n",
    "import random                # 随机数生成和随机采样功能\n",
    "import openai                # OpenAI API客户端，这里用于调用DeepSeek API\n",
    "from tqdm import tqdm        # 进度条显示库，用于显示处理进度\n",
    "import concurrent.futures    # 并发处理库，用于多线程并行处理\n",
    "import threading             # 线程相关功能，用于线程锁和同步\n",
    "import argparse              # 用于解析命令行参数的标准库\n",
    "\n",
    "# ==================== 全局配置参数 ====================\n",
    "# 这些参数控制整个数据生成过程的行为\n",
    "API_KEY = \"sk-311f5262868yu79hnpej2a0986c16\"  # DeepSeek API的密钥，用于身份验证\n",
    "INPUT_FILE = \"filtered_financial_news_5k.jsonl\"    # 输入文件路径：包含英文金融新闻的数据文件\n",
    "OUTPUT_FILE = \"grpo_prompts_dataset_5k.jsonl\"      # 输出文件路径：生成的GRPO训练数据\n",
    "SAMPLE_COUNT = 50          # 需要采样的记录数量：从输入数据中选择50条进行处理\n",
    "MAX_WORKERS = 80           # 并行处理的最大线程数：同时运行80个线程来加速处理\n",
    "REQUEST_INTERVAL = 1       # 请求间隔（秒）：避免API调用过于频繁触发限制\n",
    "RANDOM_SEED = 57           # 随机种子：确保每次运行程序时随机采样的结果都相同\n",
    "\n",
    "# ==================== 随机种子设置 ====================\n",
    "# 设置随机种子确保实验的可重现性\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==================== 线程安全机制 ====================\n",
    "# 多线程环境下需要使用锁来保护共享资源\n",
    "print_lock = threading.Lock()   # 用于保护打印输出，避免多个线程同时打印导致输出混乱\n",
    "output_lock = threading.Lock()  # 用于保护文件写入，避免多个线程同时写入同一文件导致数据损坏\n",
    "\n",
    "# ==================== API客户端初始化 ====================\n",
    "# 初始化DeepSeek API客户端\n",
    "client = openai.OpenAI(\n",
    "    api_key=API_KEY,                        # 使用上面定义的API密钥\n",
    "    base_url=\"https://api.deepseek.com\"     # DeepSeek的API服务地址\n",
    ")\n",
    "\n",
    "def truncate_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    截断文本以满足API长度限制\n",
    "    \n",
    "    参数：\n",
    "        text: 需要截断的文本字符串\n",
    "        max_length: 最大允许长度，默认5000字符\n",
    "    \n",
    "    返回：\n",
    "        截断后的文本字符串\n",
    "    \n",
    "    作用：\n",
    "    1. 检查文本长度是否超过限制\n",
    "    2. 如果超过则从开头截取指定长度\n",
    "    3. 避免API调用因文本过长而失败\n",
    "    \n",
    "    为什么需要截断：\n",
    "    - LLM API通常对输入长度有限制\n",
    "    - 过长的文本会导致API调用失败\n",
    "    - 截断可以确保API调用的稳定性\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text  # 文本长度合适，直接返回\n",
    "    return text[:max_length]  # 截取前max_length个字符\n",
    "\n",
    "def generate_grpo_prompt(item):\n",
    "    \"\"\"\n",
    "    基于英文金融文章生成适用于GRPO训练的中文金融问题\n",
    "    \n",
    "    参数：\n",
    "        item: 包含文章内容的字典，应该有'Article'字段\n",
    "    \n",
    "    返回：\n",
    "        包含GRPO prompt的字典，格式：{\"prompt\": \"问题内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    GRPO特点：\n",
    "    1. 只需要生成问题（prompt），不需要答案\n",
    "    2. 问题要足够详细，包含背景信息\n",
    "    3. 支持模型生成多样化的回复用于强化学习\n",
    "    \n",
    "    生成策略：\n",
    "    - 从英文文章中提取关键信息\n",
    "    - 转换为详细的中文问题\n",
    "    - 包含足够的背景信息支持回答\n",
    "    - 设计开放性问题支持多样化回复\n",
    "    \"\"\"\n",
    "    # 提取文章内容并截断\n",
    "    article = item.get('Article', '')\n",
    "    article_truncated = truncate_text(article)\n",
    "    \n",
    "    # 构建专门为GRPO训练优化的提示词\n",
    "    # 这个提示词指导AI生成高质量的训练问题\n",
    "    prompt = f\"\"\"\n",
    "请基于以下英文金融文章，创建一个详细的中文金融问题，专门用于强化学习训练。\n",
    "\n",
    "英文文章:\n",
    "{article_truncated}\n",
    "\n",
    "请完成以下任务:\n",
    "1. 从文章中提取关键信息、数据、事实和核心观点\n",
    "2. 创建一个专业的金融领域问题，要求该问题能够引导出多种不同的分析角度和回答方式\n",
    "3. 问题必须包含足够详细的背景信息和事实，确保仅凭问题本身就能够推导出合理的回答\n",
    "4. 问题应该具有一定的开放性，支持多样化的回答思路\n",
    "\n",
    "输出格式必须是有效的JSON，结构如下:\n",
    "{{\n",
    "  \"prompt\": \"这里是包含详细背景信息的专业金融问题\"\n",
    "}}\n",
    "\n",
    "要求:\n",
    "- 问题必须具体且深入，能够引导出金融专业领域的分析\n",
    "- 必须包含足够丰富的事实信息，使回答者仅通过阅读问题就能回答\n",
    "- 禁止出现\"本文\"、\"文章\"、\"整体基调\"、\"情绪\"等字样\n",
    "- 禁止对文章本身进行评价或总结\n",
    "- 直接以陈述事实的方式提供背景信息\n",
    "- 问题应以客观的方式呈现数据和事实，避免主观评价\n",
    "- 问题要以自然、符合实际提问习惯的方式表达\n",
    "- 问题内容要特别详细，包含文章中所有能够支持回答问题的关键信息\n",
    "- 问题应该具有一定的复杂性，支持多角度分析和思考\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用DeepSeek API生成GRPO问题\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",           # 使用DeepSeek聊天模型\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"你是一个专业的金融数据分析助手，精通英文金融文章翻译和问题构建。你的任务是创建适用于强化学习训练的专业金融问题，这些问题需要具有一定的开放性和复杂性。\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            stream=False,                    # 不使用流式响应\n",
    "            temperature=0.8                  # 提高创造性，生成更多样化的问题\n",
    "        )\n",
    "        \n",
    "        # 提取生成的结果\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # 解析JSON响应\n",
    "        # 寻找JSON开始和结束位置\n",
    "        json_start = result.find('{')\n",
    "        json_end = result.rfind('}') + 1\n",
    "        \n",
    "        if json_start != -1 and json_end != -1:\n",
    "            # 提取JSON字符串并解析\n",
    "            json_str = result[json_start:json_end]\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # 验证JSON结构\n",
    "            if \"prompt\" in data:\n",
    "                return {\"prompt\": data[\"prompt\"]}\n",
    "            else:\n",
    "                print(\"警告: 返回的JSON缺少'prompt'字段\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"无法在响应中找到有效的JSON\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"生成GRPO prompt时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_grpo_data(item, index, total):\n",
    "    \"\"\"\n",
    "    生成GRPO训练数据：只生成prompt，不生成答案\n",
    "    \n",
    "    参数：\n",
    "        item: 原始文章数据字典\n",
    "        index: 当前处理的文章索引（从0开始）\n",
    "        total: 总文章数量\n",
    "    \n",
    "    返回：\n",
    "        GRPO训练数据字典，格式：{\"prompt\": \"问题内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    GRPO数据特点：\n",
    "    - 只包含prompt字段，不需要预定义的答案\n",
    "    - 训练时模型会为每个prompt生成多个回复\n",
    "    - 使用奖励模型对生成的回复进行评分\n",
    "    - 通过强化学习优化模型参数\n",
    "    \n",
    "    处理流程：\n",
    "    1. 记录处理进度\n",
    "    2. 生成GRPO格式的问题\n",
    "    3. 格式化为标准数据结构\n",
    "    4. 处理生成失败的情况\n",
    "    \"\"\"\n",
    "    # 使用线程锁确保打印输出不混乱\n",
    "    with print_lock:\n",
    "        print(f\"处理文章 {index+1}/{total}\")\n",
    "    \n",
    "    # 生成GRPO prompt\n",
    "    prompt_data = generate_grpo_prompt(item)\n",
    "    if not prompt_data:\n",
    "        # 生成失败的处理\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: prompt生成失败\")\n",
    "        return None\n",
    "    \n",
    "    # 直接返回prompt数据，不需要其他字段\n",
    "    # GRPO训练只需要prompt，不需要预定义答案\n",
    "    grpo_data = {\n",
    "        \"prompt\": prompt_data[\"prompt\"]\n",
    "    }\n",
    "    \n",
    "    # 记录成功信息\n",
    "    with print_lock:\n",
    "        print(f\"文章 {index+1}/{total}: 成功生成GRPO prompt\")\n",
    "    \n",
    "    return grpo_data\n",
    "\n",
    "def process_article(args):\n",
    "    \"\"\"\n",
    "    处理单篇文章的包装函数，专门用于并行处理\n",
    "    \n",
    "    参数：\n",
    "        args: 包含处理参数的元组 (item, index, total, output_file)\n",
    "            item: 文章数据\n",
    "            index: 文章索引\n",
    "            total: 总数量\n",
    "            output_file: 输出文件路径\n",
    "    \n",
    "    返回：\n",
    "        bool: 处理是否成功\n",
    "    \n",
    "    作用：\n",
    "    1. 解包参数\n",
    "    2. 添加随机延迟避免API限流\n",
    "    3. 生成GRPO数据\n",
    "    4. 安全地写入文件\n",
    "    5. 返回处理结果\n",
    "    \n",
    "    为什么需要随机延迟：\n",
    "    - API服务通常有频率限制\n",
    "    - 同时大量请求可能被限流\n",
    "    - 随机延迟可以分散请求时间\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    item, index, total, output_file = args\n",
    "    \n",
    "    # 添加随机延迟，避免同时大量请求API\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 生成GRPO数据\n",
    "    result = create_grpo_data(item, index, total)\n",
    "    if result:\n",
    "        # 使用文件锁安全地写入结果\n",
    "        with output_lock:\n",
    "            with open(output_file, 'a', encoding='utf-8') as out_f:\n",
    "                # 以JSONL格式写入（每行一个JSON对象）\n",
    "                out_f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def stratified_random_sample(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    分层随机采样函数\n",
    "    \n",
    "    参数：\n",
    "        data_list: 原始数据列表\n",
    "        sample_count: 需要采样的数量\n",
    "    \n",
    "    返回：\n",
    "        采样后的数据列表\n",
    "    \n",
    "    作用：\n",
    "    1. 将数据分成多个层级\n",
    "    2. 从每个层级中随机采样\n",
    "    3. 确保采样的代表性\n",
    "    4. 避免数据偏差\n",
    "    \n",
    "    分层采样的优势：\n",
    "    - 比纯随机采样更有代表性\n",
    "    - 确保样本覆盖整个数据集\n",
    "    - 减少采样偏差\n",
    "    - 提高样本质量\n",
    "    \"\"\"\n",
    "    total_count = len(data_list)\n",
    "    \n",
    "    # 如果数据总量小于等于需要采样的数量，返回全部数据\n",
    "    if total_count <= sample_count:\n",
    "        print(f\"数据总量({total_count})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 确定分层数量\n",
    "    # 至少10个数据为一层，但不超过采样数量\n",
    "    num_strata = min(sample_count, total_count // 10 + 1)\n",
    "    stratum_size = total_count // num_strata  # 每层的大小\n",
    "    \n",
    "    sampled_items = []\n",
    "    \n",
    "    # 从每个层级中采样\n",
    "    for i in range(num_strata):\n",
    "        # 计算当前层级的数据范围\n",
    "        start_idx = i * stratum_size\n",
    "        end_idx = start_idx + stratum_size if i < num_strata - 1 else total_count\n",
    "        \n",
    "        # 提取当前层级的数据\n",
    "        stratum_data = data_list[start_idx:end_idx]\n",
    "        \n",
    "        # 计算当前层级应该采样的数量\n",
    "        stratum_sample_count = max(1, int((end_idx - start_idx) / total_count * sample_count))\n",
    "        \n",
    "        # 确保不超过总采样数量\n",
    "        if len(sampled_items) + stratum_sample_count > sample_count:\n",
    "            stratum_sample_count = sample_count - len(sampled_items)\n",
    "        \n",
    "        # 从当前层级中采样\n",
    "        if len(stratum_data) <= stratum_sample_count:\n",
    "            # 如果层级数据不足，全部采用\n",
    "            sampled_items.extend(stratum_data)\n",
    "        else:\n",
    "            # 随机采样指定数量\n",
    "            stratum_samples = random.sample(stratum_data, stratum_sample_count)\n",
    "            sampled_items.extend(stratum_samples)\n",
    "        \n",
    "        # 如果已达到采样数量，停止采样\n",
    "        if len(sampled_items) >= sample_count:\n",
    "            break\n",
    "\n",
    "    # 如果采样数量仍不足，从剩余数据中补充\n",
    "    if len(sampled_items) < sample_count:\n",
    "        # 找出未被采样的数据\n",
    "        remaining = [item for item in data_list if item not in sampled_items]\n",
    "        # 从剩余数据中随机采样补足\n",
    "        additional = random.sample(remaining, sample_count - len(sampled_items))\n",
    "        sampled_items.extend(additional)\n",
    "    \n",
    "    return sampled_items\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"\n",
    "    解析命令行参数\n",
    "    \n",
    "    返回：\n",
    "        argparse.Namespace: 解析后的参数对象\n",
    "    \n",
    "    作用：\n",
    "    1. 定义所有可配置的参数\n",
    "    2. 设置默认值\n",
    "    3. 提供参数说明\n",
    "    4. 支持命令行自定义配置\n",
    "    \n",
    "    支持的参数类别：\n",
    "    - 数据相关：输入文件、输出文件、样本数量\n",
    "    - API相关：API密钥\n",
    "    - 并发相关：线程数、请求间隔\n",
    "    - 其他：随机种子\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"GRPO金融数据生成工具\")\n",
    "    \n",
    "    # 数据相关参数\n",
    "    parser.add_argument('--input_file', type=str, \n",
    "                       default='filtered_financial_news_5k.jsonl',\n",
    "                       help='输入文件路径 (默认: filtered_financial_news_5k.jsonl)')\n",
    "    \n",
    "    parser.add_argument('--output_file', type=str,\n",
    "                       default='grpo_prompts_dataset_5k.jsonl', \n",
    "                       help='输出文件路径 (默认: grpo_prompts_dataset_5k.jsonl)')\n",
    "    \n",
    "    parser.add_argument('--sample_count', type=int,\n",
    "                       default=50,\n",
    "                       help='需要处理的样本数量 (默认: 50)')\n",
    "    \n",
    "    # API相关参数\n",
    "    parser.add_argument('--api_key', type=str,\n",
    "                       default=\"sk-311f5262868e4c02b9a98cccb5986c16\",\n",
    "                       help='DeepSeek API密钥')\n",
    "    \n",
    "    # 并发相关参数\n",
    "    parser.add_argument('--max_workers', type=int,\n",
    "                       default=80,\n",
    "                       help='最大并发线程数 (默认: 80)')\n",
    "    \n",
    "    parser.add_argument('--request_interval', type=float,\n",
    "                       default=1.0,\n",
    "                       help='请求间隔秒数 (默认: 1.0)')\n",
    "    \n",
    "    # 其他参数\n",
    "    parser.add_argument('--random_seed', type=int,\n",
    "                       default=57,\n",
    "                       help='随机种子 (默认: 57)')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：协调整个GRPO数据处理流程\n",
    "    \n",
    "    作用：\n",
    "    1. 解析命令行参数\n",
    "    2. 更新全局配置\n",
    "    3. 验证输入文件\n",
    "    4. 读取和处理数据\n",
    "    5. 执行并行处理\n",
    "    6. 验证输出结果\n",
    "    \n",
    "    处理流程：\n",
    "    1. 参数解析和配置更新\n",
    "    2. 文件验证和数据加载\n",
    "    3. 数据采样和格式化\n",
    "    4. 并行生成GRPO数据\n",
    "    5. 结果验证和示例展示\n",
    "    \"\"\"\n",
    "    # 解析命令行参数\n",
    "    args = parse_arguments()\n",
    "    \n",
    "    # 使用参数更新全局配置\n",
    "    global INPUT_FILE, OUTPUT_FILE, SAMPLE_COUNT, API_KEY, MAX_WORKERS, REQUEST_INTERVAL, RANDOM_SEED\n",
    "    INPUT_FILE = args.input_file\n",
    "    OUTPUT_FILE = args.output_file\n",
    "    SAMPLE_COUNT = args.sample_count\n",
    "    API_KEY = args.api_key\n",
    "    MAX_WORKERS = args.max_workers\n",
    "    REQUEST_INTERVAL = args.request_interval\n",
    "    RANDOM_SEED = args.random_seed\n",
    "    \n",
    "    # 重新设置随机种子确保一致性\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # 重新初始化API客户端使用新的API密钥\n",
    "    global client\n",
    "    client = openai.OpenAI(\n",
    "        api_key=API_KEY,\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    \n",
    "    # 打印配置信息\n",
    "    print(f\"=== GRPO数据生成配置 ===\")\n",
    "    print(f\"输入文件: {INPUT_FILE}\")\n",
    "    print(f\"输出文件: {OUTPUT_FILE}\")\n",
    "    print(f\"样本数量: {SAMPLE_COUNT}\")\n",
    "    print(f\"最大线程数: {MAX_WORKERS}\")\n",
    "    print(f\"请求间隔: {REQUEST_INTERVAL}秒\")\n",
    "    print(f\"随机种子: {RANDOM_SEED}\")\n",
    "    print(f\"========================\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"错误: 输入文件 '{INPUT_FILE}' 不存在!\")\n",
    "        return\n",
    "    \n",
    "    # 验证输入文件大小\n",
    "    file_size = os.path.getsize(INPUT_FILE)\n",
    "    print(f\"输入文件大小: {file_size} 字节\")\n",
    "    if file_size == 0:\n",
    "        print(\"错误: 输入文件为空!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"使用随机种子: {RANDOM_SEED} 确保可复现性\")\n",
    "    print(f\"正在为GRPO训练生成prompt数据...\")\n",
    "    \n",
    "    # ==================== 读取和解析输入文件 ====================\n",
    "    items = []          # 存储有效数据\n",
    "    line_count = 0      # 总行数\n",
    "    error_count = 0     # 错误行数\n",
    "    valid_count = 0     # 有效行数\n",
    "    \n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line_count += 1\n",
    "                line = line.strip()  # 去除首尾空白\n",
    "                if not line:         # 跳过空行\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # 解析JSON数据\n",
    "                    item = json.loads(line)\n",
    "                    \n",
    "                    # 检查必需字段\n",
    "                    if 'Article' in item:\n",
    "                        items.append(item)\n",
    "                        valid_count += 1\n",
    "                    elif 'article' in item:\n",
    "                        # 兼容小写字段名\n",
    "                        item['Article'] = item['article']\n",
    "                        items.append(item)\n",
    "                        valid_count += 1\n",
    "                    else:\n",
    "                        print(f\"警告: 第 {line_num} 行没有'Article'或'article'字段\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    # JSON解析错误\n",
    "                    error_count += 1\n",
    "                    print(f\"错误: 第 {line_num} 行JSON解析失败: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "    \n",
    "    # 打印数据加载统计\n",
    "    print(f\"文件共有 {line_count} 行\")\n",
    "    print(f\"解析错误: {error_count} 行\")\n",
    "    print(f\"成功读取: {valid_count} 条有效记录\")\n",
    "    print(f\"最终收集: {len(items)} 条记录\")\n",
    "    \n",
    "    # 检查是否有有效数据\n",
    "    if len(items) == 0:\n",
    "        print(\"没有读取到有效数据!\")\n",
    "        return\n",
    "    \n",
    "    # ==================== 数据采样 ====================\n",
    "    sampled_items = stratified_random_sample(items, SAMPLE_COUNT)\n",
    "    print(f\"分层随机采样了 {len(sampled_items)}/{len(items)} 条记录\")\n",
    "    \n",
    "    # ==================== 创建输出文件 ====================\n",
    "    # 清空输出文件（如果存在）\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        pass\n",
    "    \n",
    "    # ==================== 并行处理 ====================\n",
    "    print(f\"开始并行处理，最大线程数: {MAX_WORKERS}\")\n",
    "    print(f\"生成GRPO prompt数据...\")\n",
    "    \n",
    "    # 准备并行处理的参数列表\n",
    "    args_list = [(item, i, len(sampled_items), OUTPUT_FILE) for i, item in enumerate(sampled_items)]\n",
    "    \n",
    "    success_count = 0\n",
    "    # 使用线程池执行并行处理\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 执行所有任务并收集结果\n",
    "        results = list(executor.map(process_article, args_list))\n",
    "        # 统计成功数量\n",
    "        success_count = sum(1 for r in results if r)\n",
    "    \n",
    "    # ==================== 处理结果统计 ====================\n",
    "    print(f\"GRPO数据生成完成，成功处理 {success_count}/{len(sampled_items)} 条记录\")\n",
    "    print(f\"GRPO prompt数据已保存至 {OUTPUT_FILE}\")\n",
    "    \n",
    "    # ==================== 验证输出文件 ====================\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            # 读取前3行作为示例\n",
    "            sample_lines = [f.readline().strip() for _ in range(3)]\n",
    "        \n",
    "        print(\"\\n生成的GRPO数据示例:\")\n",
    "        for i, line in enumerate(sample_lines, 1):\n",
    "            if line:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    # 只显示前100个字符避免输出过长\n",
    "                    print(f\"示例 {i}: {data['prompt'][:100]}...\")\n",
    "                except:\n",
    "                    print(f\"示例 {i}: 格式错误\")\n",
    "\n",
    "# ==================== 程序入口点 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # 执行主函数\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe267614",
   "metadata": {},
   "source": [
    "执行命令 \n",
    "```python\n",
    "python grpo_prompt.py \\\n",
    "    --input_file \"filtered_financial_news_5k.jsonl\" \\\n",
    "    --output_file \"./test_prompts_dataset.jsonl\" \\\n",
    "    --sample_count 10 \\\n",
    "    --max_workers 10 \\\n",
    "    --request_interval 2 \\\n",
    "    --random_seed 32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1abb7e",
   "metadata": {},
   "source": [
    "config_loader.py   配置加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55797d",
   "metadata": {},
   "source": [
    "```python \n",
    "\"\"\"\n",
    "配置文件加载工具类\n",
    "作用：为GRPO训练提供统一的配置管理功能\n",
    "功能：支持JSON配置文件加载、嵌套配置访问、DeepSpeed配置管理\n",
    "\"\"\"\n",
    "import json                              # JSON数据处理\n",
    "from pathlib import Path                 # 路径操作\n",
    "from typing import Dict, Any, Optional   # 类型提示\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"\n",
    "    配置文件加载器\n",
    "    \n",
    "    作用：负责加载和缓存JSON配置文件\n",
    "    功能：\n",
    "    1. 加载JSON配置文件\n",
    "    2. 缓存配置以提高性能\n",
    "    3. 保存配置到文件\n",
    "    4. 处理文件读写错误\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \"configs\"):\n",
    "        \"\"\"\n",
    "        初始化配置加载器\n",
    "        \n",
    "        参数：\n",
    "            config_dir: 配置文件目录路径\n",
    "        \"\"\"\n",
    "        self.config_dir = Path(config_dir)  # 配置文件目录\n",
    "        self._config_cache = {}             # 配置缓存字典\n",
    "    \n",
    "    def load_json(self, file_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        加载JSON配置文件\n",
    "        \n",
    "        参数：\n",
    "            file_name: 配置文件名\n",
    "            \n",
    "        返回：\n",
    "            Dict: 配置字典\n",
    "        \n",
    "        作用：\n",
    "        1. 检查缓存中是否已有配置\n",
    "        2. 读取并解析JSON文件\n",
    "        3. 缓存配置以提高后续访问速度\n",
    "        4. 处理文件不存在和JSON格式错误\n",
    "        \"\"\"\n",
    "        # 检查缓存\n",
    "        if file_name in self._config_cache:\n",
    "            return self._config_cache[file_name]\n",
    "        \n",
    "        # 构建文件路径\n",
    "        file_path = self.config_dir / file_name\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"配置文件不存在: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取并解析JSON文件\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            # 缓存配置\n",
    "            self._config_cache[file_name] = config\n",
    "            return config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            # JSON格式错误\n",
    "            raise ValueError(f\"配置文件JSON格式错误 {file_path}: {e}\")\n",
    "    \n",
    "    def save_json(self, file_name: str, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        保存配置到JSON文件\n",
    "        \n",
    "        参数：\n",
    "            file_name: 配置文件名\n",
    "            config: 配置字典\n",
    "        \n",
    "        作用：\n",
    "        1. 确保目录存在\n",
    "        2. 保存配置到JSON文件\n",
    "        3. 更新缓存\n",
    "        \"\"\"\n",
    "        file_path = self.config_dir / file_name\n",
    "        \n",
    "        # 确保目录存在\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 保存到文件\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 更新缓存\n",
    "        self._config_cache[file_name] = config\n",
    "\n",
    "\n",
    "class ProjectConfig:\n",
    "    \"\"\"\n",
    "    项目配置管理器\n",
    "    \n",
    "    作用：管理GRPO训练项目的所有配置\n",
    "    功能：\n",
    "    1. 支持嵌套配置访问（如 \"model.model_name\"）\n",
    "    2. 提供配置的获取、设置、批量更新\n",
    "    3. 配置文件保存和加载\n",
    "    4. 配置信息打印\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_file: str = \"config.json\"):\n",
    "        \"\"\"\n",
    "        初始化项目配置\n",
    "        \n",
    "        参数：\n",
    "            config_file: 配置文件名\n",
    "        \"\"\"\n",
    "        self.config_loader = ConfigLoader()                    # 配置加载器\n",
    "        self.config_file = config_file                         # 配置文件名\n",
    "        self._config = self.config_loader.load_json(config_file)  # 加载配置\n",
    "    \n",
    "    def get(self, key_path: str, default: Any = None) -> Any:\n",
    "        \"\"\"\n",
    "        获取配置值（支持嵌套键路径）\n",
    "        \n",
    "        参数：\n",
    "            key_path: 键路径，如 \"model.model_name\"\n",
    "            default: 默认值，当键不存在时返回\n",
    "            \n",
    "        返回：\n",
    "            配置值\n",
    "        \n",
    "        作用：\n",
    "        1. 支持点号分隔的嵌套路径访问\n",
    "        2. 安全地处理不存在的键\n",
    "        3. 提供默认值机制\n",
    "        \n",
    "        示例：\n",
    "            config.get(\"model.model_name\") -> \"DeepSeek-R1-Distill-Qwen-7B\"\n",
    "            config.get(\"training.learning_rate\") -> 5e-6\n",
    "        \"\"\"\n",
    "        keys = key_path.split('.')  # 按点号分割路径\n",
    "        value = self._config\n",
    "        \n",
    "        try:\n",
    "            # 逐级访问嵌套字典\n",
    "            for key in keys:\n",
    "                value = value[key]\n",
    "            return value\n",
    "        except (KeyError, TypeError):\n",
    "            # 键不存在或类型错误时返回默认值\n",
    "            return default\n",
    "    \n",
    "    def set(self, key_path: str, value: Any):\n",
    "        \"\"\"\n",
    "        设置配置值\n",
    "        \n",
    "        参数：\n",
    "            key_path: 键路径\n",
    "            value: 要设置的值\n",
    "        \n",
    "        作用：\n",
    "        1. 支持嵌套路径设置\n",
    "        2. 自动创建中间层级的字典\n",
    "        3. 更新内存中的配置\n",
    "        \"\"\"\n",
    "        keys = key_path.split('.')\n",
    "        config = self._config\n",
    "        \n",
    "        # 导航到父级，创建中间层级\n",
    "        for key in keys[:-1]:\n",
    "            if key not in config:\n",
    "                config[key] = {}\n",
    "            config = config[key]\n",
    "        \n",
    "        # 设置最终值\n",
    "        config[keys[-1]] = value\n",
    "    \n",
    "    def update(self, updates: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        批量更新配置\n",
    "        \n",
    "        参数：\n",
    "            updates: 更新字典，键为路径格式\n",
    "        \n",
    "        作用：一次性更新多个配置项\n",
    "        \n",
    "        示例：\n",
    "            config.update({\n",
    "                \"training.learning_rate\": 1e-5,\n",
    "                \"training.batch_size\": 4\n",
    "            })\n",
    "        \"\"\"\n",
    "        for key_path, value in updates.items():\n",
    "            self.set(key_path, value)\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        保存配置到文件\n",
    "        \n",
    "        作用：将内存中的配置写入到原始配置文件\n",
    "        \"\"\"\n",
    "        self.config_loader.save_json(self.config_file, self._config)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        返回完整配置字典\n",
    "        \n",
    "        返回：\n",
    "            Dict: 配置字典的副本\n",
    "        \n",
    "        作用：获取完整配置的副本，避免外部修改\n",
    "        \"\"\"\n",
    "        return self._config.copy()\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"\n",
    "        打印当前配置\n",
    "        \n",
    "        作用：\n",
    "        1. 格式化显示关键配置信息\n",
    "        2. 便于调试和确认配置\n",
    "        3. 在训练开始前进行配置检查\n",
    "        \"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"GRPO训练配置\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"基座模型: {self.get('model.model_name')}\")\n",
    "        print(f\"奖励模型: {self.get('model.reward_model_path')}\")\n",
    "        \n",
    "        # 检查是否使用LoRA\n",
    "        lora_path = self.get('model.lora_model_path')\n",
    "        if lora_path:\n",
    "            print(f\"LoRA模型: {lora_path}\")\n",
    "        \n",
    "        # 数据和输出配置\n",
    "        print(f\"训练数据: {self.get('data.train_data_path')}\")\n",
    "        print(f\"输出目录: {self.get('output.output_dir')}\")\n",
    "        \n",
    "        # 训练超参数\n",
    "        print(f\"学习率: {self.get('training.learning_rate')}\")\n",
    "        print(f\"训练轮数: {self.get('training.num_train_epochs')}\")\n",
    "        print(f\"每GPU批次: {self.get('training.per_device_batch_size')}\")\n",
    "        print(f\"梯度累积: {self.get('training.gradient_accumulation_steps')}\")\n",
    "        \n",
    "        # GRPO特有参数\n",
    "        print(f\"生成样本数: {self.get('grpo.num_generations')}\")\n",
    "        print(f\"使用Liger优化: {self.get('grpo.use_liger_loss')}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "class DeepSpeedConfig:\n",
    "    \"\"\"\n",
    "    DeepSpeed配置管理器\n",
    "    \n",
    "    作用：管理DeepSpeed分布式训练的配置\n",
    "    功能：\n",
    "    1. 加载DeepSpeed配置文件\n",
    "    2. 分离Accelerate和DeepSpeed配置\n",
    "    3. 动态更新GPU数量配置\n",
    "    4. 支持配置导出为YAML格式\n",
    "    \n",
    "    DeepSpeed是什么：\n",
    "    - 微软开发的分布式训练框架\n",
    "    - 支持ZeRO优化器状态分片\n",
    "    - 可以训练超大规模模型\n",
    "    - 与PyTorch和Transformers集成\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_file: str = \"deepspeed_zero3.json\"):\n",
    "        \"\"\"\n",
    "        初始化DeepSpeed配置\n",
    "        \n",
    "        参数：\n",
    "            config_file: DeepSpeed配置文件名\n",
    "        \"\"\"\n",
    "        self.config_loader = ConfigLoader()\n",
    "        self.config_file = config_file\n",
    "        self._config = self.config_loader.load_json(config_file)\n",
    "    \n",
    "    def get_accelerate_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取Accelerate使用的配置部分\n",
    "        \n",
    "        返回：\n",
    "            Accelerate配置字典\n",
    "        \n",
    "        作用：\n",
    "        1. 过滤掉DeepSpeed特有的配置\n",
    "        2. 返回Accelerate框架需要的配置\n",
    "        3. 支持Accelerate + DeepSpeed集成\n",
    "        \"\"\"\n",
    "        accelerate_config = {}\n",
    "        exclude_keys = ['deepspeed_config']  # 排除的键\n",
    "        \n",
    "        for key, value in self._config.items():\n",
    "            if key not in exclude_keys:\n",
    "                accelerate_config[key] = value\n",
    "        \n",
    "        return accelerate_config\n",
    "    \n",
    "    def get_deepspeed_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取DeepSpeed引擎配置\n",
    "        \n",
    "        返回：\n",
    "            DeepSpeed配置字典\n",
    "        \n",
    "        作用：提取DeepSpeed引擎需要的配置部分\n",
    "        \"\"\"\n",
    "        return self._config.get('deepspeed_config', {})\n",
    "    \n",
    "    def update_gpu_count(self, num_gpus: int):\n",
    "        \"\"\"\n",
    "        更新GPU数量配置\n",
    "        \n",
    "        参数：\n",
    "            num_gpus: GPU数量\n",
    "        \n",
    "        作用：\n",
    "        1. 更新进程数量配置\n",
    "        2. 重新计算全局batch size\n",
    "        3. 确保配置与硬件匹配\n",
    "        \n",
    "        \"\"\"\n",
    "        # 更新进程数量\n",
    "        self._config['num_processes'] = num_gpus\n",
    "        \n",
    "        # 获取DeepSpeed配置\n",
    "        deepspeed_config = self._config.get('deepspeed_config', {})\n",
    "        micro_batch_size = deepspeed_config.get('train_micro_batch_size_per_gpu', 1)\n",
    "        gradient_accumulation = deepspeed_config.get('gradient_accumulation_steps', 4)\n",
    "        \n",
    "        # 计算全局batch size\n",
    "        global_batch_size = num_gpus * micro_batch_size * gradient_accumulation\n",
    "        deepspeed_config['train_batch_size'] = global_batch_size\n",
    "    \n",
    "    def save_to_yaml(self, output_path: str):\n",
    "        \"\"\"\n",
    "        保存为YAML格式（兼容accelerate launch）\n",
    "        \n",
    "        参数：\n",
    "            output_path: 输出文件路径\n",
    "        \n",
    "        作用：\n",
    "        1. 转换为YAML格式\n",
    "        2. 兼容accelerate launch命令\n",
    "        3. 便于人类阅读和编辑\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import yaml\n",
    "        except ImportError:\n",
    "            raise ImportError(\"需要安装PyYAML: pip install PyYAML\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(self._config, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        返回完整配置字典\n",
    "        \n",
    "        返回：\n",
    "            Dict: 配置字典的副本\n",
    "        \"\"\"\n",
    "        return self._config.copy()\n",
    "\n",
    "\n",
    "# 全局配置实例创建函数\n",
    "def load_configs():\n",
    "    \"\"\"\n",
    "    加载所有配置\n",
    "    \n",
    "    返回：\n",
    "        Tuple: (项目配置, DeepSpeed配置)\n",
    "    \n",
    "    作用：一次性加载项目需要的所有配置\n",
    "    \"\"\"\n",
    "    project_config = ProjectConfig()\n",
    "    deepspeed_config = DeepSpeedConfig()\n",
    "    return project_config, deepspeed_config\n",
    "\n",
    "\n",
    "# 便捷函数\n",
    "def get_project_config() -> ProjectConfig:\n",
    "    \"\"\"\n",
    "    获取项目配置实例\n",
    "    \n",
    "    返回：\n",
    "        ProjectConfig: 项目配置实例\n",
    "    \n",
    "    作用：提供全局访问项目配置的便捷方法\n",
    "    \"\"\"\n",
    "    return ProjectConfig()\n",
    "\n",
    "\n",
    "def get_deepspeed_config() -> DeepSpeedConfig:\n",
    "    \"\"\"\n",
    "    获取DeepSpeed配置实例\n",
    "    \n",
    "    返回：\n",
    "        DeepSpeedConfig: DeepSpeed配置实例\n",
    "    \n",
    "    作用：提供全局访问DeepSpeed配置的便捷方法\n",
    "    \"\"\"\n",
    "    return DeepSpeedConfig()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logger.py 日志管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552303b7",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "日志工具类\n",
    "作用：为GRPO训练过程提供统一的日志记录功能\n",
    "功能：支持控制台输出、文件记录、训练过程跟踪\n",
    "\"\"\"\n",
    "import logging                    # Python标准日志库\n",
    "import sys                       # 系统相关功能\n",
    "from datetime import datetime    # 日期时间处理\n",
    "from pathlib import Path        # 路径操作工具\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    训练日志管理器\n",
    "    \n",
    "    作用：统一管理GRPO训练过程中的所有日志输出\n",
    "    功能：\n",
    "    1. 支持多种日志级别（INFO、WARNING、ERROR、DEBUG）\n",
    "    2. 同时输出到控制台和文件\n",
    "    3. 提供训练专用的日志记录方法\n",
    "    4. 自动格式化日志消息\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"GRPO\", log_level: str = \"INFO\", \n",
    "                 log_file: str = None, console_output: bool = True):\n",
    "        \"\"\"\n",
    "        初始化日志器\n",
    "        \n",
    "        参数说明：\n",
    "            name: 日志器名称，用于区分不同模块的日志\n",
    "            log_level: 日志级别（DEBUG < INFO < WARNING < ERROR）\n",
    "            log_file: 日志文件路径，如果为None则不保存到文件\n",
    "            console_output: 是否同时输出到控制台\n",
    "        \"\"\"\n",
    "        # 创建Python标准日志器实例\n",
    "        self.logger = logging.getLogger(name)\n",
    "        # 设置日志级别 - 只有达到此级别的消息才会被记录\n",
    "        self.logger.setLevel(getattr(logging, log_level.upper()))\n",
    "        \n",
    "        # 清除已有的处理器，避免重复输出\n",
    "        self.logger.handlers.clear()\n",
    "        \n",
    "        # 设置日志消息格式\n",
    "        # 格式：时间 - 日志器名称 - 级别 - 消息内容\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'  # 时间格式\n",
    "        )\n",
    "        \n",
    "        # 控制台输出处理器\n",
    "        if console_output:\n",
    "            console_handler = logging.StreamHandler(sys.stdout)  # 输出到标准输出\n",
    "            console_handler.setFormatter(formatter)              # 应用格式\n",
    "            self.logger.addHandler(console_handler)              # 添加到日志器\n",
    "        \n",
    "        # 文件输出处理器\n",
    "        if log_file:\n",
    "            # 确保日志目录存在\n",
    "            Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "            file_handler = logging.FileHandler(log_file, encoding='utf-8')  # 创建文件处理器\n",
    "            file_handler.setFormatter(formatter)                           # 应用格式\n",
    "            self.logger.addHandler(file_handler)                           # 添加到日志器\n",
    "    \n",
    "    def info(self, message: str):\n",
    "        \"\"\"\n",
    "        输出信息日志\n",
    "        用途：记录一般性信息，如训练进度、状态更新等\n",
    "        \"\"\"\n",
    "        self.logger.info(message)\n",
    "    \n",
    "    def warning(self, message: str):\n",
    "        \"\"\"\n",
    "        输出警告日志\n",
    "        用途：记录可能的问题，但不影响程序继续运行\n",
    "        \"\"\"\n",
    "        self.logger.warning(message)\n",
    "    \n",
    "    def error(self, message: str):\n",
    "        \"\"\"\n",
    "        输出错误日志\n",
    "        用途：记录严重错误，可能导致程序异常\n",
    "        \"\"\"\n",
    "        self.logger.error(message)\n",
    "    \n",
    "    def debug(self, message: str):\n",
    "        \"\"\"\n",
    "        输出调试日志\n",
    "        用途：记录详细的调试信息，通常只在开发阶段使用\n",
    "        \"\"\"\n",
    "        self.logger.debug(message)\n",
    "    \n",
    "    def log_training_start(self, config):\n",
    "        \"\"\"\n",
    "        记录训练开始信息\n",
    "        \n",
    "        参数：\n",
    "            config: 训练配置对象，包含所有训练参数\n",
    "        \n",
    "        作用：在训练开始时记录完整的配置信息，便于后续追踪\n",
    "        \"\"\"\n",
    "        self.info(\"=\" * 60)\n",
    "        self.info(\"GRPO金融模型训练开始\")\n",
    "        self.info(\"=\" * 60)\n",
    "        self.info(f\"开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # 记录关键配置信息\n",
    "        # 使用嵌套路径访问配置项（如 'model.model_name'）\n",
    "        self.info(f\"基座模型: {config.get('model.model_name')}\")\n",
    "        self.info(f\"奖励模型: {config.get('model.reward_model_path')}\")\n",
    "        \n",
    "        # 检查是否使用LoRA微调\n",
    "        lora_path = config.get('model.lora_model_path')\n",
    "        if lora_path:\n",
    "            self.info(f\"LoRA模型: {lora_path}\")\n",
    "        \n",
    "        # 记录数据和输出配置\n",
    "        self.info(f\"训练数据: {config.get('data.train_data_path')}\")\n",
    "        self.info(f\"输出目录: {config.get('output.output_dir')}\")\n",
    "        \n",
    "        # 记录训练超参数\n",
    "        self.info(f\"学习率: {config.get('training.learning_rate')}\")\n",
    "        self.info(f\"训练轮数: {config.get('training.num_train_epochs')}\")\n",
    "        self.info(f\"批次大小: {config.get('training.per_device_batch_size')}\")\n",
    "        self.info(f\"梯度累积: {config.get('training.gradient_accumulation_steps')}\")\n",
    "        self.info(\"=\" * 60)\n",
    "    \n",
    "    def log_training_end(self, success: bool = True):\n",
    "        \"\"\"\n",
    "        记录训练结束信息\n",
    "        \n",
    "        参数：\n",
    "            success: 训练是否成功完成\n",
    "        \n",
    "        作用：记录训练结束状态和时间\n",
    "        \"\"\"\n",
    "        self.info(\"=\" * 60)\n",
    "        if success:\n",
    "            self.info(\"GRPO训练成功完成！\")\n",
    "        else:\n",
    "            self.error(\"GRPO训练失败！\")\n",
    "        self.info(f\"结束时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.info(\"=\" * 60)\n",
    "    \n",
    "    def log_step(self, step: int, loss: float, learning_rate: float, \n",
    "                 reward_mean: float = None):\n",
    "        \"\"\"\n",
    "        记录训练步骤信息\n",
    "        \n",
    "        参数：\n",
    "            step: 当前训练步数\n",
    "            loss: 当前损失值\n",
    "            learning_rate: 当前学习率\n",
    "            reward_mean: 平均奖励值（GRPO特有）\n",
    "        \n",
    "        作用：记录每个训练步骤的关键指标\n",
    "        \"\"\"\n",
    "        msg = f\"Step {step} - Loss: {loss:.4f} - LR: {learning_rate:.2e}\"\n",
    "        if reward_mean is not None:\n",
    "            msg += f\" - Reward: {reward_mean:.4f}\"\n",
    "        self.info(msg)\n",
    "\n",
    "\n",
    "# 创建全局日志实例的工厂函数\n",
    "def create_logger(output_dir: str = \"./output\", log_level: str = \"INFO\") -> Logger:\n",
    "    \"\"\"\n",
    "    创建日志实例\n",
    "    \n",
    "    参数：\n",
    "        output_dir: 输出目录，日志文件将保存在此目录下\n",
    "        log_level: 日志级别\n",
    "    \n",
    "    返回：\n",
    "        Logger实例\n",
    "    \n",
    "    作用：为整个项目提供统一的日志创建入口\n",
    "    \"\"\"\n",
    "    # 创建日志文件路径\n",
    "    log_file = Path(output_dir) / \"training.log\"\n",
    "    \n",
    "    return Logger(\n",
    "        name=\"GRPO_Training\",      # 日志器名称\n",
    "        log_level=log_level,       # 日志级别\n",
    "        log_file=str(log_file),    # 日志文件路径\n",
    "        console_output=True        # 启用控制台输出\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db02f07",
   "metadata": {},
   "source": [
    "### dataset.py    数据集处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0dff2",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "数据集处理工具类\n",
    "作用：为GRPO训练提供数据加载和预处理功能\n",
    "功能：支持JSONL格式数据加载、对话格式化、奖励函数创建\n",
    "\"\"\"\n",
    "import json                              # JSON数据处理\n",
    "from typing import List, Dict, Any       # 类型提示\n",
    "from pathlib import Path                 # 路径操作\n",
    "from datasets import Dataset             # Hugging Face数据集库\n",
    "from transformers import PreTrainedTokenizer  # 分词器基类\n",
    "import torch                            # PyTorch深度学习框架\n",
    "\n",
    "\n",
    "class GRPODatasetLoader:\n",
    "    \"\"\"\n",
    "    GRPO数据集加载器\n",
    "    \n",
    "    作用：专门为GRPO训练加载和处理数据\n",
    "    功能：\n",
    "    1. 加载JSONL格式的训练数据\n",
    "    2. 将数据格式化为对话格式\n",
    "    3. 创建适合GRPO训练的数据集\n",
    "    4. 处理数据加载过程中的错误\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, logger=None):\n",
    "        \"\"\"\n",
    "        初始化数据集加载器\n",
    "        \n",
    "        参数：\n",
    "            tokenizer: 分词器实例，用于文本处理\n",
    "            logger: 日志器实例，用于记录处理过程\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer  # 保存分词器引用\n",
    "        self.logger = logger        # 保存日志器引用\n",
    "    \n",
    "    def load_jsonl(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        加载JSONL格式数据文件\n",
    "        \n",
    "        参数：\n",
    "            file_path: 数据文件路径\n",
    "            \n",
    "        返回：\n",
    "            List[Dict]: 解析后的数据列表，每个元素是一个字典\n",
    "        \n",
    "        作用：\n",
    "        1. 逐行读取JSONL文件\n",
    "        2. 解析每行的JSON数据\n",
    "        3. 验证数据格式\n",
    "        4. 统计加载结果\n",
    "        \"\"\"\n",
    "        # 检查文件是否存在\n",
    "        if not Path(file_path).exists():\n",
    "            raise FileNotFoundError(f\"数据文件不存在: {file_path}\")\n",
    "        \n",
    "        data = []           # 存储有效数据\n",
    "        error_count = 0     # 错误计数\n",
    "        \n",
    "        # 逐行读取文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()  # 去除首尾空白\n",
    "                if not line:         # 跳过空行\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # 解析JSON数据\n",
    "                    item = json.loads(line)\n",
    "                    # 验证必需字段\n",
    "                    if 'prompt' in item:\n",
    "                        data.append(item)\n",
    "                    else:\n",
    "                        if self.logger:\n",
    "                            self.logger.warning(f\"第{line_num}行缺少'prompt'字段\")\n",
    "                        error_count += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # JSON解析错误\n",
    "                    if self.logger:\n",
    "                        self.logger.error(f\"第{line_num}行JSON解析失败: {e}\")\n",
    "                    error_count += 1\n",
    "        \n",
    "        # 记录加载结果\n",
    "        if self.logger:\n",
    "            self.logger.info(f\"数据加载完成: {len(data)}条有效记录, {error_count}条错误记录\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def format_chat_prompt(self, prompt: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        将prompt格式化为对话格式\n",
    "        \n",
    "        参数：\n",
    "            prompt: 原始prompt文本\n",
    "            \n",
    "        返回：\n",
    "            List[Dict]: 格式化后的对话消息列表\n",
    "        \n",
    "        作用：\n",
    "        1. 将单一的prompt转换为多轮对话格式\n",
    "        2. 添加系统提示词，定义AI助手的角色\n",
    "        3. 将用户问题包装为用户消息\n",
    "        \n",
    "        GRPO训练需要对话格式的数据，因为：\n",
    "        - 现代语言模型都是基于对话训练的\n",
    "        - 对话格式可以更好地控制模型行为\n",
    "        - 系统提示词可以定义专业领域角色\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",  # 系统角色\n",
    "                \"content\": \"你是一个专业的金融分析师，擅长投资分析和风险评估。请基于提供的信息进行详细的分析和推理。\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",    # 用户角色\n",
    "                \"content\": prompt  # 用户问题\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def create_dataset(self, data_path: str) -> Dataset:\n",
    "        \"\"\"\n",
    "        创建GRPO训练数据集\n",
    "        \n",
    "        参数：\n",
    "            data_path: 数据文件路径\n",
    "            \n",
    "        返回：\n",
    "            Dataset: HuggingFace数据集对象\n",
    "        \n",
    "        作用：\n",
    "        1. 加载原始数据\n",
    "        2. 格式化为对话格式\n",
    "        3. 转换为HuggingFace数据集\n",
    "        4. 保留原始数据的其他字段\n",
    "        \n",
    "        GRPO训练的数据特点：\n",
    "        - 只需要prompt，不需要预定义答案\n",
    "        - 训练时模型会生成多个回复\n",
    "        - 使用奖励模型评分和优化\n",
    "        \"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.info(f\"开始加载数据集: {data_path}\")\n",
    "        \n",
    "        # 加载原始数据\n",
    "        raw_data = self.load_jsonl(data_path)\n",
    "        \n",
    "        # 格式化数据\n",
    "        formatted_data = []\n",
    "        for item in raw_data:\n",
    "            # 格式化prompt为对话格式\n",
    "            formatted_item = {\n",
    "                \"prompt\": self.format_chat_prompt(item[\"prompt\"])\n",
    "            }\n",
    "            # 保留其他字段（如果有的话）\n",
    "            for key, value in item.items():\n",
    "                if key != \"prompt\":\n",
    "                    formatted_item[key] = value\n",
    "            \n",
    "            formatted_data.append(formatted_item)\n",
    "        \n",
    "        # 转换为HuggingFace数据集\n",
    "        dataset = Dataset.from_list(formatted_data)\n",
    "        \n",
    "        if self.logger:\n",
    "            self.logger.info(f\"数据集创建完成: {len(dataset)}条记录\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "\n",
    "def create_reward_function(reward_model_path: str, logger=None):\n",
    "    \"\"\"\n",
    "    创建奖励函数\n",
    "    \n",
    "    参数：\n",
    "        reward_model_path: 奖励模型路径\n",
    "        logger: 日志器实例\n",
    "    \n",
    "    返回：\n",
    "        reward_function: 奖励计算函数\n",
    "    \n",
    "    作用：\n",
    "    1. 加载预训练的奖励模型\n",
    "    2. 创建GRPO兼容的奖励函数\n",
    "    3. 确保文本格式与奖励模型训练时一致\n",
    "    \n",
    "    奖励函数的重要性：\n",
    "    - GRPO是强化学习算法，需要奖励信号指导训练\n",
    "    - 奖励模型评估生成文本的质量\n",
    "    - 文本格式必须与奖励模型训练时完全一致\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    \n",
    "    if logger:\n",
    "        logger.info(f\"加载奖励模型: {reward_model_path}\")\n",
    "    \n",
    "    # 为奖励模型使用专用的分词器\n",
    "    # 重要：奖励模型可能使用不同的分词器配置\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        reward_model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 设置填充token\n",
    "    if reward_tokenizer.pad_token is None:\n",
    "        reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "    \n",
    "    # 加载奖励模型\n",
    "    # 奖励模型是一个分类模型，输出单一分数\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        reward_model_path,\n",
    "        num_labels=1,                    # 输出单一奖励分数\n",
    "        torch_dtype=torch.bfloat16,      # 使用半精度节省显存\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False                  # 禁用缓存节省内存\n",
    "    )\n",
    "    reward_model.eval()  # 设置为评估模式\n",
    "    \n",
    "    if logger:\n",
    "        logger.info(\"奖励模型加载完成\")\n",
    "    \n",
    "    def reward_function(prompts, completions, **kwargs):\n",
    "        \"\"\"\n",
    "        GRPO兼容的奖励函数 - 使用与奖励模型训练一致的LLaMA-3格式\n",
    "        \n",
    "        参数：\n",
    "            prompts: 输入提示列表\n",
    "            completions: 模型生成的完成文本列表\n",
    "            **kwargs: 其他参数\n",
    "        \n",
    "        返回：\n",
    "            rewards: 奖励分数列表\n",
    "        \n",
    "        关键修复：\n",
    "        - 使用与奖励模型训练完全相同的文本格式\n",
    "        - LLaMA-3格式的特殊token必须一致\n",
    "        - 避免格式不匹配导致的奖励异常\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        # 禁用梯度计算，节省内存\n",
    "        with torch.no_grad():\n",
    "            for prompt, completion in zip(prompts, completions):\n",
    "                try:\n",
    "                    # 提取用户问题\n",
    "                    if isinstance(prompt, list):\n",
    "                        # 从对话格式中提取用户问题\n",
    "                        question = next(msg[\"content\"] for msg in prompt if msg[\"role\"] == \"user\")\n",
    "                    else:\n",
    "                        question = prompt\n",
    "                    \n",
    "                    # 处理completion\n",
    "                    if isinstance(completion, list):\n",
    "                        completion_text = completion[0][\"content\"]\n",
    "                    else:\n",
    "                        completion_text = completion\n",
    "                    \n",
    "                    # 关键修复：使用与奖励模型训练完全相同的LLaMA-3格式\n",
    "                    # 这个格式必须与奖励模型训练时的格式完全一致\n",
    "                    formatted_text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{completion_text}<|eot_id|>\"\n",
    "                    \n",
    "                    # 使用奖励模型计算分数\n",
    "                    inputs = reward_tokenizer(\n",
    "                        formatted_text,\n",
    "                        truncation=True,           # 截断过长文本\n",
    "                        padding=True,              # 填充到统一长度\n",
    "                        max_length=2048,          # 与奖励模型训练时一致\n",
    "                        return_tensors=\"pt\"        # 返回PyTorch张量\n",
    "                    )\n",
    "                    \n",
    "                    # 将输入移动到模型设备\n",
    "                    inputs = {k: v.to(reward_model.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    # 模型推理\n",
    "                    outputs = reward_model(**inputs)\n",
    "                    \n",
    "                    # 提取奖励分数\n",
    "                    reward = outputs.logits.squeeze().cpu().item()\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # 处理计算失败的情况\n",
    "                    if logger:\n",
    "                        logger.warning(f\"奖励计算失败: {e}\")\n",
    "                    rewards.append(0.0)  # 使用默认分数\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    return reward_function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a50c1",
   "metadata": {},
   "source": [
    "### train.py  主训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f5ba9",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "GRPO训练主文件 - 简单修复DeepSpeed参数传递\n",
    "作用：实现完整的GRPO强化学习训练流程\n",
    "功能：包含模型加载、数据处理、训练器配置、最佳模型保存等\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# ==================== PyTorch加载修复 ====================\n",
    "# 修复checkpoint加载时的weights_only问题\n",
    "# 背景：新版PyTorch对checkpoint加载有更严格的安全检查\n",
    "original_torch_load = torch.load  # 保存原始加载函数\n",
    "\n",
    "def patched_torch_load(f, map_location=None, pickle_module=None, weights_only=None, **kwargs):\n",
    "    \"\"\"\n",
    "    修复的torch.load函数\n",
    "    \n",
    "    参数：\n",
    "        f: 文件路径或文件对象\n",
    "        map_location: 设备映射\n",
    "        pickle_module: pickle模块\n",
    "        weights_only: 是否只加载权重\n",
    "        **kwargs: 其他参数\n",
    "    \n",
    "    作用：\n",
    "    1. 检测是否是checkpoint相关文件\n",
    "    2. 对于checkpoint文件，强制设置weights_only=False\n",
    "    3. 避免PyTorch安全检查导致的加载失败\n",
    "    \"\"\"\n",
    "    # 如果是加载checkpoint相关文件，强制设置weights_only=False\n",
    "    # 这样可以避免某些版本的PyTorch过于严格的安全检查\n",
    "    if weights_only is True and (\n",
    "        isinstance(f, str) and ('rng_state' in f or 'checkpoint' in f)\n",
    "    ):\n",
    "        weights_only = False\n",
    "    return original_torch_load(f, map_location=map_location, pickle_module=pickle_module, \n",
    "                              weights_only=weights_only, **kwargs)\n",
    "\n",
    "# 替换PyTorch的加载函数\n",
    "torch.load = patched_torch_load\n",
    "\n",
    "# ==================== 导入必要的库 ====================\n",
    "import time                                      # 时间处理\n",
    "from pathlib import Path                         # 路径操作\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback  # Transformers库\n",
    "from peft import PeftModel                       # LoRA等参数高效微调\n",
    "from trl import GRPOConfig, GRPOTrainer         # TRL强化学习库\n",
    "import argparse                                  # 命令行参数解析\n",
    "import numpy as np                               # 数值计算\n",
    "import os                                        # 操作系统接口\n",
    "from utils.config_loader import ProjectConfig   # 项目配置加载器\n",
    "from utils.logger import create_logger          # 日志工具\n",
    "from utils.dataset import GRPODatasetLoader, create_reward_function  # 数据集和奖励函数\n",
    "\n",
    "\n",
    "class GRPOBestModelEarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    GRPO最佳模型早停回调\n",
    "    \n",
    "    作用：\n",
    "    1. 自动保存最佳模型\n",
    "    2. 实现早停机制，防止过拟合\n",
    "    3. 监控训练和验证指标\n",
    "    4. 管理模型保存策略\n",
    "    \n",
    "    GRPO训练的特点：\n",
    "    - 强化学习训练可能不稳定\n",
    "    - 需要持续监控奖励指标\n",
    "    - 最佳模型可能出现在训练中期\n",
    "    - 需要保存最佳性能的模型而非最后的模型\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        early_stopping_patience: int = 3,           # 早停耐心值\n",
    "        early_stopping_threshold: float = 0.001,    # 早停阈值\n",
    "        train_reward_weight: float = 0.3,           # 训练奖励权重\n",
    "        eval_reward_weight: float = 0.7,            # 验证奖励权重\n",
    "        min_eval_steps: int = 2,                    # 最小验证步数\n",
    "        best_model_dir: str = None,                 # 最佳模型保存目录\n",
    "        tokenizer = None                            # 分词器实例\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化早停回调\n",
    "        \n",
    "        参数说明：\n",
    "            early_stopping_patience: 连续多少次验证无改善后停止训练\n",
    "            early_stopping_threshold: 认为有改善的最小阈值\n",
    "            train_reward_weight: 训练奖励在综合评分中的权重\n",
    "            eval_reward_weight: 验证奖励在综合评分中的权重\n",
    "            min_eval_steps: 开始早停检查前的最小验证步数\n",
    "            best_model_dir: 最佳模型保存目录\n",
    "            tokenizer: 分词器实例，用于保存\n",
    "        \"\"\"\n",
    "        # 早停参数\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.train_reward_weight = train_reward_weight\n",
    "        self.eval_reward_weight = eval_reward_weight\n",
    "        self.min_eval_steps = min_eval_steps\n",
    "        \n",
    "        # 保存tokenizer引用\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 最佳模型保存路径\n",
    "        self.best_model_dir = best_model_dir or \"./output/best_model\"\n",
    "        Path(self.best_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 训练指标跟踪\n",
    "        self.latest_train_reward = 0.0      # 最新训练奖励\n",
    "        self.best_train_reward = None       # 最佳训练奖励\n",
    "        \n",
    "        # 验证指标跟踪\n",
    "        self.best_eval_reward = None        # 最佳验证奖励\n",
    "        self.eval_count = 0                 # 验证次数计数\n",
    "        \n",
    "        # 早停控制变量\n",
    "        self.patience_counter = 0           # 耐心计数器\n",
    "        self.best_combined_score = None     # 最佳综合得分\n",
    "        self.best_step = None               # 最佳模型对应的步数\n",
    "        \n",
    "        # 调试计数器\n",
    "        self.log_call_count = 0             # 日志调用次数\n",
    "        self.eval_call_count = 0            # 验证调用次数\n",
    "        \n",
    "        print(f\"回调函数初始化: 训练权重={train_reward_weight}, 验证权重={eval_reward_weight}\")\n",
    "        print(f\"最佳模型保存目录: {self.best_model_dir}\")\n",
    "        print(f\"分词器引用: {self.tokenizer is not None}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        监听所有日志事件\n",
    "        \n",
    "        参数：\n",
    "            args: 训练参数\n",
    "            state: 训练状态\n",
    "            control: 训练控制\n",
    "            logs: 日志字典\n",
    "            **kwargs: 其他参数（包含model等）\n",
    "        \n",
    "        作用：\n",
    "        1. 监听训练过程中的所有日志\n",
    "        2. 识别验证日志并触发模型保存\n",
    "        3. 更新训练奖励记录\n",
    "        4. 实现最佳模型保存逻辑\n",
    "        \"\"\"\n",
    "        self.log_call_count += 1\n",
    "        \n",
    "        print(f\"on_log调用#{self.log_call_count} - Step {state.global_step}\")\n",
    "        \n",
    "        if logs:\n",
    "            # 检查是否是验证日志\n",
    "            # 验证日志的特征是包含以'eval_'开头的键\n",
    "            if any(key.startswith('eval_') for key in logs.keys()):\n",
    "                print(\"检测到验证日志，尝试保存最佳模型...\")\n",
    "                \n",
    "                # 获取验证奖励\n",
    "                eval_reward = logs.get('eval_reward')\n",
    "                if eval_reward is not None:\n",
    "                    print(f\"验证奖励: {eval_reward}\")\n",
    "                    \n",
    "                    # 判断是否需要保存模型\n",
    "                    should_save = False\n",
    "                    if self.best_eval_reward is None or eval_reward > self.best_eval_reward:\n",
    "                        self.best_eval_reward = eval_reward\n",
    "                        should_save = True\n",
    "                        print(f\"发现更好的验证奖励: {eval_reward}\")\n",
    "                    \n",
    "                    # 第一次验证时强制保存\n",
    "                    if self.eval_count == 0:\n",
    "                        should_save = True\n",
    "                        print(\"第一次验证，强制保存模型\")\n",
    "                    \n",
    "                    if should_save:\n",
    "                        # 从kwargs中获取模型实例\n",
    "                        model = kwargs.get('model')\n",
    "                        \n",
    "                        if model and self.tokenizer:\n",
    "                            self._save_best_model(model, self.tokenizer, state.global_step)\n",
    "                        else:\n",
    "                            print(f\"无法获取model或tokenizer: model={model is not None}, tokenizer={self.tokenizer is not None}\")\n",
    "                    \n",
    "                    self.eval_count += 1\n",
    "            \n",
    "            # 获取训练奖励\n",
    "            # GRPO训练中奖励可能以不同的键名出现\n",
    "            possible_reward_keys = ['reward', 'rewards/reward_function/mean', 'train_reward', 'reward_mean']\n",
    "            for key in possible_reward_keys:\n",
    "                if key in logs:\n",
    "                    self.latest_train_reward = logs[key]\n",
    "                    break\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        验证回调 - 简化版本\n",
    "        \n",
    "        参数：\n",
    "            args: 训练参数\n",
    "            state: 训练状态\n",
    "            control: 训练控制\n",
    "            logs: 日志字典\n",
    "            **kwargs: 其他参数\n",
    "        \n",
    "        作用：\n",
    "        1. 记录验证回调的调用\n",
    "        2. 验证逻辑已经移到on_log中处理\n",
    "        \n",
    "        注意：实际的验证处理逻辑在on_log方法中\n",
    "        \"\"\"\n",
    "        self.eval_call_count += 1\n",
    "        print(f\"on_evaluate调用#{self.eval_call_count} - Step {state.global_step}\")\n",
    "        \n",
    "        # 验证逻辑已经移到 on_log 中处理\n",
    "        return control\n",
    "    \n",
    "    def _save_best_model(self, model, tokenizer, step):\n",
    "        \"\"\"\n",
    "        保存最佳模型\n",
    "        \n",
    "        参数：\n",
    "            model: 模型实例\n",
    "            tokenizer: 分词器实例\n",
    "            step: 当前训练步数\n",
    "        \n",
    "        作用：\n",
    "        1. 处理DeepSpeed包装的模型\n",
    "        2. 保存模型权重和配置\n",
    "        3. 保存分词器\n",
    "        4. 记录模型元信息\n",
    "        5. 处理保存过程中的异常\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"开始保存最佳模型 (Step {step})...\")\n",
    "            \n",
    "            # 确保目录存在\n",
    "            Path(self.best_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 处理DeepSpeed包装的模型\n",
    "            # DeepSpeed会用module属性包装原始模型\n",
    "            actual_model = model.module if hasattr(model, 'module') else model\n",
    "                \n",
    "            # 保存模型\n",
    "            actual_model.save_pretrained(\n",
    "                self.best_model_dir,              # 保存目录\n",
    "                safe_serialization=True,          # 使用安全序列化\n",
    "                max_shard_size=\"2GB\"              # 最大分片大小\n",
    "            )\n",
    "            \n",
    "            # 保存分词器\n",
    "            tokenizer.save_pretrained(self.best_model_dir)\n",
    "            \n",
    "            # 保存元信息\n",
    "            # 记录模型的关键指标和保存时间\n",
    "            best_info = {\n",
    "                \"best_step\": step,                         # 最佳模型对应的步数\n",
    "                \"best_combined_score\": self.best_combined_score,  # 最佳综合得分\n",
    "                \"best_train_reward\": self.best_train_reward,      # 最佳训练奖励\n",
    "                \"best_eval_reward\": self.best_eval_reward,        # 最佳验证奖励\n",
    "                \"latest_train_reward\": self.latest_train_reward,  # 最新训练奖励\n",
    "                \"save_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\")   # 保存时间\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(f\"{self.best_model_dir}/best_model_info.json\", \"w\") as f:\n",
    "                json.dump(best_info, f, indent=2)\n",
    "                \n",
    "            print(f\"最佳模型保存完成！文件数量: {len(list(Path(self.best_model_dir).glob('*')))}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"保存最佳模型失败: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # 打印完整的错误堆栈\n",
    "\n",
    "\n",
    "class GRPOTraining:\n",
    "    \"\"\"\n",
    "    GRPO训练主类\n",
    "    \n",
    "    作用：\n",
    "    1. 统一管理GRPO训练的所有组件\n",
    "    2. 按照标准流程初始化各个模块\n",
    "    3. 提供完整的训练接口\n",
    "    4. 处理训练过程中的异常\n",
    "    \n",
    "    训练流程：\n",
    "    1. 设置分词器\n",
    "    2. 加载策略模型\n",
    "    3. 准备训练数据\n",
    "    4. 创建奖励函数\n",
    "    5. 配置训练器\n",
    "    6. 执行训练\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProjectConfig = None, deepspeed_config: str = None):\n",
    "        \"\"\"\n",
    "        初始化GRPO训练器\n",
    "        \n",
    "        参数：\n",
    "            config: 项目配置实例\n",
    "            deepspeed_config: DeepSpeed配置文件路径\n",
    "        \"\"\"\n",
    "        self.config = config or ProjectConfig()          # 项目配置\n",
    "        self.deepspeed_config = deepspeed_config         # DeepSpeed配置路径\n",
    "        \n",
    "        # 创建输出目录\n",
    "        output_dir = self.config.get('output.output_dir')\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 创建日志器\n",
    "        self.logger = create_logger(output_dir, 'INFO')\n",
    "        \n",
    "        # 初始化组件引用\n",
    "        self.tokenizer = None        # 分词器\n",
    "        self.model = None           # 策略模型\n",
    "        self.dataset = None         # 训练数据集\n",
    "        self.reward_function = None # 奖励函数\n",
    "        self.trainer = None         # GRPO训练器\n",
    "    \n",
    "    def setup_tokenizer(self):\n",
    "        \"\"\"\n",
    "        设置分词器\n",
    "        \n",
    "        作用：\n",
    "        1. 加载预训练分词器\n",
    "        2. 配置特殊token\n",
    "        3. 设置填充策略\n",
    "        \n",
    "        分词器的重要性：\n",
    "        - 将文本转换为模型可理解的数字序列\n",
    "        - 处理特殊token（如开始、结束、填充token）\n",
    "        - 确保文本格式与模型训练时一致\n",
    "        \"\"\"\n",
    "        self.logger.info(\"设置分词器...\")\n",
    "        model_name = self.config.get('model.model_name')\n",
    "        \n",
    "        # 加载分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,                    # 模型名称或路径\n",
    "            trust_remote_code=True,        # 允许执行远程代码\n",
    "            padding_side=\"left\"            # 左侧填充（生成任务推荐）\n",
    "        )\n",
    "        \n",
    "        # 设置填充token\n",
    "        # 如果模型没有定义填充token，使用结束token代替\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.logger.info(\"分词器设置完成\")\n",
    "    \n",
    "    def setup_model(self):\n",
    "        \"\"\"\n",
    "        设置策略模型\n",
    "        \n",
    "        作用：\n",
    "        1. 加载预训练的基座模型\n",
    "        2. 可选地加载LoRA适配器\n",
    "        3. 配置模型参数\n",
    "        4. 确保与DeepSpeed兼容\n",
    "        \n",
    "        模型加载策略：\n",
    "        - 使用bfloat16精度节省显存\n",
    "        - 禁用缓存减少内存占用\n",
    "        - 让DeepSpeed处理模型分布\n",
    "        - 确保所有参数可训练\n",
    "        \"\"\"\n",
    "        self.logger.info(\"设置策略模型...\")\n",
    "        model_name = self.config.get('model.model_name')\n",
    "        \n",
    "        # 让DeepSpeed处理模型分布\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,                    # 模型名称或路径\n",
    "            torch_dtype=torch.bfloat16,    # 使用半精度浮点\n",
    "            trust_remote_code=True,        # 允许执行远程代码\n",
    "            use_cache=False,               # 禁用KV缓存节省内存\n",
    "            device_map=None,               # 关键：让DeepSpeed处理设备分配\n",
    "            low_cpu_mem_usage=True         # 低CPU内存使用模式\n",
    "        )\n",
    "        \n",
    "        # 加载LoRA模型（如果配置了）\n",
    "        # LoRA是一种参数高效的微调方法\n",
    "        lora_path = self.config.get('model.lora_model_path')\n",
    "        if lora_path:\n",
    "            self.model = PeftModel.from_pretrained(self.model, lora_path)\n",
    "        \n",
    "        # 确保参数可训练\n",
    "        # 某些情况下模型参数可能被冻结\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.logger.info(\"策略模型设置完成\")\n",
    "    \n",
    "    def setup_dataset(self):\n",
    "        \"\"\"\n",
    "        设置数据集\n",
    "        \n",
    "        作用：\n",
    "        1. 创建数据集加载器\n",
    "        2. 加载训练数据\n",
    "        3. 加载验证数据\n",
    "        4. 格式化为GRPO训练格式\n",
    "        \n",
    "        GRPO数据特点：\n",
    "        - 只需要prompt，不需要答案\n",
    "        - 使用对话格式\n",
    "        - 支持批量处理\n",
    "        \"\"\"\n",
    "        self.logger.info(\"设置数据集...\")\n",
    "        dataset_loader = GRPODatasetLoader(self.tokenizer, self.logger)\n",
    "        \n",
    "        # 加载训练数据\n",
    "        data_path = self.config.get('data.train_data_path')\n",
    "        self.dataset = dataset_loader.create_dataset(data_path)\n",
    "        \n",
    "        # 加载验证数据\n",
    "        eval_data_path = self.config.get('data.eval_data_path')\n",
    "        self.eval_dataset = dataset_loader.create_dataset(eval_data_path)\n",
    "        \n",
    "        self.logger.info(f\"数据集设置完成: {len(self.dataset)}条记录\")\n",
    "    \n",
    "    def setup_reward_function(self):\n",
    "        \"\"\"\n",
    "        设置奖励函数\n",
    "        \n",
    "        作用：\n",
    "        1. 加载预训练的奖励模型\n",
    "        2. 创建GRPO兼容的奖励函数\n",
    "        3. 确保文本格式一致性\n",
    "        \n",
    "        奖励函数的作用：\n",
    "        - 评估模型生成文本的质量\n",
    "        - 为GRPO提供训练信号\n",
    "        - 指导模型向更好的方向优化\n",
    "        \"\"\"\n",
    "        self.logger.info(\"设置奖励函数...\")\n",
    "        reward_model_path = self.config.get('model.reward_model_path')\n",
    "        self.reward_function = create_reward_function(reward_model_path, self.logger)\n",
    "        self.logger.info(\"奖励函数设置完成\")\n",
    "    \n",
    "    def setup_trainer(self):\n",
    "        \"\"\"\n",
    "        设置GRPO训练器\n",
    "        \n",
    "        作用：\n",
    "        1. 配置训练参数\n",
    "        2. 创建早停回调\n",
    "        3. 初始化GRPO训练器\n",
    "        4. 集成所有组件\n",
    "        \n",
    "        训练器配置包括：\n",
    "        - 基本训练参数（学习率、批次大小等）\n",
    "        - GRPO特有参数（生成数量、温度等）\n",
    "        - 保存和评估策略\n",
    "        - 日志和监控设置\n",
    "        \"\"\"\n",
    "        self.logger.info(\"设置GRPO训练器...\")\n",
    "        \n",
    "        # 创建最佳模型早停回调\n",
    "        best_model_dir = f\"{self.config.get('output.output_dir')}best_model\"\n",
    "        early_stopping_callback = GRPOBestModelEarlyStoppingCallback(\n",
    "            early_stopping_patience=self.config.get('training.early_stopping_patience', 3),\n",
    "            early_stopping_threshold=self.config.get('training.early_stopping_threshold', 0.001),\n",
    "            best_model_dir=best_model_dir,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # 配置训练参数\n",
    "        training_args = GRPOConfig(\n",
    "            # 基本训练参数\n",
    "            output_dir=self.config.get('output.output_dir'),                                    # 输出目录\n",
    "            per_device_train_batch_size=self.config.get('training.per_device_train_batch_size',2),  # 每设备训练批次大小\n",
    "            per_device_eval_batch_size=self.config.get('training.per_device_eval_batch_size', 4),   # 每设备验证批次大小\n",
    "            gradient_accumulation_steps=self.config.get('training.gradient_accumulation_steps'),    # 梯度累积步数\n",
    "            learning_rate=self.config.get('training.learning_rate'),                               # 学习率\n",
    "            num_train_epochs=self.config.get('training.num_train_epochs'),                         # 训练轮数\n",
    "            logging_steps=self.config.get('output.logging_steps'),                                 # 日志记录步数\n",
    "            \n",
    "            # GRPO特有参数\n",
    "            num_generations=self.config.get('grpo.num_generations', 8),    # 每个prompt生成的回复数量\n",
    "            temperature=self.config.get('grpo.temperature', 0.8),          # 生成温度（控制随机性）\n",
    "            top_p=self.config.get('grpo.top_p', 0.9),                     # 核采样参数\n",
    "            epsilon=self.config.get('grpo.epsilon', 0.2),                  # PPO裁剪参数\n",
    "            beta=self.config.get('grpo.beta', 0.0),                       # KL散度惩罚系数\n",
    "            \n",
    "            # 修复生成长度限制\n",
    "            max_completion_length=self.config.get('data.max_completion_length', 512),  # 最大生成长度\n",
    "            \n",
    "            # Checkpoint策略\n",
    "            save_strategy=\"steps\",                                          # 按步数保存\n",
    "            save_steps=self.config.get('output.save_steps', 500),         # 保存间隔\n",
    "            save_total_limit=self.config.get('output.save_total_limit', 3), # 最大保存数量\n",
    "            \n",
    "            # 评估策略 \n",
    "            eval_strategy=\"steps\",                                          # 按步数评估\n",
    "            eval_steps=self.config.get('output.eval_steps', 100),         # 评估间隔\n",
    "            \n",
    "            # 不使用传统的最佳模型加载（我们自己管理）\n",
    "            load_best_model_at_end=False,                                  # 训练结束时不加载最佳模型\n",
    "            metric_for_best_model=None,                                    # 不指定最佳模型指标\n",
    "            \n",
    "            # 其他参数\n",
    "            warmup_steps=self.config.get('training.warmup_steps', 50),     # 预热步数\n",
    "            max_grad_norm=self.config.get('training.max_grad_norm', 1.0),  # 梯度裁剪\n",
    "            dataloader_num_workers=self.config.get('training.dataloader_num_workers', 0),  # 数据加载线程数\n",
    "            bf16=True,                                                      # 使用bfloat16精度\n",
    "            remove_unused_columns=False,                                   # 保留所有数据列\n",
    "            deepspeed=self.deepspeed_config,                               # DeepSpeed配置\n",
    "            report_to=[\"tensorboard\"],                                     # 使用TensorBoard监控\n",
    "            logging_dir=os.path.join(self.config.get('output.output_dir'), \"logs\"),  # 日志目录\n",
    "        )\n",
    "        \n",
    "        # 创建GRPO训练器\n",
    "        self.trainer = GRPOTrainer(\n",
    "            model=self.model,                    # 策略模型\n",
    "            args=training_args,                  # 训练参数\n",
    "            train_dataset=self.dataset,          # 训练数据集\n",
    "            eval_dataset=self.eval_dataset,      # 验证数据集\n",
    "            processing_class=self.tokenizer,     # 文本处理器（分词器）\n",
    "            reward_funcs=[self.reward_function], # 奖励函数列表\n",
    "            callbacks=[early_stopping_callback] # 回调函数列表\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\"GRPO训练器设置完成\")\n",
    "    \n",
    "    def train(self, resume_from_checkpoint=None):\n",
    "        \"\"\"\n",
    "        执行GRPO训练\n",
    "        \n",
    "        参数：\n",
    "            resume_from_checkpoint: 从指定checkpoint恢复训练\n",
    "        \n",
    "        作用：\n",
    "        1. 按顺序初始化所有组件\n",
    "        2. 执行完整的训练流程\n",
    "        3. 保存最终模型\n",
    "        4. 处理训练过程中的异常\n",
    "        \n",
    "        训练流程：\n",
    "        1. 记录训练开始\n",
    "        2. 初始化各个组件\n",
    "        3. 执行GRPO训练\n",
    "        4. 保存模型和分词器\n",
    "        5. 记录训练结果\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 记录训练开始\n",
    "            self.logger.log_training_start(self.config)\n",
    "            \n",
    "            # 按顺序初始化所有组件\n",
    "            self.setup_tokenizer()      # 1. 设置分词器\n",
    "            self.setup_model()          # 2. 加载模型\n",
    "            self.setup_dataset()        # 3. 准备数据\n",
    "            self.setup_reward_function() # 4. 创建奖励函数\n",
    "            self.setup_trainer()        # 5. 配置训练器\n",
    "            \n",
    "            # 开始训练\n",
    "            self.logger.info(\"开始GRPO训练...\")\n",
    "            self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "            \n",
    "            # 保存最终模型\n",
    "            self.logger.info(\"保存模型...\")\n",
    "            self.trainer.save_model()  # 保存模型权重\n",
    "            self.tokenizer.save_pretrained(self.config.get('output.output_dir'))  # 保存分词器\n",
    "            \n",
    "            # 记录训练成功\n",
    "            self.logger.log_training_end(success=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 处理训练失败\n",
    "            self.logger.error(f\"训练失败: {str(e)}\")\n",
    "            self.logger.log_training_end(success=False)\n",
    "            raise  # 重新抛出异常\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：程序入口点\n",
    "    \n",
    "    作用：\n",
    "    1. 解析命令行参数\n",
    "    2. 加载配置\n",
    "    3. 创建训练器\n",
    "    4. 启动训练\n",
    "    \n",
    "    支持的命令行参数：\n",
    "    - --deepspeed: DeepSpeed配置文件路径\n",
    "    - --resume_from_checkpoint: 从checkpoint恢复训练\n",
    "    \"\"\"\n",
    "    # 解析命令行参数\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--deepspeed', type=str, help='DeepSpeed配置文件路径')\n",
    "    parser.add_argument('--resume_from_checkpoint', type=str, default=None, \n",
    "                       help='从checkpoint恢复训练')\n",
    "    args = parser.parse_known_args()[0]  # 只取已知参数\n",
    "    \n",
    "    # 加载配置和创建训练器\n",
    "    config = ProjectConfig()  # 加载项目配置\n",
    "    trainer = GRPOTraining(config, args.deepspeed)  # 创建训练器\n",
    "    \n",
    "    # 启动训练\n",
    "    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
    "\n",
    "\n",
    "# 程序入口点\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397bb0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea9b0c08",
   "metadata": {},
   "source": [
    "config.json    主配置文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b467f",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"model\": {\n",
    "    \"model_name\": \"/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028\",\n",
    "    \"reward_model_path\": \"/shared/Skywork-Reward_checkpoint-1000/checkpoint-1000\",\n",
    "    \"lora_model_path\": null\n",
    "  },\n",
    "  \"data\": {\n",
    "    \"train_data_path\": \"/shared/grpo_financial_tuning/data/grpo_prompts_dataset_5k.jsonl\",\n",
    "    \"eval_data_path\": \"/shared/grpo_financial_tuning/data/eval_prompts_dataset.jsonl\",\n",
    "    \"max_prompt_length\": 512,\n",
    "    \"max_completion_length\": 1024\n",
    "  },\n",
    "  \"training\": {\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"bf16\": true,\n",
    "    \"gradient_checkpointing\": false,\n",
    "    \"max_grad_norm\": 1.0\n",
    "  },\n",
    "  \"grpo\": {\n",
    "    \"num_generations\": 4,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"beta\": 0.0,\n",
    "    \"use_liger_loss\": true\n",
    "  },\n",
    "  \"output\": {\n",
    "    \"output_dir\": \"./output/\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"save_steps\": 10,\n",
    "    \"eval_steps\": 10,\n",
    "    \"load_best_model_at_end\": true,\n",
    "    \"save_total_limit\": 1\n",
    "  },\n",
    "  \"logging\": {\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"use_wandb\": false,\n",
    "    \"wandb_project\": \"grpo-financial-tuning\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76bf37",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"model\": {\n",
    "    // 基座模型路径 - 这是已经过SFT训练的DeepSeek-R1模型\n",
    "    // 作用：作为GRPO训练的起始点，提供基础的对话能力\n",
    "    \"model_name\": \"/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028\",\n",
    "    \n",
    "    // 奖励模型路径 - 用于评估生成文本质量的专门模型\n",
    "    // 作用：为GRPO训练提供奖励信号，指导模型优化方向\n",
    "    // 注意：必须与策略模型使用相同的文本格式（LLaMA-3格式）\n",
    "    \"reward_model_path\": \"/shared/Skywork-Reward_checkpoint-1000/checkpoint-1000\",\n",
    "    \n",
    "    // LoRA适配器路径 - 参数高效微调适配器\n",
    "    // null表示不使用LoRA，进行全参数微调\n",
    "    // 如果启用，可以大幅减少可训练参数数量（通常<1%）\n",
    "    \"lora_model_path\": null\n",
    "  },\n",
    "  \n",
    "  \"data\": {\n",
    "    // 训练数据路径 - GRPO训练的prompt数据集\n",
    "    // 格式：每行一个JSON，包含{\"prompt\": \"问题内容\"}\n",
    "    \"train_data_path\": \"/shared/grpo_financial_tuning/data/grpo_prompts_dataset_5k.jsonl\",\n",
    "    \n",
    "    // 验证数据路径 - 用于训练过程中的模型评估\n",
    "    // 作用：监控训练进度，触发最佳模型保存\n",
    "    \"eval_data_path\": \"/shared/grpo_financial_tuning/data/eval_prompts_dataset.jsonl\",\n",
    "    \n",
    "    // 最大输入长度 - prompt的最大token数\n",
    "    // 作用：控制输入序列长度，影响显存占用和训练速度\n",
    "    // 512对应约400-500个中文字符\n",
    "    \"max_prompt_length\": 512,\n",
    "    \n",
    "    // 最大生成长度 - 模型单次生成的最大token数\n",
    "    // 作用：控制输出序列长度，直接影响显存占用\n",
    "    // 1024对应约800-1000个中文字符\n",
    "    \"max_completion_length\": 1024\n",
    "  },\n",
    "  \n",
    "  \"training\": {\n",
    "    // 学习率 - 控制参数更新的步长\n",
    "    // 1e-6 = 0.000001，这是一个很小的学习率\n",
    "    // 原因：GRPO训练需要谨慎的参数更新，避免策略崩溃\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \n",
    "    // 训练轮数 - 完整遍历训练数据的次数\n",
    "    // 3轮对于强化学习训练通常足够，过多可能导致过拟合\n",
    "    \"num_train_epochs\": 3,\n",
    "    \n",
    "    // 每GPU训练批次大小 - 每个GPU同时处理的样本数\n",
    "    // 4是在显存限制下的平衡选择（7B模型+A800-80GB）\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \n",
    "    // 每GPU验证批次大小 - 验证时每个GPU处理的样本数\n",
    "    // 通常与训练批次大小相同或略大\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \n",
    "    // 梯度累积步数 - 多少步后进行一次参数更新\n",
    "    // 实际批次大小 = per_device_batch_size × gradient_accumulation_steps × GPU数量\n",
    "    // 这里：4 × 4 × 2 = 32的有效批次大小\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    // 预热步数 - 学习率从0逐渐增加到目标值的步数\n",
    "    // 10步预热有助于训练初期的稳定性\n",
    "    \"warmup_steps\": 10,\n",
    "    \n",
    "    // 混合精度训练 - 使用bfloat16减少显存占用\n",
    "    // true启用，可节省约50%显存，对7B模型几乎无精度损失\n",
    "    \"bf16\": true,\n",
    "    \n",
    "    // 梯度检查点 - 用时间换空间的内存优化技术\n",
    "    // false表示不启用，因为当前配置显存充足\n",
    "    \"gradient_checkpointing\": false,\n",
    "    \n",
    "    // 梯度裁剪阈值 - 防止梯度爆炸\n",
    "    // 1.0是常用值，当梯度范数超过此值时进行缩放\n",
    "    \"max_grad_norm\": 1.0\n",
    "  },\n",
    "  \n",
    "  \"grpo\": {\n",
    "    // 【核心参数】每个prompt生成的回复数量\n",
    "    // 4表示为每个问题生成4个不同的回答\n",
    "    // 作用：提供组内比较的样本，是GRPO算法的核心\n",
    "    // 影响：直接影响显存占用（4倍）和训练效果\n",
    "    \"num_generations\": 4,\n",
    "    \n",
    "    // 生成温度 - 控制文本生成的随机性\n",
    "    // 0.7是平衡值：0=确定性，1=高随机性\n",
    "    // 作用：确保4个生成样本有足够的多样性进行比较\n",
    "    \"temperature\": 0.7,\n",
    "    \n",
    "    // 核采样参数 - 控制候选token的范围\n",
    "    // 0.9表示从累积概率90%的token中采样\n",
    "    // 作用：与temperature配合控制生成质量和多样性\n",
    "    \"top_p\": 0.9,\n",
    "    \n",
    "    // PPO裁剪参数 - 控制策略更新的幅度\n",
    "    // 0.2是标准值，防止策略更新过大导致训练不稳定\n",
    "    \"epsilon\": 0.2,\n",
    "    \n",
    "    // KL散度惩罚系数 - 控制新策略与原策略的差异\n",
    "    // 0.0表示不使用KL惩罚，GRPO通常不需要此项\n",
    "    \"beta\": 0.0,\n",
    "    \n",
    "    // Liger优化 - 使用优化的损失函数实现\n",
    "    // true启用，可以提升训练速度和内存效率\n",
    "    \"use_liger_loss\": true\n",
    "  },\n",
    "  \n",
    "  \"output\": {\n",
    "    // 输出目录 - 模型、日志、检查点的保存位置\n",
    "    \"output_dir\": \"./output/\",\n",
    "    \n",
    "    // 日志记录频率 - 每多少步记录一次训练指标\n",
    "    // 1表示每步都记录，便于密切监控GRPO训练过程\n",
    "    \"logging_steps\": 1,\n",
    "    \n",
    "    // 模型保存频率 - 每多少步保存一次检查点\n",
    "    // 10步保存一次，平衡存储空间和恢复粒度\n",
    "    \"save_steps\": 10,\n",
    "    \n",
    "    // 验证频率 - 每多少步进行一次模型验证\n",
    "    // 10步验证一次，及时发现最佳模型\n",
    "    \"eval_steps\": 10,\n",
    "    \n",
    "    // 训练结束时是否加载最佳模型\n",
    "    // true表示自动加载验证效果最好的模型\n",
    "    \"load_best_model_at_end\": true,\n",
    "    \n",
    "    // 最大保存检查点数量 - 控制磁盘空间占用\n",
    "    // 1表示只保留最新的检查点，节省存储空间\n",
    "    \"save_total_limit\": 1\n",
    "  },\n",
    "  \n",
    "  \"logging\": {\n",
    "    // 日志级别 - 控制日志输出的详细程度\n",
    "    // INFO级别输出关键信息，不包含调试细节\n",
    "    \"log_level\": \"INFO\",\n",
    "    \n",
    "    // 是否使用Weights & Biases进行实验跟踪\n",
    "    // false表示不使用，避免外部依赖\n",
    "    \"use_wandb\": false,\n",
    "    \n",
    "    // W&B项目名称 - 如果启用wandb时使用\n",
    "    \"wandb_project\": \"grpo-financial-tuning\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162bc68",
   "metadata": {},
   "source": [
    "deepspeed_zero2.json  DeepSpeed分布式配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af04bf7",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"allgather_partitions\": true,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": true\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 10,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false,\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"  \n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969f3fce",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"zero_optimization\": {\n",
    "        // ZeRO优化阶段 - 控制内存优化的激进程度\n",
    "        // Stage 2：分片优化器状态 + 梯度，保留完整模型参数\n",
    "        // 选择原因：平衡内存节省和通信开销，适合多机训练\n",
    "        \"stage\": 2,\n",
    "        \n",
    "        \"offload_optimizer\": {\n",
    "            // 优化器状态卸载到CPU内存\n",
    "            // 作用：释放GPU显存，支持更大模型或批次\n",
    "            // 代价：增加CPU-GPU数据传输开销\n",
    "            \"device\": \"cpu\",\n",
    "            \n",
    "            // 使用固定内存 - 避免CPU内存分配延迟\n",
    "            // true可以提升CPU-GPU传输效率\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \n",
    "        // 启用allgather分区 - 优化参数聚合过程\n",
    "        // true可以减少通信次数，提升训练效率\n",
    "        \"allgather_partitions\": true,\n",
    "        \n",
    "        // allgather操作的缓冲区大小 - 2e8 = 200MB\n",
    "        // 作用：控制通信的批次大小，影响内存和带宽利用率\n",
    "        // 较大值减少通信次数但增加内存占用\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \n",
    "        // 重叠通信和计算 - 异步执行提升效率\n",
    "        // true表示在计算的同时进行参数通信\n",
    "        \"overlap_comm\": true,\n",
    "        \n",
    "        // 启用reduce_scatter优化 - 梯度聚合优化\n",
    "        // true可以提升多GPU间梯度同步效率\n",
    "        \"reduce_scatter\": true,\n",
    "        \n",
    "        // reduce_scatter的缓冲区大小 - 2e8 = 200MB\n",
    "        // 与allgather_bucket_size配合，平衡内存和通信\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \n",
    "        // 连续梯度存储 - 优化内存布局\n",
    "        // true可以提升梯度通信效率，减少内存碎片\n",
    "        \"contiguous_gradients\": true\n",
    "    },\n",
    "    \n",
    "    // 梯度累积步数 - 由训练配置自动设置\n",
    "    // \"auto\"表示从config.json的gradient_accumulation_steps读取\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \n",
    "    // 梯度裁剪 - 由训练配置自动设置\n",
    "    // \"auto\"表示从config.json的max_grad_norm读取\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \n",
    "    // 打印训练状态的频率 - 每10步打印一次\n",
    "    // 用于监控训练进度和性能指标\n",
    "    \"steps_per_print\": 10,\n",
    "    \n",
    "    // 全局批次大小 - 自动计算\n",
    "    // 实际值 = per_device_batch_size × gradient_accumulation_steps × world_size\n",
    "    // 这里：4 × 4 × 2 = 32\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \n",
    "    // 每GPU微批次大小 - 自动从配置读取\n",
    "    // 对应config.json中的per_device_train_batch_size\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \n",
    "    // 是否启用详细的时间分析 - 性能调试工具\n",
    "    // false表示不启用，避免额外开销\n",
    "    \"wall_clock_breakdown\": false,\n",
    "    \n",
    "    \"bf16\": {\n",
    "        // 启用bfloat16混合精度训练\n",
    "        // 与config.json中的bf16参数对应\n",
    "        // 作用：减少显存占用，提升训练速度\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \n",
    "    \"optimizer\": {\n",
    "        // 优化器类型 - AdamW是当前LLM训练的标准选择\n",
    "        // 相比Adam增加了权重衰减的正则化\n",
    "        \"type\": \"AdamW\",\n",
    "        \n",
    "        \"params\": {\n",
    "            // 学习率 - 从config.json自动读取\n",
    "            // 对应training.learning_rate = 1e-6\n",
    "            \"lr\": \"auto\",\n",
    "            \n",
    "            // Adam的momentum参数 - 自动使用默认值[0.9, 0.999]\n",
    "            // 控制梯度的指数移动平均\n",
    "            \"betas\": \"auto\",\n",
    "            \n",
    "            // 数值稳定性参数 - 自动使用默认值1e-8\n",
    "            // 防止除零错误\n",
    "            \"eps\": \"auto\",\n",
    "            \n",
    "            // 权重衰减系数 - 自动使用默认值0.01\n",
    "            // L2正则化，防止过拟合\n",
    "            \"weight_decay\": \"auto\"  \n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"scheduler\": {\n",
    "        // 学习率调度器类型 - 预热后线性衰减\n",
    "        // WarmupLR先增加后保持学习率\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \n",
    "        \"params\": {\n",
    "            // 预热最小学习率 - 通常为0\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \n",
    "            // 预热最大学习率 - 等于目标学习率1e-6\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \n",
    "            // 预热步数 - 从config.json读取warmup_steps = 10\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9039e",
   "metadata": {},
   "source": [
    "run_training.sh   训练启动脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df568bc3",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# ==================== GRPO训练脚本 ====================\n",
    "# 作用：支持nohup后台运行和多机多卡训练的启动脚本\n",
    "# 功能：环境配置、进程管理、分布式训练、错误处理\n",
    "\n",
    "# 设置严格模式：任何命令失败都会导致脚本退出\n",
    "set -e\n",
    "\n",
    "# ==================== 多机多卡配置 ====================\n",
    "# 定义集群节点配置：IP地址到GPU数量的映射\n",
    "declare -A NODES=(\n",
    "    [\"10.60.68.220\"]=1    # 主节点：拥有2张GPU卡\n",
    "    [\"10.60.98.173\"]=1    # 从节点：拥有1张GPU卡\n",
    ")\n",
    "MASTER_ADDR=\"10.60.68.220\"  # 主节点IP地址\n",
    "\n",
    "# ==================== 脚本参数解析 ====================\n",
    "# 初始化命令行参数变量\n",
    "USE_NOHUP=false        # 是否使用nohup后台运行\n",
    "STOP_TRAINING=false    # 是否停止训练\n",
    "CHECKPOINT_PATH=\"\"     # checkpoint恢复路径\n",
    "USE_MULTINODE=false    # 是否使用多机训练\n",
    "\n",
    "# 解析命令行参数\n",
    "while [[ $# -gt 0 ]]; do\n",
    "    case $1 in\n",
    "        --nohup) USE_NOHUP=true; shift ;;           # 启用后台运行\n",
    "        --stop) STOP_TRAINING=true; shift ;;        # 停止训练\n",
    "        --resume) CHECKPOINT_PATH=\"$2\"; shift 2 ;;  # 从checkpoint恢复\n",
    "        --multinode) USE_MULTINODE=true; shift ;;   # 启用多机训练\n",
    "        *) echo \"用法: $0 [--nohup] [--stop] [--resume <checkpoint_path>] [--multinode]\"; exit 1 ;;\n",
    "    esac\n",
    "done\n",
    "\n",
    "# ==================== 工具函数定义 ====================\n",
    "\n",
    "create_hostfile() {\n",
    "    \"\"\"\n",
    "    创建DeepSpeed hostfile\n",
    "    \n",
    "    作用：\n",
    "    1. 生成DeepSpeed分布式训练需要的主机文件\n",
    "    2. 指定每个节点的GPU数量\n",
    "    3. 支持多机多卡训练配置\n",
    "    \n",
    "    hostfile格式：\n",
    "    IP地址 slots=GPU数量\n",
    "    \"\"\"\n",
    "    echo \"创建DeepSpeed hostfile...\"\n",
    "    > ./hostfile  # 清空hostfile\n",
    "    \n",
    "    # 遍历所有节点配置\n",
    "    for node_ip in \"${!NODES[@]}\"; do\n",
    "        gpu_count=${NODES[$node_ip]}\n",
    "        echo \"$node_ip slots=$gpu_count\" >> ./hostfile\n",
    "    done\n",
    "    \n",
    "    echo \"Hostfile创建完成:\"\n",
    "    cat ./hostfile  # 显示hostfile内容\n",
    "}\n",
    "\n",
    "stop_all_training() {\n",
    "    \"\"\"\n",
    "    停止所有节点的训练进程\n",
    "    \n",
    "    作用：\n",
    "    1. 在所有节点上查找并终止训练进程\n",
    "    2. 支持单机和多机环境\n",
    "    3. 确保进程完全清理\n",
    "    \n",
    "    进程识别：\n",
    "    - 通过进程命令行参数识别训练进程\n",
    "    - 使用pkill命令批量终止进程\n",
    "    \"\"\"\n",
    "    echo \"停止所有节点的训练进程...\"\n",
    "    \n",
    "    for node_ip in \"${!NODES[@]}\"; do\n",
    "        if [[ \"$node_ip\" == \"$MASTER_ADDR\" ]]; then\n",
    "            # 在主节点上直接执行\n",
    "            pkill -f \"deepspeed.*train.py\" 2>/dev/null || true\n",
    "        else\n",
    "            # 通过SSH在从节点上执行\n",
    "            ssh ubuntu@$node_ip \"pkill -f 'deepspeed.*train.py' 2>/dev/null || true\" &\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    wait    # 等待所有后台任务完成\n",
    "    sleep 3 # 等待进程完全退出\n",
    "    echo \"所有节点训练进程清理完成\"\n",
    "}\n",
    "\n",
    "# ==================== 停止训练功能 ====================\n",
    "if [ \"$STOP_TRAINING\" = true ]; then\n",
    "    if [ \"$USE_MULTINODE\" = true ]; then\n",
    "        # 多机环境：停止所有节点\n",
    "        stop_all_training\n",
    "    else\n",
    "        # 单机环境：只停止本地进程\n",
    "        echo \"正在停止GRPO训练进程...\"\n",
    "        TRAIN_PIDS=$(ps aux | grep \"deepspeed.*train.py\" | grep -v grep | awk '{print $2}')\n",
    "        \n",
    "        if [ -z \"$TRAIN_PIDS\" ]; then\n",
    "            echo \"未找到运行中的训练进程\"\n",
    "        else\n",
    "            echo \"停止进程: $TRAIN_PIDS\"\n",
    "            for pid in $TRAIN_PIDS; do\n",
    "                kill -TERM $pid  # 发送终止信号\n",
    "            done\n",
    "            echo \"训练进程已停止\"\n",
    "        fi\n",
    "    fi\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "# ==================== 环境准备 ====================\n",
    "\n",
    "# 切换到脚本所在目录\n",
    "cd \"$(dirname \"$0\")\"\n",
    "    \n",
    "# 创建日志目录（带时间戳）\n",
    "LOG_DIR=\"./logs/$(date +%Y%m%d_%H%M%S)\"\n",
    "mkdir -p \"$LOG_DIR\"\n",
    "\n",
    "# ==================== 环境变量设置 ====================\n",
    "# GPU和CUDA配置\n",
    "export CUDA_VISIBLE_DEVICES=0,1        # 指定可见的GPU设备\n",
    "export NCCL_DEBUG=WARN                 # NCCL调试级别\n",
    "export NCCL_IB_DISABLE=1               # 禁用InfiniBand\n",
    "export NCCL_IGNORE_DISABLED_P2P=1      # 忽略P2P通信问题\n",
    "\n",
    "# Python环境配置\n",
    "export PYTHONPATH=$PYTHONPATH:$(pwd)   # 添加当前目录到Python路径\n",
    "export PDSH_RCMD_TYPE=ssh              # 设置远程命令执行方式\n",
    "\n",
    "# ==================== NCCL优化配置 ====================\n",
    "# NCCL（NVIDIA Collective Communication Library）是GPU间通信库\n",
    "# 这些配置用于解决多机多卡训练中的通信问题\n",
    "export NCCL_BUFFSIZE=524288             # 强制使用524288字节缓冲区\n",
    "export NCCL_MAX_NCHANNELS=1             # 限制最大通道数量\n",
    "export NCCL_MIN_NCHANNELS=1             # 限制最小通道数量\n",
    "export NCCL_PROTO=Simple                # 使用简单通信协议\n",
    "export NCCL_ALGO=Ring                   # 使用Ring通信算法\n",
    "export NCCL_TREE_THRESHOLD=0            # 禁用树形通信\n",
    "export NCCL_NET_GDR_LEVEL=0             # 禁用GPU Direct RDMA\n",
    "export NCCL_SOCKET_IFNAME=eth0          # 强制使用eth0网卡\n",
    "\n",
    "# 创建输出目录\n",
    "mkdir -p \"./output/\"\n",
    "\n",
    "# ==================== 训练命令构建 ====================\n",
    "\n",
    "build_train_command() {\n",
    "    \"\"\"\n",
    "    构建训练命令\n",
    "    \n",
    "    参数：无（使用全局变量）\n",
    "    \n",
    "    返回：完整的训练命令字符串\n",
    "    \n",
    "    作用：\n",
    "    1. 根据运行模式构建不同的命令\n",
    "    2. 支持单机和多机训练\n",
    "    3. 处理checkpoint恢复\n",
    "    4. 动态配置参数\n",
    "    \"\"\"\n",
    "    local cmd\n",
    "    local current_dir=$(pwd)\n",
    "    \n",
    "    if [ \"$USE_MULTINODE\" = true ]; then\n",
    "        # 多机训练命令\n",
    "        cmd=\"cd $current_dir && deepspeed --hostfile=hostfile $current_dir/train.py --deepspeed $current_dir/configs/deepspeed_zero2.json\"\n",
    "    else\n",
    "        # 单机训练命令\n",
    "        cmd=\"deepspeed --num_gpus=2 train.py --deepspeed configs/deepspeed_zero2.json\"\n",
    "    fi\n",
    "    \n",
    "    # 添加checkpoint恢复参数\n",
    "    if [ -n \"$CHECKPOINT_PATH\" ]; then\n",
    "        cmd=\"$cmd --resume_from_checkpoint $CHECKPOINT_PATH\"\n",
    "        echo \"从checkpoint恢复训练: $CHECKPOINT_PATH\" >&2\n",
    "    fi\n",
    "    \n",
    "    echo \"$cmd\"\n",
    "}\n",
    "\n",
    "# ==================== 训练执行函数 ====================\n",
    "\n",
    "run_training() {\n",
    "    \"\"\"\n",
    "    执行训练\n",
    "    \n",
    "    作用：\n",
    "    1. 激活Python环境\n",
    "    2. 准备多机环境（如需要）\n",
    "    3. 执行训练命令\n",
    "    4. 处理训练结果\n",
    "    \n",
    "    返回：0表示成功，1表示失败\n",
    "    \"\"\"\n",
    "    echo \"开始GRPO训练 - $(date)\"\n",
    "\n",
    "    # 激活conda环境\n",
    "    source ~/miniconda3/etc/profile.d/conda.sh\n",
    "    conda activate reward3 \n",
    "    \n",
    "    # 多机模式需要先准备环境\n",
    "    if [ \"$USE_MULTINODE\" = true ]; then\n",
    "        stop_all_training  # 清理旧进程\n",
    "        create_hostfile    # 创建hostfile\n",
    "    fi\n",
    "    \n",
    "    # 构建并执行训练命令\n",
    "    local train_cmd=$(build_train_command)\n",
    "    if eval \"$train_cmd\"; then\n",
    "        echo \"训练成功完成 - $(date)\"\n",
    "        return 0\n",
    "    else\n",
    "        echo \"训练失败 - $(date)\"\n",
    "        return 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# ==================== 主执行逻辑 ====================\n",
    "\n",
    "if [ \"$USE_NOHUP\" = true ]; then\n",
    "    # ==================== 后台运行模式 ====================\n",
    "    echo \"后台运行训练，日志: $LOG_DIR/training.log\"\n",
    "    echo \"查看进度: tail -f $LOG_DIR/training.log\"\n",
    "    echo \"停止训练: pkill -f 'deepspeed.*train.py'\"\n",
    "    \n",
    "    # 多机模式需要先准备环境\n",
    "    if [ \"$USE_MULTINODE\" = true ]; then\n",
    "        stop_all_training\n",
    "        create_hostfile\n",
    "    fi\n",
    "    \n",
    "    # 构建完整的后台命令\n",
    "    train_cmd=$(build_train_command)\n",
    "    nohup bash -c \"source ~/miniconda3/etc/profile.d/conda.sh && conda activate reward3 && $train_cmd\" > \"$LOG_DIR/training.log\" 2>&1 &\n",
    "    \n",
    "    # 记录进程ID\n",
    "    TRAIN_PID=$!\n",
    "    echo \"训练进程已启动 (PID: $TRAIN_PID)\"\n",
    "    echo $TRAIN_PID > \"$LOG_DIR/train.pid\"\n",
    "else\n",
    "    # ==================== 前台运行模式 ====================\n",
    "    # 重定向输出到日志文件同时显示在控制台\n",
    "    exec > >(tee -a \"$LOG_DIR/training.log\")\n",
    "    exec 2>&1\n",
    "    run_training\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb91cbb",
   "metadata": {},
   "source": [
    "执行训练命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e833c",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 后端执行多机多卡训练\n",
    "./run_training.sh --nohup --multinode\n",
    "# 从checkpoint节点继续训练\n",
    "./run_training.sh --nohup --multinode --resume ./output/checkpoint-40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709143027262.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38dd02",
   "metadata": {},
   "source": [
    "**GRPO工作原理**：\n",
    "1. 给模型一个金融问题\n",
    "2. 模型生成4个不同的回答\n",
    "3. 奖励模型给这4个回答打分\n",
    "4. 选择高分回答，惩罚低分回答\n",
    "5. 调整模型参数，让它更倾向于生成高分回答\n",
    "\n",
    "### 参数波动的实际影响\n",
    "\n",
    "#### 1. **Reward波动：18.80→20.89→16.69→19.34**\n",
    "\n",
    "**具体影响**：\n",
    "- **20.89时**：模型生成的金融回答质量很高，专业性强，逻辑清晰\n",
    "- **16.69时**：模型生成的回答质量下降，可能出现：\n",
    "  - 回答不够专业\n",
    "  - 逻辑混乱\n",
    "  - 偏离金融主题\n",
    "- **19.34时**：质量回升，模型重新找到了生成好回答的方向\n",
    "\n",
    "**为什么会波动**？\n",
    "- 这是强化学习的正常现象\n",
    "- 模型在\"探索\"（尝试新的回答方式）和\"利用\"（使用已知的好方法）之间平衡\n",
    "- 就像学生做题，有时会尝试新方法导致暂时成绩下降\n",
    "\n",
    "#### 2. **Reward_std(Reward标准差)波动：2.08→3.21→2.76**\n",
    "\n",
    "**具体影响**：\n",
    "- **2.08时**：4个回答的质量很接近，都比较稳定\n",
    "  - 回答1：19分，回答2：18分，回答3：19分，回答4：18分\n",
    "- **3.21时**：4个回答质量差异很大，不稳定\n",
    "  - 回答1：22分，回答2：15分，回答3：20分，回答4：16分\n",
    "- **2.76时**：质量差异缩小，趋于稳定\n",
    "\n",
    "**实际意义**：\n",
    "- 低std = 模型回答质量稳定，用户体验一致\n",
    "- 高std = 模型回答质量不稳定，有时好有时差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90802f",
   "metadata": {},
   "source": [
    "#### 3. **Loss(损失函数)波动：-0.0231→0.0336→-0.0255**\n",
    "\n",
    "GRPO的策略损失，负值表示策略改进方向\n",
    "\n",
    "**具体影响**：\n",
    "- **负值（-0.0231）**：模型正在朝着正确方向学习\n",
    "  - 就像学生答题正确率在提高\n",
    "- **正值（0.0336）**：模型暂时\"迷失\"了方向\n",
    "  - 就像学生暂时做错了题\n",
    "- **负值（-0.0255）**：模型重新找到正确方向\n",
    "\n",
    "#### 4. **Grad_norm(梯度范数)波动：0.76→0.84→0.72**\n",
    "\n",
    "**具体影响**：\n",
    "- **0.76**：模型参数调整幅度适中，学习稳定\n",
    "- **0.84**：参数调整幅度稍大，学习激进\n",
    "- **0.72**：参数调整幅度较小，学习保守\n",
    "\n",
    "**类比理解**：\n",
    "- 就像调音师调钢琴，数值越大调整幅度越大\n",
    "- 太大 = 可能调过头\n",
    "- 太小 = 调整太慢\n",
    "- 0.7-0.8 = 调整幅度刚好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba291f6",
   "metadata": {},
   "source": [
    "\n",
    "### 实际用户体验对比\n",
    "\n",
    "#### **Step 3（表现最好）：reward=20.89, reward_std=2.62**\n",
    "用户问：\"如何分析一只股票的投资价值？\"\n",
    "\n",
    "模型回答质量：\n",
    "- 回答专业性强\n",
    "- 逻辑清晰\n",
    "- 4个回答都比较好，质量接近\n",
    "\n",
    "#### **Step 8（表现最差）：reward=16.69, reward_std=3.02**\n",
    "同样的问题，模型回答质量：\n",
    "- 回答可能偏离主题\n",
    "- 专业术语使用不当\n",
    "- 4个回答质量差异很大，有好有坏\n",
    "\n",
    "#### **Step 11（回升）：reward=19.34, reward_std=2.76**\n",
    "模型回答质量：\n",
    "- 专业性恢复\n",
    "- 逻辑性改善\n",
    "- 质量趋于稳定\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea823e94",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250706210948443.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7609e23",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250706185011783.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e3604",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709161020433.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1cd8c",
   "metadata": {},
   "source": [
    "tensorboard 启动命令\n",
    "\n",
    "tensorboard --logdir=./output/logs/ --port=6006 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee126c83",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709144323080.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f574d9b",
   "metadata": {},
   "source": [
    "访问对应服务器外部访问链接地址 http://服务器外部IP:6006/#timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8729d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709144507491.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895d093",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709144521529.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e15c2c",
   "metadata": {},
   "source": [
    "# 6. 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd876d",
   "metadata": {},
   "source": [
    "test.py   模型测试脚本单节点最简单测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fb253",
   "metadata": {},
   "source": [
    "```python\n",
    "# ==================== 模型测试脚本 ====================\n",
    "# 作用：加载训练好的GRPO模型并进行推理测试\n",
    "# 用途：验证模型是否正常工作，测试模型的推理能力\n",
    "\n",
    "import torch                                    # PyTorch深度学习框架\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Hugging Face模型和分词器\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    测试已训练的GRPO模型\n",
    "    \n",
    "    功能：\n",
    "    1. 加载保存的模型和分词器\n",
    "    2. 使用金融领域的复杂问题进行推理测试\n",
    "    3. 验证模型的文本生成能力\n",
    "    \"\"\"\n",
    "    # 设置模型路径 - 可以是本地路径或远程模型路径\n",
    "    model_path = \"./output/best_model\"  # 默认使用本地最佳模型\n",
    "    # model_path = \"/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028\"  # 备用路径\n",
    "    \n",
    "    print(\"加载分词器...\")\n",
    "    # 加载分词器 - 负责将文本转换为模型可理解的数字序列\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,                    # 模型路径\n",
    "        trust_remote_code=True         # 允许执行远程代码（某些模型需要）\n",
    "    )\n",
    "    print(\"分词器加载成功\")\n",
    "    \n",
    "    print(\"加载模型...\")\n",
    "    # 加载语言模型 - 用于文本生成\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,                    # 模型路径\n",
    "        torch_dtype=torch.bfloat16,    # 使用bfloat16精度节省显存\n",
    "        device_map=\"auto\",             # 自动分配设备（GPU/CPU）\n",
    "        trust_remote_code=True         # 允许执行远程代码\n",
    "    )\n",
    "    print(\"模型加载成功\")\n",
    "    \n",
    "    print(\"测试推理...\")\n",
    "    # 构造测试用的金融问题 - 这是一个复杂的金融分析问题\n",
    "    prompt = \"你是一个专业的金融领域分析师请对以下问题进行详细解答：在周二的交易中，First Trust纳斯达克网络安全ETF(CIBR)表现优于其他ETF，当日上涨约1.3%。该ETF中表现尤为强劲的成分股包括Sentinelone(上涨约5.3%)和Okta(上涨约3.8%)。与此同时，Invesco太阳能ETF(TAN)表现逊于其他ETF，周二下午交易时段下跌约3.7%。该ETF中表现最弱的成分股包括Maxeon Solar Technologies(下跌约7.8%)和Solaredge Technologies(下跌约6.7%)。基于这些市场表现数据，请分析：1) 网络安全和太阳能行业ETF表现差异可能反映出的宏观经济或行业特定因素；2) 成分股价格变动与ETF整体表现之间的传导机制；3) 这种行业间表现差异可能对投资者资产配置策略产生的影响；4) 如何利用ETF成分股的价格离散度来构建潜在的投资组合策略。请详细说明你的分析框架和逻辑推理过程。\"\n",
    "    \n",
    "    # 将文本转换为模型输入格式\n",
    "    inputs = tokenizer.encode(\n",
    "        prompt,                        # 输入文本\n",
    "        return_tensors=\"pt\"           # 返回PyTorch张量格式\n",
    "    ).to(model.device)                # 移动到模型所在设备\n",
    "    \n",
    "    # 使用模型生成回复\n",
    "    with torch.no_grad():             # 禁用梯度计算，节省内存\n",
    "        outputs = model.generate(\n",
    "            inputs,                    # 输入序列\n",
    "            max_new_tokens=2048,       # 最大生成长度\n",
    "            temperature=0.7,           # 控制生成的随机性（0-1，越高越随机）\n",
    "            do_sample=True,            # 启用采样生成\n",
    "            pad_token_id=tokenizer.eos_token_id  # 设置填充token\n",
    "        )\n",
    "    \n",
    "    # 将生成的数字序列转换回文本\n",
    "    result = tokenizer.decode(\n",
    "        outputs[0],                   # 取第一个生成结果\n",
    "        skip_special_tokens=True      # 跳过特殊标记\n",
    "    )\n",
    "    \n",
    "    print(\"推理成功\")\n",
    "    print(\"结果:\", result)\n",
    "    print(\"测试完成，模型正常工作\")\n",
    "\n",
    "# 程序入口点\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()  # 执行模型测试\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b44eb",
   "metadata": {},
   "source": [
    "执行命令 python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c1194",
   "metadata": {},
   "source": [
    "GRPO微调后模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704000439137.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca270c9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704000503767.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4b811",
   "metadata": {},
   "source": [
    "GRPO微调前"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad0fdf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704000631336.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58193c5b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250704000656072.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16f7a6",
   "metadata": {},
   "source": [
    "multigpu_evaluator.py   多GPU评估工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117903b",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "多机多卡奖励模型评分工具 - 使用DeepSpeed\n",
    "这个工具用于在多个GPU上同时评测GRPO训练后的模型质量\n",
    "主要功能：给定一些问题，让模型生成回答，然后用奖励模型给回答打分\n",
    "\"\"\"\n",
    "import torch\n",
    "import json\n",
    "import deepspeed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Prompt数据集类\n",
    "    作用：将输入的问题列表转换为PyTorch可以处理的数据集格式\n",
    "    每个问题会复制num_generations次，因为每个问题要生成多个回答\n",
    "    \"\"\"\n",
    "    def __init__(self, prompts: list, num_generations: int = 4):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        prompts: 问题列表，例如[\"如何分析股票？\", \"什么是基金？\"]\n",
    "        num_generations: 每个问题生成几个回答，默认4个\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        # 将每个问题复制num_generations次\n",
    "        # 例如：1个问题 × 4次生成 = 4个数据项\n",
    "        for prompt in prompts:\n",
    "            for _ in range(num_generations):\n",
    "                self.data.append(prompt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集总大小\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"根据索引返回对应的问题\"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "class MultiGPURewardEvaluator:\n",
    "    \"\"\"\n",
    "    多GPU奖励模型评分器\n",
    "    作用：在多个GPU上同时运行模型评测，提高评测效率\n",
    "    包含两个模型：主模型（生成回答）和奖励模型（给回答打分）\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str, reward_model_path: str):\n",
    "        \"\"\"\n",
    "        初始化多GPU评分器\n",
    "        model_path: 主模型路径（GRPO训练后的模型）\n",
    "        reward_model_path: 奖励模型路径（用于给回答打分）\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.reward_model_path = reward_model_path\n",
    "        \n",
    "        # 初始化DeepSpeed分布式环境\n",
    "        # DeepSpeed是微软开发的深度学习优化库，支持多GPU训练和推理\n",
    "        deepspeed.init_distributed()\n",
    "        \n",
    "        # 获取当前进程的GPU编号和总GPU数量\n",
    "        # LOCAL_RANK: 当前进程在本机上的GPU编号（0, 1, 2...）\n",
    "        # WORLD_SIZE: 总的GPU数量\n",
    "        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "        self.world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "        \n",
    "        # 设置当前进程使用的GPU设备\n",
    "        torch.cuda.set_device(self.local_rank)\n",
    "        self.device = torch.device(f'cuda:{self.local_rank}')\n",
    "        \n",
    "        # 加载和配置模型\n",
    "        self.setup_models()\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"\n",
    "        设置和加载模型\n",
    "        作用：加载主模型和奖励模型，并使用DeepSpeed进行优化\n",
    "        \"\"\"\n",
    "        # 只在主进程（GPU 0）上打印信息，避免重复输出\n",
    "        if self.local_rank == 0:\n",
    "            print(f\"加载模型: {self.model_path}\")\n",
    "        \n",
    "        # 加载主模型的分词器\n",
    "        # 分词器负责将文本转换为模型可以理解的数字序列\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        # 如果分词器没有padding token，使用结束token作为padding\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 加载主模型（用于生成回答）\n",
    "        # torch_dtype=torch.bfloat16: 使用半精度浮点数，节省显存\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path, torch_dtype=torch.bfloat16, trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 使用DeepSpeed初始化主模型\n",
    "        # DeepSpeed会自动处理模型的分布式部署和内存优化\n",
    "        self.model, _, _, _ = deepspeed.initialize(\n",
    "            model=model,\n",
    "            config={\n",
    "                \"train_batch_size\": 2,                    # 训练批次大小\n",
    "                \"train_micro_batch_size_per_gpu\": 1,      # 每个GPU的微批次大小\n",
    "                \"gradient_accumulation_steps\": 1,          # 梯度累积步数\n",
    "                \"bf16\": {\"enabled\": True},                 # 启用bfloat16精度\n",
    "                \"zero_optimization\": {\"stage\": 0}          # ZeRO优化级别0（不分割参数）\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 加载奖励模型 - 参考dataset.py的实现方式\n",
    "        if self.local_rank == 0:\n",
    "            print(f\"加载奖励模型: {self.reward_model_path}\")\n",
    "        \n",
    "        # 加载奖励模型的分词器\n",
    "        self.reward_tokenizer = AutoTokenizer.from_pretrained(self.reward_model_path, trust_remote_code=True)\n",
    "        if self.reward_tokenizer.pad_token is None:\n",
    "            self.reward_tokenizer.pad_token = self.reward_tokenizer.eos_token\n",
    "        \n",
    "        # 加载奖励模型（用于给回答打分）\n",
    "        # num_labels=1: 输出一个分数值\n",
    "        # use_cache=False: 不使用缓存，节省内存\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.reward_model_path, num_labels=1, torch_dtype=torch.bfloat16, \n",
    "            trust_remote_code=True, use_cache=False\n",
    "        )\n",
    "        \n",
    "        # 使用DeepSpeed初始化奖励模型\n",
    "        self.reward_model, _, _, _ = deepspeed.initialize(\n",
    "            model=reward_model,\n",
    "            config={\n",
    "                \"train_batch_size\": 2,\n",
    "                \"train_micro_batch_size_per_gpu\": 1,\n",
    "                \"gradient_accumulation_steps\": 1,\n",
    "                \"bf16\": {\"enabled\": True},\n",
    "                \"zero_optimization\": {\"stage\": 0}\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        生成回复\n",
    "        作用：给定一个问题，使用主模型生成一个回答\n",
    "        prompt: 输入的问题文本\n",
    "        返回：模型生成的回答文本\n",
    "        \"\"\"\n",
    "        # 将问题格式化为对话格式\n",
    "        # 这里使用系统提示词让模型扮演金融分析师角色\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个专业的金融分析师，擅长投资分析和风险评估。请基于提供的信息进行详细的分析和推理。\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        # 使用分词器的聊天模板功能格式化对话\n",
    "        # add_generation_prompt=True: 添加生成提示符\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # 将文本转换为模型输入格式\n",
    "        # truncation=True: 如果文本太长就截断\n",
    "        # max_length=512: 最大输入长度512个token\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # 将输入数据移动到当前GPU上\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 生成回答\n",
    "        with torch.no_grad():  # 不计算梯度，节省内存\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=512,                        # 最多生成512个新token\n",
    "                temperature=0.7,                           # 温度参数，控制生成的随机性\n",
    "                top_p=0.9,                                # 核采样参数\n",
    "                do_sample=True,                           # 启用采样生成\n",
    "                pad_token_id=self.tokenizer.eos_token_id  # 填充token ID\n",
    "            )\n",
    "        \n",
    "        # 解码生成的回答\n",
    "        # 只取新生成的部分，跳过原始输入\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "        return response\n",
    "    \n",
    "    def compute_reward(self, prompt: str, completion: str) -> float:\n",
    "        \"\"\"\n",
    "        计算奖励分数\n",
    "        作用：使用奖励模型给问题-回答对打分\n",
    "        prompt: 问题文本\n",
    "        completion: 回答文本\n",
    "        返回：奖励分数（浮点数）\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 使用与奖励模型训练完全相同的LLaMA-3格式\n",
    "            # 这个格式化非常重要，必须与训练时保持一致\n",
    "            formatted_text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{completion}<|eot_id|>\"\n",
    "            \n",
    "            # 将格式化的文本转换为模型输入\n",
    "            inputs = self.reward_tokenizer(\n",
    "                formatted_text, \n",
    "                truncation=True,      # 截断过长文本\n",
    "                padding=True,         # 填充到相同长度\n",
    "                max_length=2048,      # 最大长度2048\n",
    "                return_tensors=\"pt\"   # 返回PyTorch张量\n",
    "            )\n",
    "            \n",
    "            # 将输入移动到当前GPU\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # 计算奖励分数\n",
    "            with torch.no_grad():\n",
    "                outputs = self.reward_model(**inputs)\n",
    "                # 提取分数并转换为Python数值\n",
    "                reward = outputs.logits.squeeze().cpu().item()\n",
    "            \n",
    "            return reward\n",
    "        except Exception as e:\n",
    "            # 如果计算失败，只在主进程打印错误信息\n",
    "            if self.local_rank == 0:\n",
    "                print(f\"奖励计算失败: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_batch(self, prompts: list) -> list:\n",
    "        \"\"\"\n",
    "        批量评测\n",
    "        作用：对一批问题进行评测，每个GPU处理不同的问题\n",
    "        prompts: 问题列表\n",
    "        返回：评测结果列表\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        # 每个问题会生成4个回答\n",
    "        dataset = PromptDataset(prompts, num_generations=4)\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        # 分布式处理：每个GPU处理不同的样本\n",
    "        for i, batch_prompts in enumerate(tqdm(dataloader, desc=f\"GPU-{self.local_rank}\", disable=self.local_rank!=0)):\n",
    "            prompt = batch_prompts[0]\n",
    "            \n",
    "            # 工作分配：第i个样本由第(i % world_size)个GPU处理\n",
    "            # 例如：GPU0处理样本0,3,6..., GPU1处理样本1,4,7...\n",
    "            if i % self.world_size != self.local_rank:\n",
    "                continue\n",
    "            \n",
    "            # 生成回复和计算奖励\n",
    "            response = self.generate_response(prompt)\n",
    "            reward = self.compute_reward(prompt, response)\n",
    "            \n",
    "            # 保存结果\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'response': response,\n",
    "                'reward': reward,\n",
    "                'gpu_id': self.local_rank\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def gather_results(self, local_results: list) -> list:\n",
    "        \"\"\"\n",
    "        收集所有GPU的结果\n",
    "        作用：将各个GPU的评测结果汇总，计算每个问题的平均分数\n",
    "        local_results: 当前GPU的评测结果\n",
    "        返回：汇总后的最终结果（只在主进程返回）\n",
    "        \"\"\"\n",
    "        # 简单的结果收集 - 在实际使用中可以用torch.distributed.all_gather\n",
    "        # 这里为了简化，直接使用本地结果\n",
    "        all_results = local_results\n",
    "        \n",
    "        # 只在主进程（GPU 0）进行结果汇总\n",
    "        if self.local_rank == 0:\n",
    "            # 按问题分组\n",
    "            prompt_groups = {}\n",
    "            for result in all_results:\n",
    "                prompt = result['prompt']\n",
    "                if prompt not in prompt_groups:\n",
    "                    prompt_groups[prompt] = []\n",
    "                prompt_groups[prompt].append(result)\n",
    "            \n",
    "            # 计算每个问题的统计信息\n",
    "            final_results = []\n",
    "            for prompt, group in prompt_groups.items():\n",
    "                # 提取所有回答的奖励分数\n",
    "                rewards = [r['reward'] for r in group]\n",
    "                responses = [r['response'] for r in group]\n",
    "                \n",
    "                # 计算统计指标\n",
    "                final_results.append({\n",
    "                    'prompt': prompt,\n",
    "                    'responses': responses,           # 所有生成的回答\n",
    "                    'rewards': rewards,               # 所有奖励分数\n",
    "                    'mean_reward': sum(rewards) / len(rewards),  # 平均奖励\n",
    "                    'max_reward': max(rewards),       # 最高奖励\n",
    "                    'min_reward': min(rewards)        # 最低奖励\n",
    "                })\n",
    "            \n",
    "            return final_results\n",
    "        \n",
    "        return []\n",
    "\n",
    "def load_prompts_from_jsonl(file_path: str, sample_size: int = None) -> list:\n",
    "    \"\"\"\n",
    "    从JSONL文件加载问题\n",
    "    作用：读取训练数据文件，提取其中的问题部分\n",
    "    file_path: JSONL文件路径\n",
    "    sample_size: 采样数量，如果为None则加载全部\n",
    "    返回：问题列表\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data = json.loads(line)\n",
    "                # 处理两种数据格式\n",
    "                if isinstance(data.get('prompt'), list):\n",
    "                    # 如果prompt是消息列表格式，提取用户消息\n",
    "                    user_msg = next(msg['content'] for msg in data['prompt'] if msg['role'] == 'user')\n",
    "                    prompts.append(user_msg)\n",
    "                else:\n",
    "                    # 如果prompt是字符串格式，直接使用\n",
    "                    prompts.append(data['prompt'])\n",
    "    \n",
    "    # 如果指定了采样数量，只取前sample_size个\n",
    "    if sample_size:\n",
    "        prompts = prompts[:sample_size]\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    作用：解析命令行参数，运行多GPU评测流程\n",
    "    \"\"\"\n",
    "    # 设置命令行参数\n",
    "    parser = argparse.ArgumentParser(description=\"多机多卡奖励模型评分工具\")\n",
    "    parser.add_argument('--model', type=str, required=True, help='模型路径')\n",
    "    parser.add_argument('--reward_model', type=str, required=True, help='奖励模型路径')\n",
    "    parser.add_argument('--prompts', type=str, help='prompts文件路径')\n",
    "    parser.add_argument('--sample_size', type=int, default=20, help='评测样本数量')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='本地rank')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 创建多GPU评分器\n",
    "    evaluator = MultiGPURewardEvaluator(args.model, args.reward_model)\n",
    "    \n",
    "    # 加载问题数据\n",
    "    if args.prompts:\n",
    "        # 从文件加载问题\n",
    "        prompts = load_prompts_from_jsonl(args.prompts, args.sample_size)\n",
    "    else:\n",
    "        # 使用默认问题\n",
    "        prompts = [\"请分析一下当前股市的投资机会和风险。\"] * 5\n",
    "    \n",
    "    # 只在主进程打印开始信息\n",
    "    if evaluator.local_rank == 0:\n",
    "        print(f\"开始多GPU评测 {len(prompts)} 个prompts...\")\n",
    "    \n",
    "    # 运行评测流程\n",
    "    local_results = evaluator.evaluate_batch(prompts)      # 各GPU并行评测\n",
    "    final_results = evaluator.gather_results(local_results)  # 汇总结果\n",
    "    \n",
    "    # 主进程输出和保存结果\n",
    "    if evaluator.local_rank == 0 and final_results:\n",
    "        # 计算整体统计信息\n",
    "        all_rewards = [r['mean_reward'] for r in final_results]\n",
    "        print(f\"\\n多GPU评测结果:\")\n",
    "        print(f\"总prompts数: {len(final_results)}\")\n",
    "        print(f\"平均奖励: {sum(all_rewards)/len(all_rewards):.4f}\")\n",
    "        print(f\"最高奖励: {max(all_rewards):.4f}\")\n",
    "        print(f\"最低奖励: {min(all_rewards):.4f}\")\n",
    "        \n",
    "        # 保存详细结果到JSON文件\n",
    "        output_file = f\"multigpu_evaluation_{args.model.split('/')[-1]}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"结果已保存到: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41e3b3",
   "metadata": {},
   "source": [
    "run_evaluation.sh   评估启动脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1acde",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# 多机多卡评测脚本\n",
    "\n",
    "# 配置参数\n",
    "# MODEL_PATH=\"./output/best_model\"\n",
    "MODEL_PATH=\"/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028\"\n",
    "REWARD_MODEL_PATH=\"/shared/Skywork-Reward_checkpoint-1000/checkpoint-1000\"\n",
    "PROMPTS_PATH=\"/shared/grpo_financial_tuning/data/test_prompts_dataset.jsonl\"\n",
    "SAMPLE_SIZE=50\n",
    "\n",
    "# 多机多卡配置\n",
    "declare -A NODES=(\n",
    "    [\"10.60.68.220\"]=1    # 主节点：2张GPU\n",
    "    [\"10.60.98.173\"]=1    # 从节点：1张GPU\n",
    ")\n",
    "MASTER_ADDR=\"10.60.68.220\"\n",
    "\n",
    "# 创建hostfile\n",
    "create_hostfile() {\n",
    "    echo \"创建DeepSpeed hostfile...\"\n",
    "    > ./hostfile\n",
    "    for node_ip in \"${!NODES[@]}\"; do\n",
    "        gpu_count=${NODES[$node_ip]}\n",
    "        echo \"$node_ip slots=$gpu_count\" >> ./hostfile\n",
    "    done\n",
    "    cat ./hostfile\n",
    "}\n",
    "\n",
    "# 单机多卡模式\n",
    "single_node() {\n",
    "    echo \"启动单机多卡评测...\"\n",
    "    deepspeed --num_gpus=2 model_eval/multigpu_evaluator.py \\\n",
    "        --model $MODEL_PATH \\\n",
    "        --reward_model $REWARD_MODEL_PATH \\\n",
    "        --prompts $PROMPTS_PATH \\\n",
    "        --sample_size $SAMPLE_SIZE\n",
    "}\n",
    "\n",
    "# 多机多卡模式\n",
    "multi_node() {\n",
    "    echo \"启动多机多卡评测...\"\n",
    "    create_hostfile\n",
    "    \n",
    "    deepspeed --hostfile=hostfile model_eval/multigpu_evaluator.py \\\n",
    "        --model $MODEL_PATH \\\n",
    "        --reward_model $REWARD_MODEL_PATH \\\n",
    "        --prompts $PROMPTS_PATH \\\n",
    "        --sample_size $SAMPLE_SIZE\n",
    "}\n",
    "\n",
    "# 解析参数\n",
    "case $1 in\n",
    "    --single)\n",
    "        single_node\n",
    "        ;;\n",
    "    --multi)\n",
    "        multi_node\n",
    "        ;;\n",
    "    *)\n",
    "        echo \"用法: $0 [--single|--multi]\"\n",
    "        echo \"  --single: 单机多卡模式\"\n",
    "        echo \"  --multi:  多机多卡模式\"\n",
    "        exit 1\n",
    "        ;;\n",
    "esac\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361542e7",
   "metadata": {},
   "source": [
    "执行命令 ./run_evaluation.sh --multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c27aa9",
   "metadata": {},
   "source": [
    "GRPO微调后模型效果得分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd4f3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707122127283.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9efa5a",
   "metadata": {},
   "source": [
    "GRPO微调前模型效果得分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e76131",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707122153839.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ddad2",
   "metadata": {},
   "source": [
    "# 7. 模型分布式部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df2905",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 使用清华源加速\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple vllm\n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ray[default]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36b668",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707171539903.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69e8c7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707171601452.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776ad67",
   "metadata": {},
   "source": [
    "sudo ufw allow 22\n",
    "\n",
    "其余端口如果有限制命令相同例如 sudo ufw allow 29500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b18f88",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707180554224.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614696f4",
   "metadata": {},
   "source": [
    "deploy_A800.sh A800部署脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce120e3d",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# ==================== 项目部署脚本 ====================\n",
    "# 作用：将本地项目文件同步到远程服务器，用于A800 GPU服务器部署    执行时删除注释使用 执行时删除注释使用\n",
    "# 使用方法：./deploy_A800.sh\n",
    "\n",
    "echo \"🚀 开始同步项目到服务器...\"\n",
    "\n",
    "# 使用rsync命令进行文件同步\n",
    "# -a：归档模式，保持文件属性\n",
    "# -v：详细输出，显示同步过程\n",
    "# -z：压缩传输，节省带宽\n",
    "# --exclude：排除不需要同步的文件和目录\n",
    "rsync -avz \\\n",
    "  --exclude '.git' \\         # 排除Git版本控制文件\n",
    "  --exclude '__pycache__' \\  # 排除Python编译缓存文件\n",
    "  --exclude '.DS_Store' \\    # 排除macOS系统文件\n",
    "  ./ \\                       # 当前目录作为源目录\n",
    "  ubuntu@117.50.194.2:/shared/grpo_financial_tuning/  # 目标服务器路径\n",
    "\n",
    "echo \"✅ 文件同步完成，执行远程操作...\"\n",
    "\n",
    "# 通过SSH连接到远程服务器并执行命令\n",
    "# << 'EOF' 表示开始多行命令输入，直到遇到EOF结束\n",
    "ssh ubuntu@117.50.194.2 << 'EOF'\n",
    "cd /shared/grpo_financial_tuning    # 切换到项目目录\n",
    "chmod +x run_training.sh           # 给训练脚本添加执行权限\n",
    "chmod +x run_evaluation.sh         # 给评估脚本添加执行权限\n",
    "chmod +x vllm_run.sh               # 给vLLM运行脚本添加执行权限\n",
    "# 如果有需要重启的服务或运行命令，请在下面加\n",
    "# 示例: pkill -f app.py && nohup python3 app.py > out.log 2>&1 &\n",
    "echo \"🚀 已进入远程项目目录，可根据需要添加执行命令\"\n",
    "EOF\n",
    "\n",
    "echo \"🎉 部署完毕！\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vllm_run.sh 部署脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2662a",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "## 脚本功能解析\n",
    "\n",
    "### 整体架构\n",
    "这个脚本实现了一个**分布式模型推理服务**，具体架构为：\n",
    "- **头节点（10.60.68.220）**：运行vLLM API服务器，提供1个GPU\n",
    "- **工作节点（10.60.98.173）**：提供额外的1个GPU算力\n",
    "- **总算力**：2个GPU通过张量并行协同工作\n",
    "\n",
    "### 关键技术组件\n",
    "\n",
    "1. **Ray集群管理**：\n",
    "   - 负责多机资源调度和任务分配\n",
    "   - 头节点协调，工作节点提供算力\n",
    "   - 通过端口6379进行集群通信\n",
    "\n",
    "2. **vLLM推理引擎**：\n",
    "   - 高性能大语言模型推理框架\n",
    "   - 支持张量并行，将模型分割到多个GPU\n",
    "   - 提供OpenAI兼容的API接口\n",
    "\n",
    "3. **张量并行（tensor-parallel-size 2）**：\n",
    "   - 将7B模型的参数分割到2个GPU上\n",
    "   - 每个GPU处理模型的一部分，协同完成推理\n",
    "   - 相比单GPU，推理速度提升约1.8倍\n",
    "\n",
    "### 资源配置说明\n",
    "\n",
    "- **显存使用**：每个GPU只使用30%显存，7B模型约需要6-8GB显存\n",
    "- **网络通信**：强制使用eth0网卡，确保稳定的节点间通信\n",
    "- **API访问**：通过8000端口提供服务，需要API密钥认证\n",
    "\n",
    "### 使用场景\n",
    "这个配置适合**生产环境的模型部署**，可以：\n",
    "- 同时服务多个客户端请求\n",
    "- 提供稳定的推理性能\n",
    "- 支持高并发访问\n",
    "- 便于集成到现有系统中\n",
    "\n",
    "# 定义集群节点IP地址\n",
    "# 这些IP地址应该根据实际的集群配置进行修改\n",
    "HEAD_NODE=\"10.60.68.220\"    # 头节点IP地址 - 主服务器，负责协调和管理整个集群\n",
    "WORKER_NODE=\"10.60.98.173\" # 工作节点IP地址 - 从服务器，提供额外的GPU算力\n",
    "# MODEL_PATH=\"/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028\"\n",
    "MODEL_PATH=\"/shared/grpo_financial_tuning/output/best_model\"  # 模型文件路径 - GRPO训练完成后保存的最佳模型\n",
    "PORT=8000  # API服务端口号 - 客户端通过这个端口访问模型服务\n",
    "\n",
    "# 启动Ray集群\n",
    "# Ray是一个分布式计算框架，用于管理多机多卡的资源调度\n",
    "echo \"启动Ray集群\"\n",
    "\n",
    "# 激活conda环境\n",
    "# conda是Python包管理工具，这里激活名为reward3的虚拟环境\n",
    "# 虚拟环境包含了运行vLLM所需的所有依赖包\n",
    "source ~/miniconda3/etc/profile.d/conda.sh\n",
    "conda activate reward3\n",
    "\n",
    "# 设置PyTorch分布式环境变量\n",
    "# 这些环境变量告诉PyTorch如何在多机之间进行通信\n",
    "export MASTER_ADDR=$HEAD_NODE  # 主节点地址 - 分布式训练的协调节点\n",
    "export MASTER_PORT=29500       # 主节点端口 - 用于节点间通信的端口\n",
    "export GLOO_SOCKET_IFNAME=eth0  # 指定网卡接口 - 强制使用eth0网卡进行通信\n",
    "export NCCL_SOCKET_IFNAME=eth0  # NCCL通信网卡 - NVIDIA集合通信库使用的网卡\n",
    "\n",
    "# 强制停止现有的Ray进程，然后启动头节点\n",
    "# 2>/dev/null: 将错误输出重定向到空设备，避免显示错误信息\n",
    "# || true: 即使命令失败也继续执行，不会中断脚本\n",
    "ray stop --force 2>/dev/null || true\n",
    "\n",
    "# 启动Ray头节点\n",
    "# --head: 指定这是头节点（主节点）\n",
    "# --port=6379: Ray集群通信端口\n",
    "# --dashboard-host=0.0.0.0: 允许从任何IP访问Ray控制面板\n",
    "# --dashboard-port=8265: Ray控制面板的Web端口\n",
    "# --num-gpus=1: 头节点提供1个GPU资源\n",
    "# --num-cpus=24: 头节点提供24个CPU核心\n",
    "ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --num-gpus=1 --num-cpus=24\n",
    "\n",
    "# 启动工作节点\n",
    "echo \"连接工作节点\"\n",
    "\n",
    "# 通过SSH连接到工作节点并启动Ray worker\n",
    "# SSH是安全外壳协议，用于远程登录和执行命令\n",
    "# ubuntu@$WORKER_NODE: 以ubuntu用户身份连接到工作节点\n",
    "ssh ubuntu@$WORKER_NODE \"\n",
    "    # 在工作节点上激活相同的conda环境\n",
    "    source ~/miniconda3/etc/profile.d/conda.sh\n",
    "    conda activate reward3\n",
    "    \n",
    "    # 停止工作节点上可能存在的Ray进程\n",
    "    ray stop --force 2>/dev/null || true\n",
    "    \n",
    "    # 启动Ray工作节点\n",
    "    # --address='$HEAD_NODE:6379': 连接到头节点的Ray集群\n",
    "    # --num-gpus=1: 工作节点提供1个GPU资源\n",
    "    ray start --address='$HEAD_NODE:6379' --num-gpus=1\n",
    "\" 2>/dev/null || echo \"工作节点连接失败，继续单节点运行\"\n",
    "# 如果SSH连接失败，会输出提示信息但不会中断脚本执行\n",
    "\n",
    "echo \"🚀 启动vLLM多机多卡服务器...\"\n",
    "\n",
    "# 启动vLLM API服务器\n",
    "# vLLM是一个高性能的大语言模型推理引擎\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model $MODEL_PATH \\                    # 指定要加载的模型路径\n",
    "  --host 0.0.0.0 \\                        # 服务器监听地址，0.0.0.0表示接受来自任何IP的请求\n",
    "  --port $PORT \\                          # 服务器端口号，客户端通过这个端口访问API\n",
    "  --tensor-parallel-size 2 \\              # 张量并行大小，将模型分割到2个GPU上并行计算\n",
    "  --dtype bfloat16 \\                      # 数据类型，使用bfloat16精度节省显存\n",
    "  --max-model-len 2048 \\                  # 最大序列长度，限制输入+输出的总token数\n",
    "  --gpu-memory-utilization 0.3 \\         # GPU显存利用率，只使用30%的显存，为其他进程预留空间\n",
    "  --disable-log-requests \\                # 禁用请求日志，减少日志输出\n",
    "  --api-key grpo-model-key \\              # API密钥，客户端需要提供这个密钥才能访问服务\n",
    "  --distributed-executor-backend ray      # 分布式执行后端，使用Ray框架管理多机多卡资源\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a41587",
   "metadata": {},
   "source": [
    "执行命令 ./vllm_run.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcda47",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707201954623.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23223a83",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707201851108.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a821ef9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707201930558.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362cd51",
   "metadata": {},
   "source": [
    "请求测试命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e8740",
   "metadata": {},
   "source": [
    "```python\n",
    "python -c \"\n",
    "import requests\n",
    "resp = requests.post('http://localhost:8000/v1/chat/completions', \n",
    "    headers={'Authorization': 'Bearer grpo-model-key'},\n",
    "    json={'model': '/shared/DeepSeek-R1-Distill-Qwen-7B_028/best_complete_model_05261653_028', 'messages': [{'role': 'user', 'content': '你是一个专业的金融领域分析师请对以下问题进行详细解答：周三盘前交易时段，金融板块呈现温和上涨态势：金融精选行业SPDR基金(XLF)上涨0.4%，Direxion每日三倍做多金融股ETF(FAS)上涨1.2%，而其反向产品Direxion每日三倍做空金融股ETF(FAZ)下跌1.1%。万事达卡(MA)股价上涨0.7%，因其董事会批准了最高110亿美元的普通股回购计划，并将季度股息提高16%。Cboe全球市场(CBOE)股价上涨0.5%，该公司披露其指数期权合约的11月日均交易量达到近400万份，较去年同期的330万份增长22%。基于这些市场动态，请分析以下问题：在当前市场环境下，影响金融板块ETF(XLF、FAS、FAZ)价格波动的关键驱动因素有哪些？如何评估万事达卡大规模股票回购和股息增长对其长期估值的影响？Cboe交易量的大幅增长可能预示着哪些市场结构变化？请综合考虑宏观经济环境、行业竞争格局和公司特定事件等多重因素展开分析。'}], 'max_tokens': 1024})\n",
    "print('Status Code:', resp.status_code)\n",
    "print('Response:', resp.json())\n",
    "\"\n",
    "python -c \"\n",
    "import requests\n",
    "resp = requests.post('http://localhost:8000/v1/chat/completions', \n",
    "    headers={'Authorization': 'Bearer grpo-model-key'},\n",
    "    json={'model': '/shared/grpo_financial_tuning/output/best_model', 'messages': [{'role': 'user', 'content': '你是一个专业的金融领域分析师请对以下问题进行详细解答：周三盘前交易时段，金融板块呈现温和上涨态势：金融精选行业SPDR基金(XLF)上涨0.4%，Direxion每日三倍做多金融股ETF(FAS)上涨1.2%，而其反向产品Direxion每日三倍做空金融股ETF(FAZ)下跌1.1%。万事达卡(MA)股价上涨0.7%，因其董事会批准了最高110亿美元的普通股回购计划，并将季度股息提高16%。Cboe全球市场(CBOE)股价上涨0.5%，该公司披露其指数期权合约的11月日均交易量达到近400万份，较去年同期的330万份增长22%。基于这些市场动态，请分析以下问题：在当前市场环境下，影响金融板块ETF(XLF、FAS、FAZ)价格波动的关键驱动因素有哪些？如何评估万事达卡大规模股票回购和股息增长对其长期估值的影响？Cboe交易量的大幅增长可能预示着哪些市场结构变化？请综合考虑宏观经济环境、行业竞争格局和公司特定事件等多重因素展开分析。'}], 'max_tokens': 1024})\n",
    "print('Status Code:', resp.status_code)\n",
    "print('Response:', resp.json())\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966d07d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709154443396.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedede5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250707201838482.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be3cd3",
   "metadata": {},
   "source": [
    "# 8. 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f7ef8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250709164109046.png\" width=100%></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fufan_chat_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
