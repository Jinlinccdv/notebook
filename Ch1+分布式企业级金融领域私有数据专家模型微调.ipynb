{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe98a9e5-4646-47d0-9445-19bf22ef3a54",
   "metadata": {},
   "source": [
    "# <center>Ch1 分布式企业级金融领域私有数据专家模型微调</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838487f",
   "metadata": {},
   "source": [
    "# 项目背景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e142582-17e5-477c-af01-eec2ea699873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 入职背景\n",
    "\n",
    "小王是一名刚入职\"金智投\"公司的NLP算法工程师，拥有深度学习和自然语言处理背景。\"金智投\"是一家新兴金融科技公司，计划推出一款面向普通投资者的AI金融分析助手，帮助用户理解财经新闻并提供投资参考。\n",
    "\n",
    "#### 领导的需求描述\n",
    "\n",
    "小王的领导李总在第一次项目会议上提出了以下需求：\n",
    "\n",
    "\"我们想做一个产品级的模型，开放给用户使用，现在已经使用市面上的开源模型作为支撑，但它对金融领域的理解还不够深入，用户体验也不够好。我们需要对它进行微调，让它成为一个专业的金融助手。具体来说，我们希望这个模型能够：\n",
    "\n",
    "1. 准确解读财经新闻背后的含义，分析可能对市场产生的影响\n",
    "2. 针对用户的投资组合提供个性化的风险提示\n",
    "3. 能够用通俗易懂的语言解释复杂的金融概念\n",
    "4. 在提供分析的同时，不给出明确的'买入/卖出'建议，避免监管风险\n",
    "5. 保持客观中立，不过度乐观或悲观\n",
    "6. 能够识别市场异常波动，给出合理解释\n",
    "7. 尽可能少的资源来做当前项目，方便后续扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e01a5",
   "metadata": {},
   "source": [
    "#### 小王的分析过程\n",
    "\n",
    "小王回到工位后，开始分析这个任务：\n",
    "\n",
    "1. **项目目标分析**：\n",
    "   - 核心是将通用大模型转变为金融专家模型\n",
    "   - 既要专业准确，又要通俗易懂\n",
    "   - 有明确的安全边界（不直接给投资建议）\n",
    "   - 时间和资源有限\n",
    "\n",
    "2. **数据资源评估**：\n",
    "   - 需要金融分析报告 - 可用于领域知识学习\n",
    "   - 最好有用户咨询记录 - 可用于理解用户意图和问题模式\n",
    "   - 数据量不算大，但对于微调已经有一定基础\n",
    "\n",
    "3. **技术路径思考**：\n",
    "   - 是只做SFT就够了，还是需要RLHF/GRPO？\n",
    "   - 如何平衡专业性和可理解性？\n",
    "   - 如何设计奖励模型的评价标准？\n",
    "\n",
    "小王决定先咨询一下公司的金融专家，了解更多领域知识，同时查阅相关论文和实践案例。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49b321",
   "metadata": {},
   "source": [
    "\n",
    "#### 小王提出的技术方案\n",
    "\n",
    "经过一周的调研和思考，小王在项目会议上提出了以下微调方案：\n",
    "\n",
    "##### 一、整体技术路线\n",
    "\n",
    "\"我建议采用两阶段微调策略：先进行监督微调(SFT)构建金融领域基础能力，再通过基于人类反馈的强化学习优化用户体验和安全边界。\"\n",
    "\n",
    "##### 二、SFT阶段（第一阶段）\n",
    "\n",
    "1. **数据处理**：\n",
    "   - 将金融报告转化为指令-回应对，覆盖新闻解读、市场分析、概念解释等场景\n",
    "   - 从用户咨询中提取高质量问答对\n",
    "   - 人工构建特殊场景的示例（如市场异常波动、风险提示等）\n",
    "\n",
    "2. **训练策略**：\n",
    "   - 使用LoRA技术进行参数高效微调，节省计算资源\n",
    "   - 设置适当学习率和训练轮次，避免过拟合\n",
    "\n",
    "3. **评估方法**：\n",
    "   - 构建金融专业测试集，评估专业准确性\n",
    "   - 邀请非专业用户测试，评估可理解性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79105312",
   "metadata": {},
   "source": [
    "\n",
    "##### 三、强化学习阶段（第二阶段）\n",
    "\n",
    "1. **奖励模型设计**：\n",
    "   - 构建多维度评价标准：专业准确性、可理解性、风险揭示充分性、客观中立性\n",
    "   - 为每个维度设计评分规则和权重\n",
    "\n",
    "2. **偏好数据集构建**：\n",
    "   - 构建典型金融问题构建优/良/差三级答案\n",
    "   - 形成600对比较数据用于训练奖励模型\n",
    "   - 确保数据覆盖不同市场环境和投资品类\n",
    "\n",
    "3. **强化学习训练**：\n",
    "   - 使用训练好的奖励模型指导模型优化\n",
    "   - 设置适当的KL惩罚系数，避免偏离SFT基础能力\n",
    "   - 重点优化\"不给出明确投资建议，使对话通俗易懂\"的安全边界"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912fddf4",
   "metadata": {},
   "source": [
    "# 1. 方案说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72373c4",
   "metadata": {},
   "source": [
    "整体流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95cc84",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526150400399.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a3e97",
   "metadata": {},
   "source": [
    "专家模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f7ef8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528171000691.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d979a55",
   "metadata": {},
   "source": [
    "进行数据收集："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c6e49",
   "metadata": {},
   "source": [
    "数据集主要分为三个方向    \n",
    "1、公司自有数据集，完全基于公司的场景与业务是模型微调数据的最优选择呢   \n",
    "2、直接在其他平台进行购买或者抓取，基本是市面上的通用数据集，或者是针对某个方向内的综合数据，如果是购买的专业性比较强格式比较干净    \n",
    "3、开源数据集，数据来源比较丰富，但是数据更新相对不及时有一定滞后性、格式较为杂乱通常需要二次处理。主要来源包括"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21754d",
   "metadata": {},
   "source": [
    "## 1.1 公开数据源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797da45",
   "metadata": {},
   "source": [
    "#### 1. 公开金融FAQ与知识库\n",
    "##### 渠道\n",
    "- 各大银行、券商、保险公司官网的“常见问题”或“帮助中心”\n",
    "- 监管机构官网（如中国证监会、银保监会、央行）的政策解读、业务指南\n",
    "- 金融产品说明书、业务流程介绍页面\n",
    "##### 优点\n",
    "- 数据权威、合规\n",
    "- 问答格式天然适合微调\n",
    "\n",
    "##### 注意事项\n",
    "- 需注意版权声明，部分内容仅限参考学习，商用需谨慎\n",
    "\n",
    "#### 2. 金融年报、公告、招股书\n",
    "##### 渠道\n",
    "- 上市公司官网、巨潮资讯、东方财富、同花顺等平台\n",
    "- 证券交易所官网（如上交所、深交所）\n",
    "##### 优点\n",
    "- 数据详实、结构化\n",
    "- 可提取大量金融术语、业务流程、产品信息\n",
    "##### 注意事项\n",
    "- 需自行整理为问答或对话格式\n",
    "- 注意数据时效性\n",
    "\n",
    "#### 3. 金融论坛与社区\n",
    "##### 渠道\n",
    "- 雪球、知乎、东方财富、金融界等社区\n",
    "- 微信公众号、知乎专栏等\n",
    "##### 优点\n",
    "- 贴近用户真实问题\n",
    "- 多样性强，覆盖实际业务场景\n",
    "##### 注意事项\n",
    "- 数据质量参差不齐，需清洗\n",
    "- 注意隐私和合规，避免采集用户敏感信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c1d7",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. [FinNLP（AI4Finance-Foundation）](https://github.com/AI4Finance-Foundation/FinNLP)\n",
    "- **简介**：全球金融自然语言处理领域最全的开源数据集集合，涵盖新闻、公告、问答、情感分析等多种任务。\n",
    "- **适用**：金融文本分类、问答、信息抽取等\n",
    "- **获取方式**：直接在GitHub页面下载\n",
    "\n",
    "\n",
    "#### 2. [Hugging Face Datasets - 金融相关数据集](https://huggingface.co/datasets?search=finance)\n",
    "- **简介**：Hugging Face Datasets平台收录了大量金融领域的开源数据集，包括金融问答、金融新闻、金融对话等。\n",
    "- **适用**：金融QA、对话、文本生成等\n",
    "- **获取方式**：可用`datasets`库一键加载，如：\n",
    "  ```python\n",
    "  from datasets import load_dataset\n",
    "  ds = load_dataset(\"zeroshot/financial_phrasebank\")\n",
    "  ```\n",
    "\n",
    "\n",
    "#### 3. [Kaggle - Finance Datasets](https://www.kaggle.com/datasets?search=finance)\n",
    "- **简介**：Kaggle上有大量金融相关的开源数据集，包括金融新闻、股票公告、金融问答等。\n",
    "- **适用**：金融文本分析、问答、预测等\n",
    "- **获取方式**：注册Kaggle账号后可直接下载\n",
    "\n",
    "\n",
    "#### 4. [中国证监会/银保监会/央行官网](http://www.csrc.gov.cn/) [银保监会](http://www.cbirc.gov.cn/) [中国人民银行](http://www.pbc.gov.cn/)\n",
    "- **简介**：权威政策解读、业务指南、金融知识普及等，适合整理成问答数据。\n",
    "- **适用**：金融政策、合规、基础知识问答\n",
    "- **获取方式**：手动爬取或整理\n",
    "\n",
    "\n",
    "#### 5. [各大银行/券商官网FAQ](如[招商银行FAQ](https://www.cmbchina.com/cfweb/faq/faq.aspx))\n",
    "- **简介**：银行、券商等金融机构的常见问题解答页面，内容权威且结构化。\n",
    "- **适用**：金融产品、业务流程、客户服务等问答\n",
    "- **获取方式**：手动采集或爬虫\n",
    "\n",
    "\n",
    "#### 6. [金融知识图谱 - OpenKG](https://www.openkg.cn/dataset/finance)\n",
    "- **简介**：OpenKG收录了金融知识图谱、金融术语、金融产品等结构化数据。\n",
    "- **适用**：知识抽取、问答、知识增强\n",
    "- **获取方式**：网站直接下载\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3aa14",
   "metadata": {},
   "source": [
    "## 1.2 模型类型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a1004",
   "metadata": {},
   "source": [
    "### 模型分类依据\n",
    "\n",
    "##### 1. 训练目标与优化方向\n",
    "\n",
    "这是最基础的分类依据，根据模型训练时优化的主要目标进行区分：\n",
    "\n",
    "- **推理增强型模型**：明确优化了逻辑推理、因果分析能力\n",
    "- **知识密集型模型**：优化了事实召回与知识表示\n",
    "- **指令对齐型模型**：专注于遵循用户指令，通常经过RLHF训练\n",
    "- **领域专精型模型**：针对特定垂直领域进行了额外训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ddfac",
   "metadata": {},
   "source": [
    "\n",
    "##### 2. 架构与技术特性\n",
    "\n",
    "基于模型架构设计和技术实现的分类：\n",
    "\n",
    "- **基础语言模型**：仅完成预训练，无指令微调\n",
    "- **对话型模型**：经过SFT/RLHF优化，适合多轮交互\n",
    "- **多模态模型**：能处理文本以外的输入（图像、音频等）\n",
    "- **检索增强模型**：内置检索机制，可访问外部知识\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3698e",
   "metadata": {},
   "source": [
    "\n",
    "### 主流模型分类示例\n",
    "\n",
    "##### 推理增强型模型\n",
    "- **具体模型**：\n",
    "  - DeepSeek-R1系列\n",
    "  - Claude Opus/Sonnet\n",
    "  - GPT-4-Turbo/GPT-4o\n",
    "  - Gemini Pro/Ultra\n",
    "- **技术依据**：这些模型在基准测试(如GSM8K、MATH、HumanEval)中表现突出，说明它们的推理能力经过了专门优化\n",
    "- **适合金融场景**：\n",
    "  - 投资策略论证与评估\n",
    "  - 市场趋势因果分析\n",
    "  - 交易决策链推导\n",
    "  - 风险评估与传导分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838fd1d",
   "metadata": {},
   "source": [
    "\n",
    "##### 知识密集型模型\n",
    "- **具体模型**：\n",
    "  - Llama 2/3\n",
    "  - Qwen-72B/14B\n",
    "  - GLM-4\n",
    "  - 文心一言\n",
    "- **技术依据**：在MMLU、TruthfulQA等知识密集型任务上表现优秀，说明其知识存储与召回能力强\n",
    "- **适合金融场景**：\n",
    "  - 金融法规与政策解读\n",
    "  - 产品条款与规则说明\n",
    "  - 市场历史事件分析\n",
    "  - 金融知识教育普及\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76102d6",
   "metadata": {},
   "source": [
    "\n",
    "##### 领域专精型模型\n",
    "- **具体模型**：\n",
    "  - BloombergGPT\n",
    "  - FinGPT\n",
    "- **技术依据**：这些模型使用了大量金融领域数据进行预训练或微调，在金融特定任务上表现优于通用模型\n",
    "- **适合金融场景**：\n",
    "  - 专业金融研报生成\n",
    "  - 证券分析与估值\n",
    "\n",
    "##### 多模态模型\n",
    "- **具体模型**：\n",
    "  - GPT-4V\n",
    "  - Gemini Pro Vision\n",
    "  - Claude Sonnet/Opus Vision\n",
    "  - Qwen-VL\n",
    "- **技术依据**：这些模型能同时处理文本和图像输入，通过图文对齐训练实现多模态理解\n",
    "- **适合金融场景**：\n",
    "  - 财报图表解读\n",
    "  - 技术分析图形识别\n",
    "  - 交易终端界面理解\n",
    "  - 金融数据可视化分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007d1b1",
   "metadata": {},
   "source": [
    "\n",
    "#### 金融场景适配性分析\n",
    "\n",
    "##### 投资分析与决策推理场景\n",
    "\n",
    "- **最适合模型类型**：推理增强型模型\n",
    "- **技术适配原因**：\n",
    "  - 投资分析需要多步骤推理和逻辑论证\n",
    "  - 推理型模型经过了链式思考(Chain-of-Thought)训练，能展现完整推理过程\n",
    "  - 在处理\"如果...那么...\"假设情境时表现更佳\n",
    "- **具体应用案例**：\n",
    "  - 使用DeepSeek-R1分析特斯拉财报对其股价的潜在影响\n",
    "  - 应用GPT-4评估加息对不同行业的差异化影响\n",
    "- **适配度量化**：\n",
    "  - 在金融推理测试集上，推理增强模型通常比通用模型高15-20%的准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd491a",
   "metadata": {},
   "source": [
    "\n",
    "##### 金融监管与合规场景\n",
    "\n",
    "- **最适合模型类型**：知识密集型模型\n",
    "- **技术适配原因**：\n",
    "  - 合规领域需要大量专业知识储备\n",
    "  - 知识密集型模型训练数据覆盖更广泛的法规政策\n",
    "  - 能准确引用特定条款和规定\n",
    "- **具体应用案例**：\n",
    "  - 使用GLM-4解读最新的银行业监管政策变化\n",
    "  - 应用Qwen-72B说明不同国家的反洗钱要求\n",
    "- **适配度量化**：\n",
    "  - 在金融法规问答测试中，知识密集型模型准确率通常高出10-15%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859880b",
   "metadata": {},
   "source": [
    "\n",
    "##### 专业金融研究场景\n",
    "\n",
    "- **最适合模型类型**：领域专精型模型\n",
    "- **技术适配原因**：\n",
    "  - 专门针对金融语料进行了训练\n",
    "  - 更好地理解行业特定术语和隐含知识\n",
    "  - 预训练数据包含大量金融文献\n",
    "- **具体应用案例**：\n",
    "  - 使用BloombergGPT生成完整的股票研究报告\n",
    "  - 应用Fin-R1分析债券收益率曲线变化\n",
    "- **适配度量化**：\n",
    "  - 在专业金融术语理解测试中，领域专精模型比通用模型高25-30%的准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7102e",
   "metadata": {},
   "source": [
    "\n",
    "##### 金融图表与数据分析场景\n",
    "\n",
    "- **最适合模型类型**：多模态模型\n",
    "- **技术适配原因**：\n",
    "  - 能同时处理文本和视觉信息\n",
    "  - 理解图表、K线形态等视觉元素\n",
    "  - 可从图像中提取数据进行分析\n",
    "- **具体应用案例**：\n",
    "  - 使用GPT-4V分析candlestick图表中的市场模式\n",
    "  - 应用Gemini Pro Vision解读财报中的复杂图表\n",
    "- **适配度量化**：\n",
    "  - 在金融图表理解任务中，多模态模型比纯文本模型高出40%以上的准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e48b3e",
   "metadata": {},
   "source": [
    "#### DeepSeek-R1的合理性分析\n",
    "\n",
    "DeepSeek-R1-Distill-Qwen-7B作为推理增强型模型，适合金融分析场景：\n",
    "\n",
    "1. **技术优势**：\n",
    "   - 在数学推理任务上表现出色，这与金融分析的逻辑性要求高度一致\n",
    "   - 蒸馏自更大的模型，保留了强大的推理能力同时提高了效率\n",
    "   - 采用了链式思考(CoT)训练方法，能给出结构化的分析过程\n",
    "\n",
    "2. **金融场景适配性**：\n",
    "   - 投资建议需要充分的论据支持和清晰的推理路径\n",
    "   - 金融市场分析需要因果关系推导和逻辑性强的论证\n",
    "   - 风险评估需要多步骤的条件推理能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645583",
   "metadata": {},
   "source": [
    "## 1.3 微调技术类型选型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf79dec",
   "metadata": {},
   "source": [
    "### 1. LoRA (Low-Rank Adaptation)\n",
    "\n",
    "### 技术原理\n",
    "LoRA通过矩阵分解来近似权重更新。在标准微调中，权重矩阵 W 会更新为 W + ΔW。LoRA将这个更新分解为两个低秩矩阵的乘积：ΔW = A × B，其中A的尺寸为 d × r，B的尺寸为 r × k，r远小于d和k（r称为秩）。\n",
    "\n",
    "具体实现时，对于原始权重矩阵W ∈ ℝᵈˣᵏ，LoRA通过两个小矩阵A ∈ ℝᵈˣʳ和B ∈ ℝʳˣᵏ来参数化更新。前向传播计算变为：h = Wx + BAx，其中只训练A和B，而W保持冻结。\n",
    "\n",
    "### 金融场景适配性详解\n",
    "**适用于金融推理分析的原因**：\n",
    "1. **知识保留机制**：LoRA通过冻结原始权重，保留了模型在预训练阶段获得的广泛知识，包括数学计算和逻辑推理能力。这对金融分析至关重要，因为投资决策需要准确的数值计算和因果推理。\n",
    "\n",
    "2. **表达能力与模型大小的平衡**：虽然LoRA参数少，但其在低秩子空间中能有效捕获特定领域的知识分布。对金融领域，这意味着能够学习复杂的金融概念和关系，同时保持计算效率。\n",
    "\n",
    "3. **渐进适应能力**：通过调整rank值，LoRA可以灵活控制适应程度。金融分析既需要通用推理能力，又需要专业领域知识，这种渐进式适应非常适合。\n",
    "\n",
    "\n",
    "#### 金融场景适配分析\n",
    "\n",
    "**优势**：\n",
    "- 保留模型原有推理能力，同时注入金融专业知识\n",
    "- 训练速度快，单次迭代速度比全量微调快3-5倍\n",
    "- 支持多个LoRA适配器合并或切换，可满足不同金融分析场景\n",
    "- 在推理任务表现接近全量微调，对金融分析逻辑链至关重要\n",
    "\n",
    "**劣势**：\n",
    "- 仅调整部分层可能导致知识注入不均衡\n",
    "- 当rank值设置过小时，难以学习复杂的金融知识分布\n",
    "- 对不同rank值的选择敏感，需要针对特定任务多次实验\n",
    "\n",
    "**适用金融场景**：投资分析报告生成、市场趋势推理、财务状况评估\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeaa3ac",
   "metadata": {},
   "source": [
    "### 2. QLoRA (Quantized LoRA)\n",
    "#### 技术原理\n",
    "QLoRA建立在LoRA基础上，但引入了模型量化技术。它使用4位NormalFloat(NF4)量化将基础模型压缩存储，在需要计算时再反量化到BF16/FP16精度。\n",
    "\n",
    "具体而言，QLoRA实现了：\n",
    "- 4位NF4量化存储预训练权重\n",
    "- 二阶模拟量化计算梯度\n",
    "- 分页优化器减少内存碎片\n",
    "- 反量化到BF16进行计算\n",
    "\n",
    "这些技术结合允许在单个GPU上微调大型模型，同时保持性能接近全精度LoRA。\n",
    "\n",
    "#### 金融场景适配性详解\n",
    "**适用于大规模金融模型的原因**：\n",
    "1. **精度与资源平衡机制**：使用NF4量化可以在保持高数值精度的同时大幅降低内存需求。这对金融模型尤为重要，因为金融分析往往需要更大模型来处理复杂规则和关系网络。\n",
    "\n",
    "2. **内存效率与推理深度的权衡**：通过量化，QLoRA使单GPU环境能够微调33B参数的模型。金融领域的深度分析(如系统性风险评估)通常需要大型模型的推理能力，QLoRA使这成为可能。\n",
    "\n",
    "3. **适用的量化尺度**：NF4量化专门设计用于保持权重分布的精确性，对数值敏感的金融任务尤为重要。与其他量化方法相比，NF4在保持数学准确性方面表现更好。\n",
    "\n",
    "#### 金融场景适配分析\n",
    "\n",
    "**优势**：\n",
    "- 极大降低显存需求，允许在单GPU上微调大型金融模型\n",
    "- 为资源受限环境提供唯一可行的大模型微调方案\n",
    "- 性能损失有限，通常在2-3%范围内\n",
    "\n",
    "**劣势**：\n",
    "- 量化导致的精度损失可能影响金融计算的准确性\n",
    "- 训练速度比标准LoRA慢20-30%\n",
    "- 在复杂数值推理任务上精度下降更明显\n",
    "- 生产环境部署需要处理量化/反量化，增加工程复杂性\n",
    "\n",
    "**适用金融场景**：需要大规模模型(>13B)但计算资源有限的金融分析系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b79d4",
   "metadata": {},
   "source": [
    "### 3. P-Tuning v2\n",
    "\n",
    "#### 技术原理\n",
    "P-Tuning v2在每个Transformer层添加一小组可训练的连续提示向量(prompt vectors)。这些提示向量不对应实际的词汇表单词，而是在嵌入空间中直接优化的向量。\n",
    "\n",
    "具体实现上，P-Tuning v2:\n",
    "- 在每层Transformer的self-attention中添加可学习的前缀向量\n",
    "- 这些向量在训练过程中更新，其余模型参数保持冻结\n",
    "- 向量数量通常为模型隐藏大小的0.1-1%\n",
    "\n",
    "这种设计使得P-Tuning v2特别高效，同时提供了足够的调整能力。\n",
    "\n",
    "#### 金融场景适配性详解\n",
    "**适用于金融文本分类、市场情绪分析和实体识别的原因**：\n",
    "1. **表示学习效率机制**：P-Tuning v2在NLU(自然语言理解)任务上表现尤为出色。这是因为它通过修改每层的注意力计算，实现了有效的表示学习。金融文本分类和情绪分析本质上是理解任务，需要精确提取文本特征进行判断。\n",
    "\n",
    "2. **上下文表示能力**：P-Tuning v2在捕获上下文语义关系方面非常高效。金融实体识别(如从文本中提取公司名称、金融产品、数值指标等)严重依赖上下文理解，P-Tuning v2的这一特性使其特别适合。\n",
    "\n",
    "3. **参数高效与精度权衡**：与其他技术相比，P-Tuning v2使用极少的参数(通常<0.2%)就能达到接近全量微调的效果，尤其是在分类任务上。在参数量相同情况下，P-Tuning v2在分类任务上的表现优于LoRA。\n",
    "\n",
    "#### 金融场景适配分析\n",
    "\n",
    "**优势**：\n",
    "- 参数量极小，训练迅速\n",
    "- 在分类和理解任务上表现优异\n",
    "- 显存占用极低，适合在边缘设备部署\n",
    "\n",
    "**劣势**：\n",
    "- 在生成任务上表现不如LoRA和Prefix-Tuning\n",
    "- Wang等人(2022)研究显示，对长文本推理能力提升有限\n",
    "- 无法有效改变模型已有的知识偏见\n",
    "- 难以注入复杂的领域专业知识\n",
    "\n",
    "**适用金融场景**：金融文本分类、市场情绪分析、金融实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67714b40",
   "metadata": {},
   "source": [
    "### 4. Prefix-Tuning\n",
    "\n",
    "#### 技术原理\n",
    "Prefix-Tuning在输入序列前添加一组可训练的前缀向量。与P-Tuning v2不同，Prefix-Tuning主要在输入层面操作，而非每个Transformer层。\n",
    "\n",
    "具体实现上，Prefix-Tuning:\n",
    "- 在输入序列前添加一组可训练的前缀向量\n",
    "- 这些向量通过一个小型MLP网络参数化，以提高训练稳定性\n",
    "- 前缀长度通常为10-100个token，视任务复杂度而定\n",
    "\n",
    "这种设计使Prefix-Tuning特别适合生成任务，因为它能有效控制模型的生成方向。\n",
    "\n",
    "#### 金融场景适配性详解\n",
    "**适用于金融报告生成和市场评论的原因**：\n",
    "1. **生成控制机制**：Prefix-Tuning在文本生成任务上表现接近全量微调。这是因为前缀向量实际上为生成过程提供了一个\"起点\"和\"风格指导\"。金融报告需要特定的格式和风格，Prefix-Tuning可以有效控制这些特性。\n",
    "\n",
    "2. **长文本一致性表现**：Prefix-Tuning在生成长文本时能保持更好的风格一致性和结构连贯性。金融市场评论和报告通常篇幅较长，需要保持一致的专业语气和逻辑结构。\n",
    "\n",
    "3. **知识表达与风格分离**：Prefix-Tuning能更好地分离内容知识和表达风格。这使得金融分析可以保持专业性的同时，调整为不同受众(专业投资者vs普通大众)理解的表达方式。\n",
    "\n",
    "\n",
    "#### 金融场景适配分析\n",
    "\n",
    "**优势**：\n",
    "- 在生成任务上表现接近全量微调\n",
    "- 文本生成连贯性和流畅性保持良好\n",
    "- 适合控制金融文本的风格和语调\n",
    "\n",
    "**劣势**：\n",
    "- 训练稳定性差，容易出现梯度爆炸问题\n",
    "- 对超参数选择极为敏感\n",
    "- 在推理任务上表现不如LoRA\n",
    "- 与原始预训练任务差异大时，效果下降明显\n",
    "\n",
    "**适用金融场景**：金融报告生成、市场评论撰写、投资观点总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239aece",
   "metadata": {},
   "source": [
    "### 5. Adapter Tuning\n",
    "\n",
    "#### 技术原理\n",
    "Adapter Tuning通过在Transformer架构中插入小型可训练模块(adapters)实现高效微调。这些adapters通常是瓶颈结构(bottleneck architecture)，即先将高维特征映射到低维空间，再映射回高维空间。\n",
    "\n",
    "具体实现上，Adapter:\n",
    "- 插入在Transformer层的前馈网络后和/或注意力模块后\n",
    "- 使用两个线性层和一个非线性激活函数构成\n",
    "- 通常包含下投影、激活函数和上投影三部分\n",
    "- 使用残差连接保证信息流动\n",
    "\n",
    "这种设计使得Adapter可以有效学习任务特定的表示，同时保留原模型能力。\n",
    "\n",
    "#### 金融场景适配性详解\n",
    "**适用于多任务金融系统的原因**：\n",
    "1. **模块化设计机制**：不同任务的adapters可以独立训练和组合。这种模块化设计使金融系统能够同时处理多种相关但不同的任务，如风险评估、收益预测、合规检查等。\n",
    "\n",
    "2. **任务隔离与共享能力**：Adapter能有效防止灾难性遗忘，同时允许任务间的知识迁移。金融分析系统通常需要处理多种相关任务，既需要避免任务间的负面干扰，又要利用任务间的知识共享。\n",
    "\n",
    "3. **持续学习能力**：Adapter特别适合增量学习场景，可以不断添加新任务而不影响旧任务性能。金融环境快速变化，新的规则和分析需求不断出现，这种持续学习能力非常重要。\n",
    "\n",
    "\n",
    "#### 金融场景适配分析\n",
    "\n",
    "**优势**：\n",
    "- 多任务学习能力强，支持任务间快速切换\n",
    "- 训练稳定性高，不易出现梯度问题\n",
    "- 在低资源环境下表现稳定\n",
    "- 可组合性好，不同任务的Adapter可独立训练后组合\n",
    "\n",
    "**劣势**：\n",
    "- 在相同参数量下，通常表现不如LoRA\n",
    "- 架构改变导致部署复杂度增加\n",
    "- 多个Adapter堆叠可能导致推理延迟增加\n",
    "\n",
    "**适用金融场景**：需要同时处理多种金融任务的统一系统，如综合金融服务平台"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed74ee",
   "metadata": {},
   "source": [
    "#### 最佳选择：LoRA\n",
    "\n",
    "**推荐理由**：\n",
    "- 在保留推理能力同时注入金融知识方面表现最佳\n",
    "- 资源效率与效果平衡最优\n",
    "- 实施复杂度适中，大量实践验证有效\n",
    "- 对金融推理任务的支持最为全面\n",
    "\n",
    "**特定参数建议**：\n",
    "- rank：建议设置为128-256，复杂金融任务可考虑更高\n",
    "- 学习率：1e-4左右，金融领域词汇多时可适当降低\n",
    "- 目标层：至少包括所有注意力层和MLP层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7a887",
   "metadata": {},
   "source": [
    "### 金融推理场景的最终建议\n",
    "\n",
    "综合考虑您使用DeepSeek-R1-Distill-Qwen-7B模型开发金融推理应用的需求，我的详细建议是：\n",
    "\n",
    "1. **基础微调**：使用LoRA(rank=32)进行基础微调，重点注入金融专业知识和推理能力\n",
    "\n",
    "2. **安全对齐**：在LoRA基础上，使用DPO进行第二阶段微调，确保模型输出符合金融监管要求\n",
    "\n",
    "3. **资源受限场景**：如果计算资源严格受限，可考虑QLoRA作为替代方案，接受轻微性能损失换取显著资源节省\n",
    "\n",
    "4. **扩展功能**：随着应用发展，可以考虑引入Adapter实现多功能扩展，而不干扰核心分析能力\n",
    "\n",
    "这种分阶段、组合应用的策略，能够最大限度发挥DeepSeek-R1模型在金融推理方面的潜力，同时满足不同应用场景的需求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb71396",
   "metadata": {},
   "source": [
    "# 2. 数据集构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12855cb",
   "metadata": {},
   "source": [
    "## 1.数据集下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9801e8",
   "metadata": {},
   "source": [
    "国际新闻  https://huggingface.co/datasets/ashraq/financial-news-articles/viewer/default/train?row=1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d773971",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526184327596.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b4b19",
   "metadata": {},
   "source": [
    "带投资建议的 https://huggingface.co/datasets/ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59c4df",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518155037071.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5dc941",
   "metadata": {},
   "source": [
    "带有思考过程的财务建议 https://huggingface.co/datasets/nihiluis/financial-advisor-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe807c7a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518155726504.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4fa2e",
   "metadata": {},
   "source": [
    "与比特币相关 https://huggingface.co/datasets/xinqiyang/tradingview_msn_financial_news_1k/viewer/default/train?row=220250518171457856"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e269d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518171457856.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b51aa",
   "metadata": {},
   "source": [
    "包含主题与详细内容https://huggingface.co/datasets/gunnybd01/Financial_Services_News_smr/viewer/default/train?row=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59df1cb",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526184617525.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d762ca4",
   "metadata": {},
   "source": [
    "翻译后内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1591d7b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518171301913.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbf867",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526185101347.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c5638",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518180536819.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a29265",
   "metadata": {},
   "source": [
    "autodl科学加速命令：source /etc/network_turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c595390",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526190125144.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa701cf0",
   "metadata": {},
   "source": [
    "安装huggingface 数据安装包方便后续进行数据下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27761bd7-3adb-4fd3-b45d-fb2fa8f47cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting datasets\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/20/34/a08b0ee99715eaba118cbe19a71f7b5e2425c2718ef96007c325944a1152/datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bb/1b/2168d6050e52ff1e6cefc61d600723870bf569cbf41d13db939c8cf97a16/pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/38/f8/d8fddee9ed0d0c0f4a2132c1dfcf0e3e53265055da8df952a53e7eaf178c/pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/11/a7/81dba5010f7e733de88af9555725146fc133be97ce36533867f4c7e75066/xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/83/81/a8fd9c226f7e3bc8918f1e456131717cb38e93f18ccc109bf3c8471e464f/huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.2/484.2 kB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/23/06/4203ffa2beb5bedb07f0da0f79b7d9039d1c33f522e0d1a2d5b6218e6f2e/aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/a6/564ecde55ee633270a793999ef4fd1d2c2b32b5a7eec903b1012cb7c5143/frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f0/ac/7ced59dcdfeddd03e601edb05adff0c66d81ed4a5160c443e44f2379eef0/multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3b/4c/f72c9e1022b3b043ec7dc475a0f405d4c3e10b9b1d378a7330fecf0652da/propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.0/245.0 kB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/15/45/212604d3142d84b4065d5f8cab6582ed3d78e4cc250568ef2a36fe1cf0a5/yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.2/349.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, requests, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 huggingface-hub-0.31.2 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 requests-2.32.3 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0466a",
   "metadata": {},
   "source": [
    "建议直接在窗口执行数据下砸，并制定数据下载路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b55f2",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"gunnybd01/Financial_Services_News_smr\")      \n",
    "指定路径\n",
    "ds = load_dataset(\"gunnybd01/Financial_Services_News_smr\",cache_dir=\"/root/autodl-tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f43a60",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526193332151.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09ad72",
   "metadata": {},
   "source": [
    "查看数据存储路径下数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f6116",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518192903087.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9dd7a",
   "metadata": {},
   "source": [
    "按照个人习惯对数据进行移动(不移动也可以)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf3141",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518193617899.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcca685",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250518193524620.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2458a5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526193914310.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5730a",
   "metadata": {},
   "source": [
    "对数据进行解析，并进行过滤，转换成jsonl形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ab63d-8ccf-4897-aa88-24b6e09276d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尝试使用 datasets.Dataset.from_file 加载数据...\n",
      "成功加载文件: /root/autodl-tmp/data/financial_services_news_smr-train-00000-of-00002.arrow\n",
      "成功加载文件: /root/autodl-tmp/data/financial_services_news_smr-train-00001-of-00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106640/106640 [00:08<00:00, 13124.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数据量: 106640\n",
      "过滤后的数据量: 70061\n",
      "数据已保存到 /root/autodl-tmp/data/filtered_financial_news_5k.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import json                                    # 用于处理JSON格式数据的读写\n",
    "import os                                      # 用于操作系统相关功能，如文件路径操作\n",
    "from datasets import load_from_disk, Dataset  # Hugging Face datasets库，用于加载和处理数据集\n",
    "from tqdm import tqdm                          # 用于显示进度条，让用户知道处理进度\n",
    "\n",
    "# 指定数据目录和输出文件路径\n",
    "data_dir = \"/root/autodl-tmp/data\"                                          # 数据存储的根目录\n",
    "output_file = os.path.join(data_dir, \"filtered_financial_news_5k.jsonl\")   # 过滤后数据的输出文件路径\n",
    "\n",
    "try:\n",
    "    # 第一种方法：使用 Hugging Face 的方法直接读取 Arrow 文件\n",
    "    # Arrow是一种高效的列式数据存储格式，Hugging Face datasets经常使用这种格式\n",
    "    from datasets import Dataset  # 重新导入Dataset类（虽然上面已经导入，但这里明确表示要使用）\n",
    "    import pyarrow as pa          # PyArrow库，用于处理Arrow格式文件\n",
    "    \n",
    "    # 定义两个Arrow文件的完整路径\n",
    "    # 这些文件是数据集的分片文件（split files），大数据集通常会分成多个文件存储\n",
    "    arrow_file_1 = os.path.join(data_dir, \"financial_services_news_smr-train-00000-of-00002.arrow\")  # 第一个分片文件\n",
    "    arrow_file_2 = os.path.join(data_dir, \"financial_services_news_smr-train-00001-of-00002.arrow\")  # 第二个分片文件\n",
    "    \n",
    "    # 使用 datasets 库的特殊方法加载数据\n",
    "    print(\"尝试使用 datasets.Dataset.from_file 加载数据...\")\n",
    "    \n",
    "    # 尝试直接读取每个分片文件\n",
    "    datasets = []  # 创建一个空列表，用于存储成功加载的数据集\n",
    "    \n",
    "    # 检查第一个Arrow文件是否存在，如果存在则尝试加载\n",
    "    if os.path.exists(arrow_file_1):\n",
    "        try:\n",
    "            ds1 = Dataset.from_file(arrow_file_1)  # 从Arrow文件创建Dataset对象\n",
    "            datasets.append(ds1)                   # 将成功加载的数据集添加到列表中\n",
    "            print(f\"成功加载文件: {arrow_file_1}\")\n",
    "        except Exception as e:\n",
    "            # 如果加载失败，打印错误信息但不停止程序\n",
    "            print(f\"加载 {arrow_file_1} 失败: {e}\")\n",
    "    \n",
    "    # 检查第二个Arrow文件是否存在，如果存在则尝试加载\n",
    "    if os.path.exists(arrow_file_2):\n",
    "        try:\n",
    "            ds2 = Dataset.from_file(arrow_file_2)  # 从Arrow文件创建Dataset对象\n",
    "            datasets.append(ds2)                   # 将成功加载的数据集添加到列表中\n",
    "            print(f\"成功加载文件: {arrow_file_2}\")\n",
    "        except Exception as e:\n",
    "            # 如果加载失败，打印错误信息但不停止程序\n",
    "            print(f\"加载 {arrow_file_2} 失败: {e}\")\n",
    "    \n",
    "    # 如果成功加载了至少一个数据集文件\n",
    "    if datasets:\n",
    "        # 合并数据集（如果有多个分片文件）\n",
    "        if len(datasets) > 1:\n",
    "            # 如果有多个数据集，使用concatenate_datasets函数将它们合并成一个\n",
    "            from datasets import concatenate_datasets\n",
    "            dataset = concatenate_datasets(datasets)\n",
    "        else:\n",
    "            # 如果只有一个数据集，直接使用它\n",
    "            dataset = datasets[0]\n",
    "            \n",
    "        # 开始过滤数据并写入到输出文件\n",
    "        total_count = len(dataset)      # 获取数据集的总记录数\n",
    "        filtered_count = 0              # 初始化过滤后的记录计数器\n",
    "        \n",
    "        # 打开输出文件，准备写入过滤后的数据\n",
    "        # 'w'表示写入模式，'encoding=\"utf-8\"'确保正确处理中文等Unicode字符\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            # 遍历数据集中的每一条记录，tqdm提供进度条显示\n",
    "            for item in tqdm(dataset):\n",
    "                # 过滤条件：只保留文章长度小于5000字符的记录\n",
    "                if len(item[\"Article\"]) < 5000:\n",
    "                    # 创建一个新的字典，只包含我们需要的字段\n",
    "                    filtered_item = {\n",
    "                        \"Article\": item[\"Article\"],    # 文章内容\n",
    "                        \"Summary\": item[\"Summary\"]     # 文章摘要\n",
    "                    }\n",
    "                    # 将过滤后的记录转换为JSON格式并写入文件\n",
    "                    # ensure_ascii=False确保中文字符正常显示，而不是转义序列\n",
    "                    f.write(json.dumps(filtered_item, ensure_ascii=False) + \"\\n\")\n",
    "                    filtered_count += 1  # 增加过滤后记录的计数\n",
    "        \n",
    "        # 打印处理结果的统计信息\n",
    "        print(f\"总数据量: {total_count}\")\n",
    "        print(f\"过滤后的数据量: {filtered_count}\")\n",
    "        print(f\"数据已保存到 {output_file}\")\n",
    "    else:\n",
    "        # 如果没有成功加载任何数据集文件，抛出异常\n",
    "        raise Exception(\"无法加载任何数据集文件\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 如果第一种方法失败，打印错误信息并尝试第二种方法\n",
    "    print(f\"使用 Dataset.from_file 方法失败: {e}\")\n",
    "    \n",
    "    # 第二种方法：如果上述方法失败，尝试使用 memory_mapped_arrow 格式\n",
    "    try:\n",
    "        print(\"\\n尝试另一种方法 - 直接读取内存映射的 arrow 文件...\")\n",
    "        # 导入datasets库的内部函数，用于连接多个数据集\n",
    "        from datasets.arrow_dataset import _concatenate_map_style_datasets\n",
    "        \n",
    "        # 定义一个函数来尝试读取 Arrow 文件\n",
    "        def try_load_arrow(file_path):\n",
    "            \"\"\"\n",
    "            尝试使用底层方法读取Arrow文件\n",
    "            参数: file_path - Arrow文件的完整路径\n",
    "            返回: Dataset对象或None（如果失败）\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # 使用 dataset 库的底层方法尝试读取\n",
    "                from datasets.arrow_dataset import ArrowReader\n",
    "                reader = ArrowReader(file_path)  # 创建Arrow文件读取器\n",
    "                dataset = reader.read()          # 读取数据集\n",
    "                return dataset\n",
    "            except Exception as e:\n",
    "                # 如果读取失败，打印错误信息并返回None\n",
    "                print(f\"无法读取 Arrow 文件 {file_path}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # 尝试读取所有的 Arrow 文件\n",
    "        datasets = []  # 创建空列表存储成功加载的数据集\n",
    "        # 遍历所有可能的Arrow文件名\n",
    "        for file_name in [\"financial_services_news_smr-train-00000-of-00002.arrow\", \n",
    "                         \"financial_services_news_smr-train-00001-of-00002.arrow\"]:\n",
    "            file_path = os.path.join(data_dir, file_name)  # 构建完整文件路径\n",
    "            if os.path.exists(file_path):                  # 检查文件是否存在\n",
    "                ds = try_load_arrow(file_path)             # 尝试加载文件\n",
    "                if ds is not None:                         # 如果加载成功\n",
    "                    datasets.append(ds)                    # 添加到数据集列表\n",
    "        \n",
    "        # 如果成功加载了至少一个数据集\n",
    "        if datasets:\n",
    "            # 合并数据集（如果有多个）\n",
    "            if len(datasets) > 1:\n",
    "                # 使用内部函数合并多个数据集\n",
    "                dataset = _concatenate_map_style_datasets(datasets)\n",
    "            else:\n",
    "                # 如果只有一个数据集，直接使用\n",
    "                dataset = datasets[0]\n",
    "                \n",
    "            # 这里应该有数据过滤和写入的代码，但原代码中用注释省略了\n",
    "            # 实际应用中，这里会包含与上面相同的数据处理逻辑\n",
    "        else:\n",
    "            # 如果没有成功加载任何Arrow文件，抛出异常\n",
    "            raise Exception(\"无法加载任何 Arrow 文件\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        # 如果第二种方法也失败，打印错误信息并尝试第三种方法\n",
    "        print(f\"使用备选方法也失败了: {e2}\")\n",
    "        \n",
    "        # 第三种方法：最后的方案，尝试使用原始 Hugging Face API 重新下载\n",
    "        print(\"\\n尝试使用 Hugging Face API 重新加载数据集...\")\n",
    "        try:\n",
    "            # 导入load_dataset函数，用于从Hugging Face Hub下载数据集\n",
    "            from datasets import load_dataset\n",
    "            \n",
    "            # 直接从 Hugging Face Hub 重新下载和加载数据集\n",
    "            # \"gunnybd01/Financial_Services_News_smr\" 是数据集在Hugging Face上的标识符\n",
    "            dataset = load_dataset(\"gunnybd01/Financial_Services_News_smr\")\n",
    "            \n",
    "            # 检查数据集是否包含训练集分割\n",
    "            if \"train\" in dataset:\n",
    "                train_dataset = dataset[\"train\"]           # 获取训练集部分\n",
    "                total_count = len(train_dataset)           # 获取训练集的总记录数\n",
    "                print(f\"直接从 Hugging Face 加载成功! 数据集大小: {total_count}\")\n",
    "                \n",
    "                # 过滤并写入数据（与第一种方法相同的逻辑）\n",
    "                filtered_count = 0  # 初始化过滤后记录计数器\n",
    "                \n",
    "                # 打开输出文件准备写入\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    # 遍历训练集中的每条记录\n",
    "                    for item in tqdm(train_dataset):\n",
    "                        # 过滤条件：文章长度小于14000字符（注意这里的阈值与第一种方法不同）\n",
    "                        if len(item[\"Article\"]) < 14000:\n",
    "                            # 创建包含所需字段的新字典\n",
    "                            filtered_item = {\n",
    "                                \"Article\": item[\"Article\"],    # 文章内容\n",
    "                                \"Summary\": item[\"Summary\"]     # 文章摘要\n",
    "                            }\n",
    "                            # 将记录转换为JSON格式并写入文件\n",
    "                            f.write(json.dumps(filtered_item, ensure_ascii=False) + \"\\n\")\n",
    "                            filtered_count += 1  # 增加过滤后记录计数\n",
    "                \n",
    "                # 打印最终的处理统计信息\n",
    "                print(f\"总数据量: {total_count}\")\n",
    "                print(f\"过滤后的数据量: {filtered_count}\")\n",
    "                print(f\"数据已保存到 {output_file}\")\n",
    "            \n",
    "        except Exception as e3:\n",
    "            # 如果所有方法都失败了，打印最终的错误信息\n",
    "            print(f\"所有尝试都失败了: {e3}\")\n",
    "            \n",
    "            # 给用户提供一些建议和可能的解决方案\n",
    "            print(\"\\n建议尝试以下方法:\")\n",
    "            print(\"1. 检查 dataset_info.json 文件中的详细内容，看是否有关于如何加载数据的提示\")\n",
    "            print(\"2. 尝试在环境中更新 datasets 库: pip install -U datasets\")\n",
    "            print(\"3. 尝试在全新环境中重新下载并加载数据集\")\n",
    "            print(\"4. 或者直接联系数据集作者询问正确的加载方式\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b18db6-669e-46e3-9e77-d7035f16e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 数据 1 =====\n",
      "Article (前100字符): The following are today's upgrades for Validea's Small-Cap Growth Investor model based on the publis...\n",
      "Summary: The article reports on Validea's upgrades for its Small-Cap Growth Investor model, based on the Motley Fool strategy.  The sentiment is positive, focused on stock valuations and underlying fundamentals.  The analysis suggests that Atlanticus Holdings Corp (ATLC) has improved from 65% to 79% based on positive factors like passing profit margin, insider holdings, and cash flow.  Triumph Financial Inc (TFIN) has also seen an improvement from 73% to 80%.  Mueller Industries Inc (MLI) has seen a slight improvement from 59% to 72%.  Key catalysts include strong underlying fundamentals, stock valuation, and positive performance metrics such as profit margin, insider holdings, and cash flow.  Risk factors are implied by the mixed results across the analyzed companies;  some criteria, such as relative strength and sales growth, failed for certain stocks, which suggests that certain criteria are not equally weighted.  The article highlights the criteria used to assess the stocks, including profit margin, relative strength, sales and EPS growth, insider holdings, cash flow, and others. The analysis emphasizes the use of published investment strategies and guru recommendations by Validea.\n",
      "\n",
      "\n",
      "===== 数据 2 =====\n",
      "Article (前100字符): Investors will likely remember 2023 as the year artificial intelligence (AI) found footing in the ma...\n",
      "Summary: The article expresses a cautiously optimistic sentiment about Verizon (VZ) stock for 2024.  While the stock has declined 35% over five years due to slow growth, competition, and substantial capital expenditures,  the news highlights potential catalysts that could drive the stock higher. The key driver is the anticipated reduction in Federal Reserve interest rates, potentially easing Verizon's heavy debt burden. Lower interest rates could also improve Verizon's revenue prospects as businesses resume spending.  Verizon's consistently high network quality ranking (No. 1 for 31 consecutive times by J.D. Power) and its crucial role in supporting AI-driven activities are also positive factors.  The article also emphasizes the potential for AI-driven applications to significantly increase revenue for Verizon.  Risk factors include the substantial debt load (nearly $147 billion) and the need for refinancing at potentially higher rates unless the dividend is reduced or eliminated.  The article concludes with a cautionary note that Verizon wasn't on a list of top stocks.\n",
      "\n",
      "\n",
      "===== 数据 3 =====\n",
      "Article (前100字符): What a year for Microsoft (NASDAQ: MSFT). The tech stalwart kicked off 2023 by hitting a 52-week low...\n",
      "Summary: The article presents a strongly positive sentiment toward Microsoft (MSFT) stock.  Evidence includes Microsoft's impressive financial performance in its fiscal 2024 first quarter, with double-digit year-over-year revenue growth ($56.5 billion) and a 27% increase in net income ($22.3 billion).  The article highlights sustained multi-year revenue growth fueled by strong positions in cloud computing and AI.   Key catalysts are Microsoft's leadership in cloud computing (Azure) and AI technologies, its expansive global data center network, and exclusive partnerships like the one with Oracle.   Significant mentions of the AI capabilities, positive comments from CEO Satya Nadella and CFO Amy Hood, and continued strong financial strength with substantial assets and manageable liabilities all contribute to the positive sentiment.  The acquisition of Activision Blizzard is also a key driver for future revenue in the gaming division. The article implicitly suggests that potential risk factors are mitigated by Microsoft's strong financial health, but mentions that Microsoft was not on a list of recommended stocks.\n",
      "\n",
      "\n",
      "===== 数据 4 =====\n",
      "Article (前100字符): There's no doubt that 2022 will go down in history as one of the toughest years on record for Wall S...\n",
      "Summary: The article expresses a positive sentiment about Amazon (AMZN), highlighting the potential for continued gains following a significant market rebound in 2023.  The article points to historical data suggesting that the Nasdaq and tech-heavy stocks, following market rebounds, typically continue to rise, with an average of 19% gains.  The article suggests the recent resurgence of stock splits is a positive indicator of future growth potential, citing Amazon's split in 2022.  Amazon's significant presence in the AI market and its related strategies are key drivers.  The article emphasizes Amazon's existing and ongoing AI development, including services like AWS Bedrock and Titan, positioning it to take advantage of the AI revolution.  Amazon's forward sales valuation appears undervalued compared to the company's historical average, further emphasizing the stock's potential.  Risk factors are not explicitly addressed in the article. However, the article implies potential risks are mitigated by Amazon's established strength and future-proof AI strategy.  The article concludes by acknowledging that the stock is not among a list of recommended stocks.\n",
      "\n",
      "\n",
      "===== 数据 5 =====\n",
      "Article (前100字符): For a Reuters live blog on U.S., UK and European stock markets, click LIVE/ or type LIVE/ in a news ...\n",
      "Summary: The article presents a positive sentiment toward the U.S. stock market, with futures and indices trending upward. The optimism is based on investor expectations for a Federal Reserve policy pivot toward interest rate cuts in 2024. This expectation, stemming from comments by Fed Chair Jerome Powell, has translated into a 67% chance of a 25-basis-point rate cut in March 2024.   Key drivers include improved inflation readings and statements from Fed officials signaling rate cuts.  The article also mentions a long winning streak for the S&P 500, highlighting a positive market trend.  Risk factors are not central to the article, but some potential concerns are hinted at with a focus on upcoming economic data releases (third-quarter GDP and PCE index).  The article does briefly discuss a few negative stock movements, indicating specific concerns about Apple's pause in US sales of smartwatches (patent dispute), PepsiCo (downgrade), and Plug Power (downgrade). However, the overall tone is overwhelmingly positive toward the market, primarily due to the anticipated Fed rate cuts.\n",
      "\n",
      "\n",
      "===== 数据 6 =====\n",
      "Article (前100字符): Warren Buffett has led the Berkshire Hathaway (NYSE: BRK.A)(NYSE: BRK.B) for more than 50 years. Bet...\n",
      "Summary: The article analyzes Berkshire Hathaway's (BRK.A/B) significant investment in AI-related stocks, specifically Snowflake, Amazon, and Apple.  The sentiment is overwhelmingly positive, highlighting Buffett's long-term investment strategy focused on profitable, growing companies with strong management.  Evidence includes the substantial returns Berkshire Hathaway has achieved over 50 years, and its portfolio's significant holdings in these three companies, particularly Apple, which accounts for a majority of the publicly traded portion.  The article identifies the companies' future AI growth potential,  specifically Snowflake's new AI tools, Amazon's AWS expansion in AI, and Apple's advancements in AI-powered devices, as key catalysts for continued positive performance.  Risk factors are implied, mostly in the form of concerns about  potential future competition in the AI sector from emerging players like Intel and Google, though the article ultimately argues this does not drastically threaten Berkshire's position.\n",
      "\n",
      "\n",
      "===== 数据 7 =====\n",
      "Article (前100字符): By Allison Lampert\n",
      "MONTREAL, Dec 19 (Reuters) - Aerospace supplier CEO Hugue Meloche spends more tha...\n",
      "Summary: The aerospace sector faces a critical labor shortage, impacting smaller suppliers more significantly than larger companies like plane and engine manufacturers.  The sentiment is cautious but also highlights the determination of companies to address the issue.  Evidence shows the significant expense (over C$10,000 per foreign worker) associated with recruiting skilled foreign labor, particularly for smaller suppliers. Key catalysts include booming client orders, retirements during the COVID-19 pandemic, and the persistence of supply chain issues into 2025.  The risk factors are primarily the substantial costs incurred in relocating and supporting foreign workers for smaller suppliers, which can affect profitability, competitive pricing, and production demands.  There's also the pressure of maintaining strong production and competitive pricing against larger companies who can better attract and retain domestic talent. This may lead to smaller suppliers potentially passing on increased costs to customers or reducing costs in other areas.\n",
      "\n",
      "\n",
      "===== 数据 8 =====\n",
      "Article (前100字符): By Anirban Sen and Milana Vinn\n",
      "NEW YORK, Dec 19 (Reuters) - ServiceTitan Inc, a Los Angeles-based st...\n",
      "Summary: The article expresses a cautiously optimistic sentiment towards ServiceTitan's potential IPO in 2024.  The company, which makes software for contract workers, is reviving IPO preparations after previous market conditions led to a delay.  Evidence suggests that ServiceTitan, along with other companies, is anticipating a possible revival of the IPO market in the first half of 2024. Key catalysts include positive business trends, evidenced by the company's substantial growth in a niche HVAC market,  with more than 11,800 client businesses. Positive investor sentiment, with a diverse group of established venture capital investors, also points to potential market interest.  Risk factors are implied and center around the recent unpredictable market performance and the potential challenges of a fluctuating IPO market.\n",
      "\n",
      "\n",
      "===== 数据 9 =====\n",
      "Article (前100字符): Investors suspected that a special dividend was coming soon, but Costco Wholesale (NASDAQ: COST) has...\n",
      "Summary: The article expresses a positive sentiment towards Costco's upcoming special dividend, but also emphasizes the stock's less predictable dividend growth compared to competitors like Walmart and Target.  Evidence includes Costco's substantial cash holdings ($17+ billion) and positive comparable-store sales growth (3% US, 4% globally), robust customer traffic (5% growth), and record-high renewal rates (92.8%).  Key catalysts include strong sales and earnings trends, driven by membership and merchandise offerings.  Risk factors are implied, as the article cautions against viewing Costco as a dependable income investment solely based on the special dividend, contrasting it with the more predictable dividend growth of competitors.  The unpredictable nature of special dividends and the slower, more occasional dividend growth might be a concern to income investors who prefer more reliable sources of dividend income.\n",
      "\n",
      "\n",
      "===== 数据 10 =====\n",
      "Article (前100字符): Prepare the crown for Nvidia (NASDAQ: NVDA). The high-flying stock is almost certainly going to be t...\n",
      "Summary: The article presents a mixed sentiment regarding Nvidia's (NVDA) prospects for 2024. While acknowledging Nvidia's likely strong performance in 2023 due to high GPU demand, the article points to potential challenges and risk factors for 2024.  Evidence includes Intel's upcoming Gaudi3 and Core Ultra chips, which are designed to compete with Nvidia's AI chips, and Google's new Cloud TPU v5p, which may reduce reliance on Nvidia's technology.  Key catalysts for positive sentiment were the previous strong demand and limited supply for Nvidia's GPUs.  Risk factors stem from Intel and Google's emerging competition, suggesting a potential decline in demand for Nvidia's products.  The article implies that while Nvidia is expected to remain a dominant player, increased competition could diminish growth compared to the exceptionally high growth seen in 2023.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 指定文件路径\n",
    "file_path = \"/root/autodl-tmp/data/filtered_financial_news_5k.jsonl\"\n",
    "\n",
    "# 读取并显示前10条数据\n",
    "count = 0\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if count < 10:\n",
    "            data = json.loads(line)\n",
    "            print(f\"===== 数据 {count+1} =====\")\n",
    "            # 显示 Article 的前100个字符\n",
    "            print(f\"Article (前100字符): {data['Article'][:100]}...\")\n",
    "            print(f\"Summary: {data['Summary']}\")\n",
    "            print(\"\\n\")\n",
    "            count += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fc349",
   "metadata": {},
   "source": [
    "### 创建数据集策略\n",
    "\n",
    "### 1. **两阶段生成策略**\n",
    "```python\n",
    "# 特色1：分步骤生成，质量更高\n",
    "第一阶段：英文文章 → 中文问题（信息提取和转换）\n",
    "第二阶段：中文问题 → 带思考过程的答案（推理和分析）\n",
    "\n",
    "# 优势：\n",
    "- 避免直接翻译，而是重新组织信息\n",
    "- 确保问题的独立性和完整性\n",
    "- 生成的数据更适合训练推理能力\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd57d8",
   "metadata": {},
   "source": [
    "### 2. **思考过程训练格式**\n",
    "```python\n",
    "# 特色2：CoT (Chain of Thought) 格式\n",
    "输出格式：\n",
    "<think>\n",
    "详细的分析思考过程...\n",
    "</think>\n",
    "\n",
    "最终的专业结论...\n",
    "\n",
    "# 优势：\n",
    "- 训练模型学会逐步推理\n",
    "- 提高模型的可解释性\n",
    "- 模拟人类专家的思考方式\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5efb1",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **高质量数据过滤**\n",
    "```python\n",
    "# 特色3：多层质量控制\n",
    "- 文本长度过滤（避免过长文章）\n",
    "- JSON格式验证\n",
    "- 必要字段检查\n",
    "- API调用错误处理\n",
    "\n",
    "# 结果：确保生成的每条数据都是高质量的\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0817a79",
   "metadata": {},
   "source": [
    "\n",
    "### 4. **分层随机采样**\n",
    "```python\n",
    "# 特色4：智能采样策略\n",
    "- 不是简单随机选择\n",
    "- 将数据分成多个区间\n",
    "- 从每个区间随机选择\n",
    "- 既保证代表性又保持随机性\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03eabb1",
   "metadata": {},
   "source": [
    "\n",
    "### 5.随机种子的基本原理：\n",
    "```python\n",
    "# 1. 什么是随机种子？\n",
    "随机种子 = 伪随机数生成器的起始值\n",
    "\n",
    "# 2. 为什么需要随机种子？\n",
    "计算机的\"随机\"实际上是伪随机：\n",
    "- 通过数学算法生成看似随机的数列\n",
    "- 给定相同的种子，会产生相同的\"随机\"序列\n",
    "- 这样可以让\"随机\"过程变得可重现\n",
    "\n",
    "# 3. 随机种子的作用\n",
    "random.seed(57)  # 设置种子为57\n",
    "# 之后所有的random操作都会产生相同的结果\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9aad99",
   "metadata": {},
   "source": [
    "\n",
    "### 6.在当前代码中的应用：\n",
    "```python\n",
    "# 1. 全局设置\n",
    "RANDOM_SEED = 57        # 定义种子值\n",
    "random.seed(RANDOM_SEED) # 设置种子\n",
    "\n",
    "# 2. 影响的操作\n",
    "- random.sample()：数据采样时选择相同的数据\n",
    "- random.uniform()：API请求延迟时间相同\n",
    "- stratified_random_sample()：分层采样选择相同的数据\n",
    "\n",
    "# 3. 实际效果\n",
    "每次运行程序：\n",
    "- 会选择相同的5000条文章\n",
    "- API请求的延迟模式相同\n",
    "- 最终生成的数据集完全一致\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c382383",
   "metadata": {},
   "source": [
    "\n",
    "### 可重现性的重要意义：\n",
    "```python\n",
    "# 1. 科学研究\n",
    "- 实验结果可以被其他人重复\n",
    "- 便于调试和问题定位\n",
    "- 确保研究的可信度\n",
    "\n",
    "# 2. 生产环境\n",
    "- 便于版本控制和回滚\n",
    "- 团队协作时结果一致\n",
    "- 便于A/B测试对比\n",
    "\n",
    "# 3. 调试开发\n",
    "- 问题可以稳定重现\n",
    "- 便于测试代码修改效果\n",
    "- 减少随机因素干扰\n",
    "```\n",
    "\n",
    "这个代码的核心价值在于：**将原始的英文金融新闻转换为高质量的中文SFT训练数据，同时保证整个过程的可重现性和数据质量**！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187902f",
   "metadata": {},
   "source": [
    "## 2.SFT单轮数据构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cf480",
   "metadata": {},
   "source": [
    "单轮对话标准的SFT数据格式生成，用做后续训练。共计生成 train 数据1.5w条，test数据1.1k eval数据1k 分别适用不同随机种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc211bd-f2df-453a-a9c4-ec0df978df10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文件大小: 278687859 字节\n",
      "使用随机种子: 57 确保可复现性\n",
      "文件共有 70061 行\n",
      "解析错误: 0 行\n",
      "成功读取: 70061 条有效记录\n",
      "最终收集: 70061 条记录\n",
      "分层随机采样了 5000/70061 条记录\n",
      "开始并行处理，最大线程数: 80\n",
      "处理文章 55/5000\n",
      "处理文章 48/5000\n",
      "处理文章 9/5000\n",
      "处理文章 47/5000\n",
      "处理文章 74/5000\n",
      "处理文章 35/5000\n",
      "处理文章 67/5000\n",
      "处理文章 16/5000\n",
      "处理文章 32/5000\n",
      "处理文章 28/5000\n",
      "处理文章 12/5000\n",
      "处理文章 3/5000\n",
      "处理文章 25/5000\n",
      "处理文章 14/5000\n",
      "处理文章 10/5000\n",
      "处理文章 39/5000\n",
      "处理文章 46/5000\n",
      "处理文章 75/5000\n",
      "处理文章 19/5000\n",
      "处理文章 43/5000\n",
      "处理文章 23/5000\n",
      "处理文章 21/5000\n",
      "处理文章 38/5000\n",
      "处理文章 78/5000\n",
      "处理文章 45/5000\n",
      "处理文章 71/5000\n",
      "处理文章 68/5000\n",
      "处理文章 60/5000\n",
      "处理文章 30/5000\n",
      "处理文章 40/5000\n",
      "处理文章 54/5000\n",
      "处理文章 70/5000\n",
      "处理文章 26/5000\n",
      "处理文章 66/5000\n",
      "处理文章 17/5000\n",
      "处理文章 15/5000\n",
      "处理文章 63/5000\n",
      "处理文章 7/5000\n",
      "处理文章 52/5000\n",
      "处理文章 57/5000\n",
      "处理文章 49/5000\n",
      "处理文章 72/5000\n",
      "处理文章 50/5000\n",
      "处理文章 33/5000\n",
      "处理文章 42/5000\n",
      "处理文章 34/5000\n",
      "处理文章 76/5000\n",
      "处理文章 20/5000\n",
      "处理文章 58/5000\n",
      "处理文章 53/5000\n",
      "处理文章 69/5000\n",
      "处理文章 11/5000\n",
      "处理文章 44/5000\n",
      "处理文章 13/5000\n",
      "处理文章 61/5000\n",
      "处理文章 22/5000\n",
      "处理文章 59/5000\n",
      "处理文章 29/5000\n",
      "处理文章 80/5000\n",
      "处理文章 62/5000\n",
      "处理文章 64/5000\n",
      "处理文章 31/5000\n",
      "处理文章 41/5000\n",
      "处理文章 36/5000\n",
      "处理文章 18/5000\n",
      "处理文章 4/5000\n",
      "处理文章 2/5000\n",
      "处理文章 73/5000\n",
      "处理文章 77/5000\n",
      "处理文章 37/5000\n",
      "处理文章 27/5000\n",
      "处理文章 6/5000\n",
      "处理文章 51/5000\n",
      "处理文章 79/5000\n",
      "处理文章 1/5000\n",
      "处理文章 8/5000\n",
      "处理文章 56/5000\n",
      "处理文章 65/5000\n",
      "处理文章 5/5000\n",
      "处理文章 24/5000\n",
      "处理文章 81/5000\n",
      "处理文章 82/5000\n",
      "处理文章 83/5000\n",
      "处理文章 85/5000\n",
      "处理文章 84/5000\n",
      "处理文章 86/5000\n",
      "处理文章 88/5000\n",
      "处理文章 87/5000\n",
      "处理文章 89/5000\n",
      "处理文章 90/5000\n",
      "处理文章 91/5000\n",
      "处理文章 93/5000\n",
      "处理文章 94/5000\n",
      "处理文章 92/5000\n",
      "处理文章 95/5000\n",
      "处理文章 96/5000\n",
      "处理文章 97/5000\n",
      "处理文章 98/5000\n",
      "处理文章 99/5000\n",
      "处理文章 104/5000\n",
      "处理文章 100/5000\n",
      "处理文章 101/5000\n",
      "处理文章 102/5000\n",
      "处理文章 107/5000\n",
      "处理文章 105/5000\n",
      "处理文章 103/5000\n",
      "处理文章 106/5000\n",
      "处理文章 108/5000\n",
      "处理文章 109/5000\n",
      "处理文章 110/5000\n",
      "处理文章 111/5000\n",
      "处理文章 112/5000\n",
      "处理文章 113/5000\n",
      "处理文章 115/5000\n",
      "处理文章 114/5000\n",
      "处理文章 118/5000\n",
      "处理文章 117/5000\n",
      "处理文章 116/5000\n",
      "处理文章 119/5000\n",
      "处理文章 120/5000\n",
      "处理文章 124/5000\n",
      "处理文章 123/5000\n",
      "处理文章 121/5000\n",
      "处理文章 122/5000\n",
      "处理文章 128/5000\n",
      "处理文章 125/5000\n",
      "处理文章 129/5000\n",
      "处理文章 126/5000\n",
      "处理文章 132/5000\n",
      "处理文章 127/5000\n",
      "处理文章 131/5000\n",
      "处理文章 130/5000\n",
      "处理文章 133/5000\n",
      "处理文章 134/5000\n",
      "处理文章 138/5000\n",
      "处理文章 135/5000\n",
      "处理文章 136/5000\n",
      "处理文章 137/5000\n",
      "处理文章 140/5000\n",
      "处理文章 139/5000\n",
      "处理文章 141/5000\n",
      "处理文章 142/5000\n",
      "处理文章 143/5000\n",
      "处理文章 145/5000\n",
      "处理文章 144/5000\n",
      "处理文章 146/5000\n",
      "处理文章 147/5000\n",
      "处理文章 148/5000\n",
      "处理文章 149/5000\n",
      "处理文章 150/5000\n",
      "处理文章 151/5000\n",
      "处理文章 152/5000\n",
      "处理文章 153/5000\n",
      "处理文章 154/5000\n",
      "处理文章 156/5000\n",
      "处理文章 155/5000\n",
      "处理文章 158/5000\n",
      "处理文章 157/5000\n",
      "处理文章 159/5000\n",
      "处理文章 160/5000\n",
      "处理文章 161/5000\n",
      "处理文章 162/5000\n",
      "处理文章 163/5000\n",
      "处理文章 165/5000\n",
      "处理文章 164/5000\n",
      "处理文章 166/5000\n",
      "处理文章 167/5000\n",
      "处理文章 168/5000\n",
      "处理文章 169/5000\n",
      "处理文章 170/5000\n",
      "处理文章 172/5000\n",
      "处理文章 171/5000\n",
      "处理文章 173/5000\n",
      "处理文章 174/5000\n",
      "处理文章 175/5000\n",
      "处理文章 177/5000\n",
      "处理文章 176/5000\n",
      "处理文章 178/5000\n",
      "处理文章 180/5000\n",
      "处理文章 179/5000\n",
      "处理文章 182/5000\n",
      "处理文章 181/5000\n",
      "处理文章 183/5000\n",
      "处理文章 184/5000\n",
      "处理文章 187/5000\n",
      "处理文章 186/5000\n",
      "处理文章 185/5000\n",
      "处理文章 189/5000\n",
      "处理文章 188/5000\n",
      "处理文章 190/5000\n",
      "处理文章 191/5000\n",
      "处理文章 192/5000\n",
      "处理文章 194/5000\n",
      "处理文章 193/5000\n",
      "处理文章 196/5000\n",
      "处理文章 195/5000\n",
      "处理文章 197/5000\n",
      "处理文章 199/5000\n",
      "处理文章 198/5000\n",
      "处理文章 200/5000\n",
      "处理文章 204/5000\n",
      "处理文章 207/5000\n",
      "处理文章 203/5000\n",
      "处理文章 205/5000\n",
      "处理文章 201/5000\n",
      "处理文章 202/5000\n",
      "处理文章 206/5000\n",
      "处理文章 208/5000\n",
      "处理文章 212/5000\n",
      "处理文章 213/5000\n",
      "处理文章 209/5000\n",
      "处理文章 211/5000\n",
      "处理文章 210/5000\n",
      "处理文章 214/5000\n",
      "处理文章 216/5000\n",
      "处理文章 215/5000\n",
      "处理文章 219/5000\n",
      "处理文章 218/5000\n",
      "处理文章 217/5000\n",
      "处理文章 221/5000\n",
      "处理文章 220/5000\n",
      "处理文章 222/5000\n",
      "处理文章 223/5000\n",
      "处理文章 224/5000\n",
      "处理文章 225/5000\n",
      "处理文章 227/5000\n",
      "处理文章 226/5000\n",
      "处理文章 230/5000\n",
      "处理文章 229/5000\n",
      "处理文章 231/5000\n",
      "处理文章 228/5000\n",
      "处理文章 232/5000\n",
      "处理文章 233/5000\n",
      "处理文章 235/5000\n",
      "处理文章 234/5000\n",
      "处理文章 236/5000\n",
      "处理文章 237/5000\n",
      "处理文章 239/5000\n",
      "处理文章 238/5000\n",
      "处理文章 240/5000\n",
      "处理文章 241/5000\n",
      "处理文章 242/5000\n",
      "处理文章 243/5000\n",
      "处理文章 244/5000\n",
      "处理文章 245/5000\n",
      "处理文章 246/5000\n",
      "处理文章 247/5000\n",
      "处理文章 248/5000\n",
      "处理文章 249/5000\n",
      "处理文章 250/5000\n",
      "处理文章 251/5000\n",
      "处理文章 252/5000\n",
      "处理文章 253/5000\n",
      "处理文章 254/5000\n",
      "处理文章 257/5000\n",
      "处理文章 255/5000\n",
      "处理文章 258/5000\n",
      "处理文章 256/5000\n",
      "处理文章 259/5000\n",
      "处理文章 260/5000\n",
      "处理文章 262/5000\n",
      "处理文章 261/5000\n",
      "处理文章 263/5000\n",
      "处理文章 264/5000\n",
      "处理文章 265/5000\n",
      "处理文章 266/5000\n",
      "处理文章 267/5000\n",
      "处理文章 268/5000\n",
      "处理文章 269/5000\n",
      "处理文章 271/5000\n",
      "处理文章 270/5000\n",
      "处理文章 273/5000\n",
      "处理文章 272/5000\n",
      "处理文章 274/5000\n",
      "处理文章 276/5000\n",
      "处理文章 278/5000\n",
      "处理文章 277/5000\n",
      "处理文章 275/5000\n",
      "处理文章 279/5000\n",
      "处理文章 281/5000\n",
      "处理文章 282/5000\n",
      "处理文章 285/5000\n",
      "处理文章 280/5000\n",
      "处理文章 284/5000\n",
      "处理文章 283/5000\n",
      "处理文章 286/5000\n",
      "处理文章 287/5000\n",
      "处理文章 289/5000\n",
      "处理文章 288/5000\n",
      "处理文章 290/5000\n",
      "处理文章 291/5000\n",
      "处理文章 292/5000\n",
      "处理文章 293/5000\n",
      "处理文章 294/5000\n",
      "处理文章 295/5000\n",
      "处理文章 296/5000\n",
      "处理文章 298/5000\n",
      "处理文章 297/5000\n",
      "处理文章 299/5000\n",
      "处理文章 300/5000\n",
      "处理文章 301/5000\n",
      "处理文章 304/5000\n",
      "处理文章 302/5000\n",
      "处理文章 303/5000\n",
      "处理文章 305/5000\n",
      "处理文章 306/5000\n",
      "处理文章 309/5000\n",
      "处理文章 307/5000\n",
      "处理文章 308/5000\n",
      "处理文章 310/5000\n",
      "处理文章 311/5000\n",
      "处理文章 312/5000\n",
      "处理文章 313/5000\n",
      "处理文章 315/5000\n",
      "处理文章 314/5000\n",
      "处理文章 316/5000\n",
      "处理文章 317/5000\n",
      "处理文章 318/5000\n",
      "处理文章 319/5000\n",
      "处理文章 320/5000\n",
      "处理文章 321/5000\n",
      "处理文章 322/5000\n",
      "处理文章 323/5000\n",
      "处理文章 324/5000\n",
      "处理文章 325/5000\n",
      "处理文章 326/5000\n",
      "处理文章 327/5000\n",
      "处理文章 328/5000\n",
      "处理文章 329/5000\n",
      "处理文章 330/5000\n",
      "处理文章 331/5000\n",
      "处理文章 333/5000\n",
      "处理文章 332/5000\n",
      "处理文章 334/5000\n",
      "处理文章 337/5000\n",
      "处理文章 335/5000\n",
      "处理文章 336/5000\n",
      "处理文章 338/5000\n",
      "处理文章 339/5000\n",
      "处理文章 340/5000\n",
      "处理文章 344/5000\n",
      "处理文章 343/5000\n",
      "处理文章 342/5000\n",
      "处理文章 341/5000\n",
      "处理文章 345/5000\n",
      "处理文章 346/5000\n",
      "处理文章 347/5000\n",
      "处理文章 348/5000\n",
      "处理文章 349/5000\n",
      "处理文章 350/5000\n",
      "处理文章 351/5000\n",
      "处理文章 352/5000\n",
      "处理文章 354/5000\n",
      "处理文章 353/5000\n",
      "处理文章 355/5000\n",
      "处理文章 357/5000\n",
      "处理文章 356/5000\n",
      "处理文章 359/5000\n",
      "处理文章 358/5000\n",
      "处理文章 360/5000\n",
      "处理文章 361/5000\n",
      "处理文章 362/5000\n",
      "处理文章 366/5000\n",
      "处理文章 363/5000\n",
      "处理文章 365/5000\n",
      "处理文章 364/5000\n",
      "处理文章 368/5000\n",
      "处理文章 367/5000\n",
      "处理文章 369/5000\n",
      "处理文章 371/5000\n",
      "处理文章 370/5000\n",
      "处理文章 372/5000\n",
      "处理文章 373/5000\n",
      "处理文章 374/5000\n",
      "处理文章 375/5000\n",
      "处理文章 376/5000\n",
      "处理文章 379/5000\n",
      "处理文章 380/5000\n",
      "处理文章 377/5000\n",
      "处理文章 378/5000\n",
      "处理文章 382/5000\n",
      "处理文章 381/5000\n",
      "处理文章 383/5000\n",
      "处理文章 384/5000\n",
      "处理文章 385/5000\n",
      "处理文章 386/5000\n",
      "处理文章 387/5000\n",
      "处理文章 388/5000\n",
      "处理文章 389/5000\n",
      "处理文章 390/5000\n",
      "处理文章 391/5000\n",
      "处理文章 392/5000\n",
      "处理文章 393/5000\n",
      "处理文章 394/5000\n",
      "处理文章 395/5000\n",
      "处理文章 397/5000\n",
      "处理文章 396/5000\n",
      "处理文章 398/5000\n",
      "处理文章 399/5000\n",
      "处理文章 400/5000\n",
      "处理文章 401/5000\n",
      "处理文章 402/5000\n",
      "处理文章 403/5000\n",
      "处理文章 404/5000\n",
      "处理文章 405/5000\n",
      "处理文章 406/5000\n",
      "处理文章 408/5000\n",
      "处理文章 407/5000\n",
      "处理文章 409/5000\n",
      "处理文章 410/5000\n",
      "处理文章 411/5000\n",
      "处理文章 412/5000\n",
      "处理文章 414/5000\n",
      "处理文章 413/5000\n",
      "处理文章 415/5000\n",
      "处理文章 416/5000\n",
      "处理文章 418/5000\n",
      "处理文章 419/5000\n",
      "处理文章 417/5000\n",
      "处理文章 420/5000\n",
      "处理文章 421/5000\n",
      "处理文章 424/5000\n",
      "处理文章 425/5000\n",
      "处理文章 422/5000\n",
      "处理文章 423/5000\n",
      "处理文章 426/5000\n",
      "处理文章 427/5000\n",
      "处理文章 429/5000\n",
      "处理文章 428/5000\n",
      "处理文章 430/5000\n",
      "处理文章 432/5000\n",
      "处理文章 433/5000\n",
      "处理文章 431/5000\n",
      "处理文章 435/5000\n",
      "处理文章 434/5000\n",
      "处理文章 436/5000\n",
      "处理文章 437/5000\n",
      "处理文章 438/5000\n",
      "处理文章 441/5000\n",
      "处理文章 440/5000\n",
      "处理文章 439/5000\n",
      "处理文章 442/5000\n",
      "处理文章 443/5000\n",
      "处理文章 444/5000\n",
      "处理文章 445/5000\n",
      "处理文章 447/5000\n",
      "处理文章 446/5000\n",
      "处理文章 448/5000\n",
      "处理文章 449/5000\n",
      "处理文章 450/5000\n",
      "处理文章 451/5000\n",
      "处理文章 452/5000\n",
      "处理文章 453/5000\n",
      "处理文章 454/5000\n",
      "处理文章 455/5000\n",
      "处理文章 458/5000\n",
      "处理文章 456/5000\n",
      "处理文章 457/5000\n",
      "处理文章 459/5000\n",
      "处理文章 460/5000\n",
      "处理文章 462/5000\n",
      "处理文章 461/5000\n",
      "处理文章 463/5000\n",
      "处理文章 464/5000\n",
      "处理文章 465/5000\n",
      "处理文章 466/5000\n",
      "处理文章 468/5000\n",
      "处理文章 467/5000\n",
      "处理文章 469/5000\n",
      "处理文章 470/5000\n",
      "处理文章 471/5000\n",
      "处理文章 472/5000\n",
      "处理文章 473/5000\n",
      "处理文章 474/5000\n",
      "处理文章 475/5000\n",
      "处理文章 476/5000\n",
      "处理文章 477/5000\n",
      "处理文章 478/5000\n",
      "处理文章 479/5000\n",
      "处理文章 480/5000\n",
      "处理文章 481/5000\n",
      "处理文章 482/5000\n",
      "处理文章 483/5000\n",
      "处理文章 484/5000\n",
      "处理文章 485/5000\n",
      "处理文章 486/5000\n",
      "处理文章 487/5000\n",
      "处理文章 488/5000\n",
      "处理文章 489/5000\n",
      "处理文章 490/5000\n",
      "处理文章 491/5000\n",
      "处理文章 493/5000\n",
      "处理文章 494/5000\n",
      "处理文章 492/5000\n",
      "处理文章 495/5000\n",
      "处理文章 496/5000\n",
      "处理文章 497/5000\n",
      "处理文章 499/5000\n",
      "处理文章 500/5000\n",
      "处理文章 498/5000\n",
      "处理文章 502/5000\n",
      "处理文章 503/5000\n",
      "处理文章 501/5000\n",
      "处理文章 505/5000\n",
      "处理文章 507/5000\n",
      "处理文章 504/5000\n",
      "处理文章 506/5000\n",
      "处理文章 510/5000\n",
      "处理文章 511/5000\n",
      "处理文章 508/5000\n",
      "处理文章 509/5000\n",
      "处理文章 512/5000\n",
      "处理文章 513/5000\n",
      "处理文章 514/5000\n",
      "处理文章 515/5000\n",
      "处理文章 517/5000\n",
      "处理文章 516/5000\n",
      "处理文章 518/5000\n",
      "处理文章 519/5000\n",
      "处理文章 521/5000\n",
      "处理文章 520/5000\n",
      "处理文章 522/5000\n",
      "处理文章 523/5000\n",
      "处理文章 524/5000\n",
      "处理文章 527/5000\n",
      "处理文章 525/5000\n",
      "处理文章 526/5000\n",
      "处理文章 528/5000\n",
      "处理文章 529/5000\n",
      "处理文章 530/5000\n",
      "处理文章 531/5000\n",
      "处理文章 532/5000\n",
      "处理文章 533/5000\n",
      "处理文章 534/5000\n",
      "处理文章 536/5000\n",
      "处理文章 535/5000\n",
      "处理文章 537/5000\n",
      "处理文章 538/5000\n",
      "处理文章 539/5000\n",
      "处理文章 540/5000\n",
      "处理文章 541/5000\n",
      "处理文章 542/5000\n",
      "处理文章 545/5000\n",
      "处理文章 543/5000\n",
      "处理文章 546/5000\n",
      "处理文章 544/5000\n",
      "处理文章 547/5000\n",
      "处理文章 548/5000\n",
      "处理文章 550/5000\n",
      "处理文章 549/5000\n",
      "处理文章 551/5000\n",
      "处理文章 553/5000\n",
      "处理文章 552/5000\n",
      "处理文章 554/5000\n",
      "处理文章 555/5000\n",
      "处理文章 556/5000\n",
      "处理文章 557/5000\n",
      "处理文章 558/5000\n",
      "处理文章 559/5000\n",
      "处理文章 560/5000\n",
      "处理文章 562/5000\n",
      "处理文章 561/5000\n",
      "处理文章 563/5000\n",
      "处理文章 565/5000\n",
      "处理文章 564/5000\n",
      "处理文章 566/5000\n",
      "处理文章 569/5000\n",
      "处理文章 567/5000\n",
      "处理文章 568/5000\n",
      "处理文章 570/5000\n",
      "处理文章 571/5000\n",
      "处理文章 573/5000\n",
      "处理文章 572/5000\n",
      "处理文章 575/5000\n",
      "处理文章 574/5000\n",
      "处理文章 576/5000\n",
      "处理文章 577/5000\n",
      "处理文章 579/5000\n",
      "处理文章 578/5000\n",
      "处理文章 580/5000\n",
      "处理文章 581/5000\n",
      "处理文章 582/5000\n",
      "处理文章 584/5000\n",
      "处理文章 583/5000\n",
      "处理文章 585/5000\n",
      "处理文章 586/5000\n",
      "处理文章 587/5000\n",
      "处理文章 588/5000\n",
      "处理文章 589/5000\n",
      "处理文章 590/5000\n",
      "处理文章 592/5000\n",
      "处理文章 591/5000\n",
      "处理文章 593/5000\n",
      "处理文章 594/5000\n",
      "处理文章 596/5000\n",
      "处理文章 595/5000\n",
      "处理文章 597/5000\n",
      "处理文章 599/5000\n",
      "处理文章 598/5000\n",
      "处理文章 600/5000\n",
      "处理文章 601/5000\n",
      "处理文章 603/5000\n",
      "处理文章 602/5000\n",
      "处理文章 604/5000\n",
      "处理文章 606/5000\n",
      "处理文章 605/5000\n",
      "处理文章 607/5000\n",
      "处理文章 609/5000\n",
      "处理文章 610/5000\n",
      "处理文章 608/5000\n",
      "处理文章 611/5000\n",
      "处理文章 613/5000\n",
      "处理文章 612/5000\n",
      "处理文章 614/5000\n",
      "处理文章 615/5000\n",
      "处理文章 617/5000\n",
      "处理文章 616/5000\n",
      "处理文章 618/5000\n",
      "处理文章 619/5000\n",
      "处理文章 620/5000\n",
      "处理文章 621/5000\n",
      "处理文章 622/5000\n",
      "处理文章 624/5000\n",
      "处理文章 623/5000\n",
      "处理文章 625/5000\n",
      "处理文章 627/5000\n",
      "处理文章 626/5000\n",
      "处理文章 628/5000\n",
      "处理文章 629/5000\n",
      "处理文章 630/5000\n",
      "处理文章 631/5000\n",
      "处理文章 633/5000\n",
      "处理文章 632/5000\n",
      "处理文章 634/5000\n",
      "处理文章 635/5000\n",
      "处理文章 636/5000\n",
      "处理文章 637/5000\n",
      "处理文章 638/5000\n",
      "处理文章 639/5000\n",
      "处理文章 640/5000\n",
      "处理文章 641/5000\n",
      "处理文章 642/5000\n",
      "处理文章 643/5000\n",
      "处理文章 644/5000\n",
      "处理文章 645/5000\n",
      "处理文章 646/5000\n",
      "处理文章 647/5000\n",
      "处理文章 650/5000\n",
      "处理文章 648/5000\n",
      "处理文章 649/5000\n",
      "处理文章 653/5000\n",
      "处理文章 652/5000\n",
      "处理文章 651/5000\n",
      "处理文章 654/5000\n",
      "处理文章 655/5000\n",
      "处理文章 656/5000\n",
      "处理文章 658/5000\n",
      "处理文章 659/5000\n",
      "处理文章 657/5000\n",
      "处理文章 660/5000\n",
      "处理文章 661/5000\n",
      "处理文章 662/5000\n",
      "处理文章 664/5000\n",
      "处理文章 663/5000\n",
      "处理文章 665/5000\n",
      "处理文章 666/5000\n",
      "处理文章 667/5000\n",
      "处理文章 668/5000\n",
      "处理文章 669/5000\n",
      "处理文章 670/5000\n",
      "处理文章 671/5000\n",
      "处理文章 672/5000\n",
      "处理文章 673/5000\n",
      "处理文章 675/5000\n",
      "处理文章 674/5000\n",
      "处理文章 676/5000\n",
      "处理文章 677/5000\n",
      "处理文章 678/5000\n",
      "处理文章 679/5000\n",
      "处理文章 680/5000\n",
      "处理文章 681/5000\n",
      "处理文章 682/5000\n",
      "处理文章 683/5000\n",
      "处理文章 684/5000\n",
      "处理文章 685/5000\n",
      "处理文章 686/5000\n",
      "处理文章 687/5000\n",
      "处理文章 688/5000\n",
      "处理文章 690/5000\n",
      "处理文章 689/5000\n",
      "处理文章 691/5000\n",
      "处理文章 692/5000\n",
      "处理文章 693/5000\n",
      "处理文章 695/5000\n",
      "处理文章 694/5000\n",
      "处理文章 696/5000\n",
      "处理文章 698/5000\n",
      "处理文章 697/5000\n",
      "处理文章 699/5000\n",
      "处理文章 700/5000\n",
      "处理文章 701/5000\n",
      "处理文章 702/5000\n",
      "处理文章 703/5000\n",
      "处理文章 704/5000\n",
      "处理文章 705/5000\n",
      "处理文章 706/5000\n",
      "处理文章 707/5000\n",
      "处理文章 708/5000\n",
      "处理文章 709/5000\n",
      "处理文章 710/5000\n",
      "处理文章 711/5000\n",
      "处理文章 712/5000\n",
      "处理文章 713/5000\n",
      "处理文章 714/5000\n",
      "处理文章 715/5000\n",
      "处理文章 717/5000\n",
      "处理文章 716/5000\n",
      "处理文章 718/5000\n",
      "处理文章 720/5000\n",
      "处理文章 719/5000\n",
      "处理文章 721/5000\n",
      "处理文章 722/5000\n",
      "处理文章 723/5000\n",
      "处理文章 724/5000\n",
      "处理文章 725/5000\n",
      "处理文章 726/5000\n",
      "处理文章 727/5000\n",
      "处理文章 728/5000\n",
      "处理文章 729/5000\n",
      "处理文章 732/5000\n",
      "处理文章 731/5000\n",
      "处理文章 730/5000\n",
      "处理文章 733/5000\n",
      "处理文章 734/5000\n",
      "处理文章 735/5000\n",
      "处理文章 736/5000\n",
      "处理文章 737/5000\n",
      "处理文章 738/5000\n",
      "处理文章 739/5000\n",
      "处理文章 740/5000\n",
      "处理文章 741/5000\n",
      "处理文章 742/5000\n",
      "处理文章 743/5000\n",
      "处理文章 744/5000\n",
      "处理文章 745/5000\n",
      "处理文章 746/5000\n",
      "处理文章 747/5000\n",
      "处理文章 748/5000\n",
      "处理文章 749/5000\n",
      "处理文章 750/5000\n",
      "处理文章 751/5000\n",
      "处理文章 752/5000\n",
      "处理文章 753/5000\n",
      "处理文章 754/5000\n",
      "处理文章 755/5000\n",
      "处理文章 756/5000\n",
      "处理文章 757/5000\n",
      "处理文章 758/5000\n",
      "处理文章 759/5000\n",
      "处理文章 760/5000\n",
      "处理文章 761/5000\n",
      "处理文章 762/5000\n",
      "处理文章 763/5000\n",
      "处理文章 764/5000\n",
      "处理文章 765/5000\n",
      "处理文章 766/5000\n",
      "处理文章 767/5000\n",
      "处理文章 768/5000\n",
      "处理文章 772/5000\n",
      "处理文章 770/5000\n",
      "处理文章 769/5000\n",
      "处理文章 771/5000\n",
      "处理文章 773/5000\n",
      "处理文章 774/5000\n",
      "处理文章 776/5000\n",
      "处理文章 775/5000\n",
      "处理文章 777/5000\n",
      "处理文章 778/5000\n",
      "处理文章 779/5000\n",
      "处理文章 780/5000\n",
      "处理文章 781/5000\n",
      "处理文章 783/5000\n",
      "处理文章 784/5000\n",
      "处理文章 782/5000\n",
      "处理文章 785/5000\n",
      "处理文章 786/5000\n",
      "处理文章 787/5000\n",
      "处理文章 788/5000\n",
      "处理文章 789/5000\n",
      "处理文章 790/5000\n",
      "处理文章 791/5000\n",
      "处理文章 792/5000\n",
      "处理文章 793/5000\n",
      "处理文章 794/5000\n",
      "处理文章 795/5000\n",
      "处理文章 796/5000\n",
      "处理文章 797/5000\n",
      "处理文章 798/5000\n",
      "处理文章 799/5000\n",
      "处理文章 800/5000\n",
      "处理文章 801/5000\n",
      "处理文章 802/5000\n",
      "处理文章 804/5000\n",
      "处理文章 803/5000\n",
      "处理文章 805/5000\n",
      "处理文章 806/5000\n",
      "处理文章 808/5000\n",
      "处理文章 807/5000\n",
      "处理文章 809/5000\n",
      "处理文章 811/5000\n",
      "处理文章 812/5000\n",
      "处理文章 810/5000\n",
      "处理文章 813/5000\n",
      "处理文章 814/5000\n",
      "处理文章 815/5000\n",
      "处理文章 816/5000\n",
      "处理文章 817/5000\n",
      "处理文章 819/5000\n",
      "处理文章 820/5000\n",
      "处理文章 818/5000\n",
      "处理文章 821/5000\n",
      "处理文章 822/5000\n",
      "处理文章 823/5000\n",
      "处理文章 826/5000\n",
      "处理文章 824/5000\n",
      "处理文章 825/5000\n",
      "处理文章 827/5000\n",
      "处理文章 828/5000\n",
      "处理文章 829/5000\n",
      "处理文章 830/5000\n",
      "处理文章 833/5000\n",
      "处理文章 831/5000\n",
      "处理文章 832/5000\n",
      "处理文章 834/5000\n",
      "处理文章 835/5000\n",
      "处理文章 836/5000\n",
      "处理文章 838/5000\n",
      "处理文章 837/5000\n",
      "处理文章 839/5000\n",
      "处理文章 840/5000\n",
      "处理文章 841/5000\n",
      "处理文章 842/5000\n",
      "处理文章 843/5000\n",
      "处理文章 844/5000\n",
      "处理文章 846/5000\n",
      "处理文章 847/5000\n",
      "处理文章 845/5000\n",
      "处理文章 848/5000\n",
      "处理文章 849/5000\n",
      "处理文章 850/5000\n",
      "处理文章 851/5000\n",
      "处理文章 852/5000\n",
      "处理文章 854/5000\n",
      "处理文章 853/5000\n",
      "处理文章 855/5000\n",
      "处理文章 856/5000\n",
      "处理文章 857/5000\n",
      "处理文章 859/5000\n",
      "处理文章 858/5000\n",
      "处理文章 860/5000\n",
      "处理文章 862/5000\n",
      "处理文章 861/5000\n",
      "处理文章 863/5000\n",
      "处理文章 864/5000\n",
      "处理文章 865/5000\n",
      "处理文章 866/5000\n",
      "处理文章 868/5000\n",
      "处理文章 867/5000\n",
      "处理文章 869/5000\n",
      "处理文章 870/5000\n",
      "处理文章 871/5000\n",
      "处理文章 872/5000\n",
      "处理文章 873/5000\n",
      "处理文章 874/5000\n",
      "处理文章 875/5000\n",
      "处理文章 876/5000\n",
      "处理文章 877/5000\n",
      "处理文章 878/5000\n",
      "处理文章 879/5000\n",
      "处理文章 880/5000\n",
      "处理文章 881/5000\n",
      "处理文章 882/5000\n",
      "处理文章 883/5000\n",
      "处理文章 884/5000\n",
      "处理文章 886/5000\n",
      "处理文章 885/5000\n",
      "处理文章 887/5000\n",
      "处理文章 888/5000\n",
      "处理文章 889/5000\n",
      "处理文章 890/5000\n",
      "处理文章 891/5000\n",
      "处理文章 893/5000\n",
      "处理文章 892/5000\n",
      "处理文章 896/5000\n",
      "处理文章 894/5000\n",
      "处理文章 895/5000\n",
      "处理文章 897/5000\n",
      "处理文章 898/5000\n",
      "处理文章 901/5000\n",
      "处理文章 900/5000\n",
      "处理文章 899/5000\n",
      "处理文章 902/5000\n",
      "处理文章 903/5000\n",
      "处理文章 905/5000\n",
      "处理文章 904/5000\n",
      "处理文章 906/5000\n",
      "处理文章 907/5000\n",
      "处理文章 908/5000\n",
      "处理文章 909/5000\n",
      "处理文章 911/5000\n",
      "处理文章 910/5000\n",
      "处理文章 912/5000\n",
      "处理文章 913/5000\n",
      "处理文章 914/5000\n",
      "处理文章 915/5000\n",
      "处理文章 916/5000\n",
      "处理文章 918/5000\n",
      "处理文章 917/5000\n",
      "处理文章 919/5000\n",
      "处理文章 920/5000\n",
      "处理文章 921/5000\n",
      "处理文章 922/5000\n",
      "处理文章 923/5000\n",
      "处理文章 924/5000\n",
      "处理文章 925/5000\n",
      "处理文章 926/5000\n",
      "处理文章 927/5000\n",
      "处理文章 929/5000\n",
      "处理文章 928/5000\n",
      "处理文章 930/5000\n",
      "处理文章 931/5000\n",
      "处理文章 933/5000\n",
      "处理文章 935/5000\n",
      "处理文章 932/5000\n",
      "处理文章 936/5000\n",
      "处理文章 934/5000\n",
      "处理文章 937/5000\n",
      "处理文章 938/5000\n",
      "处理文章 940/5000\n",
      "处理文章 939/5000\n",
      "处理文章 941/5000\n",
      "处理文章 942/5000\n",
      "处理文章 943/5000\n",
      "处理文章 944/5000\n",
      "处理文章 947/5000\n",
      "处理文章 945/5000\n",
      "处理文章 946/5000\n",
      "处理文章 948/5000\n",
      "处理文章 950/5000\n",
      "处理文章 949/5000\n",
      "处理文章 951/5000\n",
      "处理文章 952/5000\n",
      "处理文章 953/5000\n",
      "处理文章 954/5000\n",
      "处理文章 955/5000\n",
      "处理文章 956/5000\n",
      "处理文章 957/5000\n",
      "处理文章 958/5000\n",
      "处理文章 959/5000\n",
      "处理文章 960/5000\n",
      "处理文章 961/5000\n",
      "处理文章 962/5000\n",
      "处理文章 963/5000\n",
      "处理文章 964/5000\n",
      "处理文章 965/5000\n",
      "处理文章 967/5000\n",
      "处理文章 966/5000\n",
      "处理文章 968/5000\n",
      "处理文章 969/5000\n",
      "处理文章 971/5000\n",
      "处理文章 970/5000\n",
      "处理文章 972/5000\n",
      "处理文章 974/5000\n",
      "处理文章 973/5000\n",
      "处理文章 975/5000\n",
      "处理文章 976/5000\n",
      "处理文章 978/5000\n",
      "处理文章 977/5000\n",
      "处理文章 979/5000\n",
      "处理文章 980/5000\n",
      "处理文章 981/5000\n",
      "处理文章 982/5000\n",
      "处理文章 983/5000\n",
      "处理文章 984/5000\n",
      "处理文章 985/5000\n",
      "处理文章 986/5000\n",
      "处理文章 987/5000\n",
      "处理文章 988/5000\n",
      "处理文章 989/5000\n",
      "处理文章 990/5000\n",
      "处理文章 991/5000\n",
      "处理文章 992/5000\n",
      "处理文章 993/5000\n",
      "处理文章 994/5000\n",
      "处理文章 995/5000\n",
      "处理文章 996/5000\n",
      "处理文章 997/5000\n",
      "处理文章 998/5000\n",
      "处理文章 999/5000\n",
      "处理文章 1000/5000\n",
      "处理文章 1001/5000\n",
      "处理文章 1002/5000\n",
      "处理文章 1003/5000\n",
      "处理文章 1005/5000\n",
      "处理文章 1006/5000\n",
      "处理文章 1004/5000\n",
      "处理文章 1007/5000\n",
      "处理文章 1008/5000\n",
      "处理文章 1009/5000\n",
      "处理文章 1010/5000\n",
      "处理文章 1011/5000\n",
      "处理文章 1012/5000\n",
      "处理文章 1013/5000\n",
      "处理文章 1015/5000\n",
      "处理文章 1014/5000\n",
      "处理文章 1017/5000\n",
      "处理文章 1016/5000\n",
      "处理文章 1018/5000\n",
      "处理文章 1019/5000\n",
      "处理文章 1020/5000\n",
      "处理文章 1021/5000\n",
      "处理文章 1023/5000\n",
      "处理文章 1022/5000\n",
      "处理文章 1024/5000\n",
      "处理文章 1025/5000\n",
      "处理文章 1026/5000\n",
      "处理文章 1027/5000\n",
      "处理文章 1029/5000\n",
      "处理文章 1030/5000\n",
      "处理文章 1028/5000\n",
      "处理文章 1032/5000\n",
      "处理文章 1033/5000\n",
      "处理文章 1031/5000\n",
      "处理文章 1035/5000\n",
      "处理文章 1034/5000\n",
      "处理文章 1036/5000\n",
      "处理文章 1037/5000\n",
      "处理文章 1039/5000\n",
      "处理文章 1038/5000\n",
      "处理文章 1040/5000\n",
      "处理文章 1041/5000\n",
      "处理文章 1042/5000\n",
      "处理文章 1044/5000\n",
      "处理文章 1043/5000\n",
      "处理文章 1045/5000\n",
      "处理文章 1046/5000\n",
      "处理文章 1047/5000\n",
      "处理文章 1048/5000\n",
      "处理文章 1049/5000\n",
      "处理文章 1050/5000\n",
      "处理文章 1051/5000\n",
      "处理文章 1052/5000\n",
      "处理文章 1053/5000\n",
      "处理文章 1054/5000\n",
      "处理文章 1055/5000\n",
      "处理文章 1056/5000\n",
      "处理文章 1057/5000\n",
      "处理文章 1058/5000\n",
      "处理文章 1059/5000\n",
      "处理文章 1060/5000\n",
      "处理文章 1061/5000\n",
      "处理文章 1062/5000\n",
      "处理文章 1063/5000\n",
      "处理文章 1064/5000\n",
      "处理文章 1065/5000\n",
      "处理文章 1066/5000\n",
      "处理文章 1067/5000\n",
      "处理文章 1068/5000\n",
      "处理文章 1069/5000\n",
      "处理文章 1070/5000\n",
      "处理文章 1072/5000\n",
      "处理文章 1073/5000\n",
      "处理文章 1071/5000\n",
      "处理文章 1075/5000\n",
      "处理文章 1074/5000\n",
      "处理文章 1076/5000\n",
      "处理文章 1077/5000\n",
      "处理文章 1078/5000\n",
      "处理文章 1079/5000\n",
      "处理文章 1081/5000\n",
      "处理文章 1080/5000\n",
      "处理文章 1082/5000\n",
      "处理文章 1084/5000\n",
      "处理文章 1083/5000\n",
      "处理文章 1085/5000\n",
      "处理文章 1086/5000\n",
      "处理文章 1087/5000\n",
      "处理文章 1088/5000\n",
      "处理文章 1089/5000\n",
      "处理文章 1090/5000\n",
      "处理文章 1091/5000\n",
      "处理文章 1092/5000\n",
      "处理文章 1093/5000\n",
      "处理文章 1094/5000\n",
      "处理文章 1095/5000\n",
      "处理文章 1096/5000\n",
      "处理文章 1097/5000\n",
      "处理文章 1098/5000\n",
      "处理文章 1101/5000\n",
      "处理文章 1099/5000\n",
      "处理文章 1100/5000\n",
      "处理文章 1102/5000\n",
      "处理文章 1103/5000\n",
      "处理文章 1104/5000\n",
      "处理文章 1106/5000\n",
      "处理文章 1105/5000\n",
      "处理文章 1107/5000\n",
      "处理文章 1108/5000\n",
      "处理文章 1109/5000\n",
      "处理文章 1110/5000\n",
      "处理文章 1111/5000\n",
      "处理文章 1112/5000\n",
      "处理文章 1113/5000\n",
      "处理文章 1114/5000\n",
      "处理文章 1115/5000\n",
      "处理文章 1116/5000\n",
      "处理文章 1117/5000\n",
      "处理文章 1118/5000\n",
      "处理文章 1119/5000\n",
      "处理文章 1120/5000\n",
      "处理文章 1121/5000\n",
      "处理文章 1122/5000\n",
      "处理文章 1124/5000\n",
      "处理文章 1123/5000\n",
      "处理文章 1125/5000\n",
      "处理文章 1128/5000\n",
      "处理文章 1127/5000\n",
      "处理文章 1126/5000\n",
      "处理文章 1129/5000\n",
      "处理文章 1130/5000\n",
      "处理文章 1132/5000\n",
      "处理文章 1131/5000\n",
      "处理文章 1133/5000\n",
      "处理文章 1134/5000\n",
      "处理文章 1135/5000\n",
      "处理文章 1136/5000\n",
      "处理文章 1137/5000\n",
      "处理文章 1138/5000\n",
      "处理文章 1139/5000\n",
      "处理文章 1140/5000\n",
      "处理文章 1141/5000\n",
      "处理文章 1143/5000\n",
      "处理文章 1142/5000\n",
      "处理文章 1144/5000\n",
      "处理文章 1145/5000\n",
      "处理文章 1146/5000\n",
      "处理文章 1147/5000\n",
      "处理文章 1148/5000\n",
      "处理文章 1149/5000\n",
      "处理文章 1151/5000\n",
      "处理文章 1150/5000\n",
      "处理文章 1152/5000\n",
      "处理文章 1153/5000\n",
      "处理文章 1155/5000\n",
      "处理文章 1154/5000\n",
      "处理文章 1157/5000\n",
      "处理文章 1156/5000\n",
      "处理文章 1158/5000\n",
      "处理文章 1159/5000\n",
      "处理文章 1160/5000\n",
      "处理文章 1161/5000\n",
      "处理文章 1162/5000\n",
      "处理文章 1163/5000\n",
      "处理文章 1164/5000\n",
      "处理文章 1165/5000\n",
      "处理文章 1166/5000\n",
      "处理文章 1167/5000\n",
      "处理文章 1168/5000\n",
      "处理文章 1169/5000\n",
      "处理文章 1171/5000\n",
      "处理文章 1170/5000\n",
      "处理文章 1172/5000\n",
      "处理文章 1173/5000\n",
      "处理文章 1175/5000\n",
      "处理文章 1176/5000\n",
      "处理文章 1174/5000\n",
      "处理文章 1177/5000\n",
      "处理文章 1178/5000\n",
      "处理文章 1182/5000\n",
      "处理文章 1180/5000\n",
      "处理文章 1179/5000\n",
      "处理文章 1181/5000\n",
      "处理文章 1184/5000\n",
      "处理文章 1187/5000\n",
      "处理文章 1185/5000\n",
      "处理文章 1183/5000\n",
      "处理文章 1186/5000\n",
      "处理文章 1189/5000\n",
      "处理文章 1188/5000\n",
      "处理文章 1190/5000\n",
      "处理文章 1192/5000\n",
      "处理文章 1191/5000\n",
      "处理文章 1194/5000\n",
      "处理文章 1193/5000\n",
      "处理文章 1195/5000\n",
      "处理文章 1196/5000\n",
      "处理文章 1197/5000\n",
      "处理文章 1198/5000\n",
      "处理文章 1199/5000\n",
      "处理文章 1200/5000\n",
      "处理文章 1201/5000\n",
      "处理文章 1202/5000\n",
      "处理文章 1203/5000\n",
      "处理文章 1204/5000\n",
      "处理文章 1206/5000\n",
      "处理文章 1205/5000\n",
      "处理文章 1207/5000\n",
      "处理文章 1208/5000\n",
      "处理文章 1209/5000\n",
      "处理文章 1210/5000\n",
      "处理文章 1211/5000\n",
      "处理文章 1212/5000\n",
      "处理文章 1213/5000\n",
      "处理文章 1214/5000\n",
      "处理文章 1216/5000\n",
      "处理文章 1215/5000\n",
      "处理文章 1217/5000\n",
      "处理文章 1218/5000\n",
      "处理文章 1219/5000\n",
      "处理文章 1220/5000\n",
      "处理文章 1221/5000\n",
      "处理文章 1222/5000\n",
      "处理文章 1223/5000\n",
      "处理文章 1224/5000\n",
      "处理文章 1225/5000\n",
      "处理文章 1226/5000\n",
      "处理文章 1227/5000\n",
      "处理文章 1228/5000\n",
      "处理文章 1229/5000\n",
      "处理文章 1230/5000\n",
      "处理文章 1231/5000\n",
      "处理文章 1232/5000\n",
      "处理文章 1234/5000\n",
      "处理文章 1235/5000\n",
      "处理文章 1233/5000\n",
      "处理文章 1236/5000\n",
      "处理文章 1237/5000\n",
      "处理文章 1238/5000\n",
      "处理文章 1239/5000\n",
      "处理文章 1240/5000\n",
      "处理文章 1241/5000\n",
      "处理文章 1242/5000\n",
      "处理文章 1243/5000\n",
      "处理文章 1244/5000\n",
      "处理文章 1245/5000\n",
      "处理文章 1248/5000\n",
      "处理文章 1247/5000\n",
      "处理文章 1246/5000\n",
      "处理文章 1249/5000\n",
      "处理文章 1250/5000\n",
      "处理文章 1251/5000\n",
      "处理文章 1253/5000\n",
      "处理文章 1252/5000\n",
      "处理文章 1254/5000\n",
      "处理文章 1255/5000\n",
      "处理文章 1256/5000\n",
      "处理文章 1257/5000\n",
      "处理文章 1258/5000\n",
      "处理文章 1259/5000\n",
      "处理文章 1260/5000\n",
      "处理文章 1261/5000\n",
      "处理文章 1262/5000\n",
      "处理文章 1263/5000\n",
      "处理文章 1264/5000\n",
      "处理文章 1265/5000\n",
      "处理文章 1266/5000\n",
      "处理文章 1267/5000\n",
      "处理文章 1268/5000\n",
      "处理文章 1269/5000\n",
      "处理文章 1270/5000\n",
      "处理文章 1271/5000\n",
      "处理文章 1272/5000\n",
      "处理文章 1274/5000\n",
      "处理文章 1273/5000\n",
      "处理文章 1276/5000\n",
      "处理文章 1275/5000\n",
      "处理文章 1277/5000\n",
      "处理文章 1278/5000\n",
      "处理文章 1279/5000\n",
      "处理文章 1280/5000\n",
      "处理文章 1281/5000\n",
      "处理文章 1282/5000\n",
      "处理文章 1283/5000\n",
      "处理文章 1284/5000\n",
      "处理文章 1285/5000\n",
      "处理文章 1286/5000\n",
      "处理文章 1288/5000\n",
      "处理文章 1287/5000\n",
      "处理文章 1289/5000\n",
      "处理文章 1292/5000\n",
      "处理文章 1290/5000\n",
      "处理文章 1291/5000\n",
      "处理文章 1293/5000\n",
      "处理文章 1294/5000\n",
      "处理文章 1295/5000\n",
      "处理文章 1296/5000\n",
      "处理文章 1297/5000\n",
      "处理文章 1298/5000\n",
      "处理文章 1299/5000\n",
      "处理文章 1300/5000\n",
      "处理文章 1301/5000\n",
      "处理文章 1302/5000\n",
      "处理文章 1303/5000\n",
      "处理文章 1304/5000\n",
      "处理文章 1305/5000\n",
      "处理文章 1306/5000\n",
      "处理文章 1308/5000\n",
      "处理文章 1307/5000\n",
      "处理文章 1309/5000\n",
      "处理文章 1310/5000\n",
      "处理文章 1311/5000\n",
      "处理文章 1312/5000\n",
      "处理文章 1313/5000\n",
      "处理文章 1314/5000\n",
      "处理文章 1315/5000\n",
      "处理文章 1316/5000\n",
      "处理文章 1318/5000\n",
      "处理文章 1317/5000\n",
      "处理文章 1319/5000\n",
      "处理文章 1320/5000\n",
      "处理文章 1321/5000\n",
      "处理文章 1322/5000\n",
      "处理文章 1323/5000\n",
      "处理文章 1324/5000\n",
      "处理文章 1325/5000\n",
      "处理文章 1326/5000\n",
      "处理文章 1327/5000\n",
      "处理文章 1329/5000\n",
      "处理文章 1328/5000\n",
      "处理文章 1330/5000\n",
      "处理文章 1331/5000\n",
      "处理文章 1332/5000\n",
      "处理文章 1333/5000\n",
      "处理文章 1334/5000\n",
      "处理文章 1335/5000\n",
      "处理文章 1336/5000\n",
      "处理文章 1337/5000\n",
      "处理文章 1339/5000\n",
      "处理文章 1338/5000\n",
      "处理文章 1340/5000\n",
      "处理文章 1341/5000\n",
      "处理文章 1342/5000\n",
      "处理文章 1343/5000\n",
      "处理文章 1345/5000\n",
      "处理文章 1344/5000\n",
      "处理文章 1346/5000\n",
      "处理文章 1347/5000\n",
      "处理文章 1348/5000\n",
      "处理文章 1349/5000\n",
      "处理文章 1351/5000\n",
      "处理文章 1350/5000\n",
      "处理文章 1352/5000\n",
      "处理文章 1353/5000\n",
      "处理文章 1354/5000\n",
      "处理文章 1357/5000\n",
      "处理文章 1355/5000\n",
      "处理文章 1356/5000\n",
      "处理文章 1359/5000\n",
      "处理文章 1358/5000\n",
      "处理文章 1360/5000\n",
      "处理文章 1361/5000\n",
      "处理文章 1362/5000\n",
      "处理文章 1364/5000\n",
      "处理文章 1363/5000\n",
      "处理文章 1365/5000\n",
      "处理文章 1366/5000\n",
      "处理文章 1367/5000\n",
      "处理文章 1369/5000\n",
      "处理文章 1368/5000\n",
      "处理文章 1370/5000\n",
      "处理文章 1371/5000\n",
      "处理文章 1373/5000\n",
      "处理文章 1372/5000\n",
      "处理文章 1375/5000\n",
      "处理文章 1374/5000\n",
      "处理文章 1376/5000\n",
      "处理文章 1377/5000\n",
      "处理文章 1378/5000\n",
      "处理文章 1379/5000\n",
      "处理文章 1380/5000\n",
      "处理文章 1381/5000\n",
      "处理文章 1382/5000\n",
      "处理文章 1383/5000\n",
      "处理文章 1385/5000\n",
      "处理文章 1384/5000\n",
      "处理文章 1386/5000\n",
      "处理文章 1387/5000\n",
      "处理文章 1388/5000\n",
      "处理文章 1389/5000\n",
      "处理文章 1392/5000\n",
      "处理文章 1390/5000\n",
      "处理文章 1391/5000\n",
      "处理文章 1393/5000\n",
      "处理文章 1394/5000\n",
      "处理文章 1395/5000\n",
      "处理文章 1396/5000\n",
      "处理文章 1397/5000\n",
      "处理文章 1398/5000\n",
      "处理文章 1399/5000\n",
      "处理文章 1400/5000\n",
      "处理文章 1402/5000\n",
      "处理文章 1401/5000\n",
      "处理文章 1403/5000\n",
      "处理文章 1405/5000\n",
      "处理文章 1407/5000\n",
      "处理文章 1404/5000\n",
      "处理文章 1406/5000\n",
      "处理文章 1408/5000\n",
      "处理文章 1409/5000\n",
      "处理文章 1410/5000\n",
      "处理文章 1411/5000\n",
      "处理文章 1412/5000\n",
      "处理文章 1413/5000\n",
      "处理文章 1414/5000\n",
      "处理文章 1416/5000\n",
      "处理文章 1415/5000\n",
      "处理文章 1417/5000\n",
      "处理文章 1418/5000\n",
      "处理文章 1419/5000\n",
      "处理文章 1420/5000\n",
      "处理文章 1421/5000\n",
      "处理文章 1422/5000\n",
      "处理文章 1423/5000\n",
      "处理文章 1424/5000\n",
      "处理文章 1425/5000\n",
      "处理文章 1427/5000\n",
      "处理文章 1426/5000\n",
      "处理文章 1428/5000\n",
      "处理文章 1429/5000\n",
      "处理文章 1430/5000\n",
      "处理文章 1433/5000\n",
      "处理文章 1431/5000\n",
      "处理文章 1435/5000\n",
      "处理文章 1432/5000\n",
      "处理文章 1434/5000\n",
      "处理文章 1436/5000\n",
      "处理文章 1437/5000\n",
      "处理文章 1438/5000\n",
      "处理文章 1440/5000\n",
      "处理文章 1439/5000\n",
      "处理文章 1441/5000\n",
      "处理文章 1442/5000\n",
      "处理文章 1443/5000\n",
      "处理文章 1444/5000\n",
      "处理文章 1445/5000\n",
      "处理文章 1446/5000\n",
      "处理文章 1447/5000\n",
      "处理文章 1448/5000\n",
      "处理文章 1451/5000\n",
      "处理文章 1450/5000\n",
      "处理文章 1449/5000\n",
      "处理文章 1452/5000\n",
      "处理文章 1453/5000\n",
      "处理文章 1454/5000\n",
      "处理文章 1458/5000\n",
      "处理文章 1455/5000\n",
      "处理文章 1457/5000\n",
      "处理文章 1459/5000\n",
      "处理文章 1456/5000\n",
      "处理文章 1460/5000\n",
      "处理文章 1461/5000\n",
      "处理文章 1462/5000\n",
      "处理文章 1463/5000\n",
      "处理文章 1464/5000\n",
      "处理文章 1465/5000\n",
      "处理文章 1466/5000\n",
      "处理文章 1467/5000\n",
      "处理文章 1468/5000\n",
      "处理文章 1469/5000\n",
      "处理文章 1471/5000\n",
      "处理文章 1470/5000\n",
      "处理文章 1472/5000\n",
      "处理文章 1473/5000\n",
      "处理文章 1474/5000\n",
      "处理文章 1475/5000\n",
      "处理文章 1476/5000\n",
      "处理文章 1477/5000\n",
      "处理文章 1478/5000\n",
      "处理文章 1479/5000\n",
      "处理文章 1480/5000\n",
      "处理文章 1481/5000\n",
      "处理文章 1482/5000\n",
      "处理文章 1485/5000\n",
      "处理文章 1483/5000\n",
      "处理文章 1486/5000\n",
      "处理文章 1484/5000\n",
      "处理文章 1488/5000\n",
      "处理文章 1487/5000\n",
      "处理文章 1489/5000\n",
      "处理文章 1490/5000\n",
      "处理文章 1491/5000\n",
      "处理文章 1492/5000\n",
      "处理文章 1493/5000\n",
      "处理文章 1494/5000\n",
      "处理文章 1496/5000\n",
      "处理文章 1497/5000\n",
      "处理文章 1495/5000\n",
      "处理文章 1498/5000\n",
      "处理文章 1499/5000\n",
      "处理文章 1500/5000\n",
      "处理文章 1501/5000\n",
      "处理文章 1502/5000\n",
      "处理文章 1503/5000\n",
      "处理文章 1504/5000\n",
      "处理文章 1505/5000\n",
      "处理文章 1506/5000\n",
      "处理文章 1507/5000\n",
      "处理文章 1508/5000\n",
      "处理文章 1509/5000\n",
      "处理文章 1510/5000\n",
      "处理文章 1511/5000\n",
      "处理文章 1512/5000\n",
      "处理文章 1513/5000\n",
      "处理文章 1514/5000\n",
      "处理文章 1515/5000\n",
      "处理文章 1516/5000\n",
      "处理文章 1517/5000\n",
      "处理文章 1518/5000\n",
      "处理文章 1519/5000\n",
      "处理文章 1520/5000\n",
      "处理文章 1522/5000\n",
      "处理文章 1521/5000\n",
      "处理文章 1523/5000\n",
      "处理文章 1524/5000\n",
      "处理文章 1525/5000\n",
      "处理文章 1526/5000\n",
      "处理文章 1527/5000\n",
      "处理文章 1528/5000\n",
      "处理文章 1529/5000\n",
      "处理文章 1530/5000\n",
      "处理文章 1531/5000\n",
      "处理文章 1534/5000\n",
      "处理文章 1532/5000\n",
      "处理文章 1533/5000\n",
      "处理文章 1536/5000\n",
      "处理文章 1535/5000\n",
      "处理文章 1537/5000\n",
      "处理文章 1538/5000\n",
      "处理文章 1539/5000\n",
      "处理文章 1543/5000\n",
      "处理文章 1540/5000\n",
      "处理文章 1544/5000\n",
      "处理文章 1541/5000\n",
      "处理文章 1542/5000\n",
      "处理文章 1545/5000\n",
      "处理文章 1546/5000\n",
      "处理文章 1547/5000\n",
      "处理文章 1548/5000\n",
      "处理文章 1549/5000\n",
      "处理文章 1551/5000\n",
      "处理文章 1552/5000\n",
      "处理文章 1550/5000\n",
      "处理文章 1553/5000\n",
      "处理文章 1555/5000\n",
      "处理文章 1554/5000\n",
      "处理文章 1560/5000\n",
      "处理文章 1556/5000\n",
      "处理文章 1557/5000\n",
      "处理文章 1558/5000\n",
      "处理文章 1559/5000\n",
      "处理文章 1561/5000\n",
      "处理文章 1562/5000\n",
      "处理文章 1563/5000\n",
      "处理文章 1565/5000\n",
      "处理文章 1564/5000\n",
      "处理文章 1566/5000\n",
      "处理文章 1567/5000\n",
      "处理文章 1568/5000\n",
      "处理文章 1569/5000\n",
      "处理文章 1570/5000\n",
      "处理文章 1571/5000\n",
      "处理文章 1572/5000\n",
      "处理文章 1574/5000\n",
      "处理文章 1573/5000\n",
      "处理文章 1576/5000\n",
      "处理文章 1575/5000\n",
      "处理文章 1577/5000\n",
      "处理文章 1579/5000\n",
      "处理文章 1578/5000\n",
      "处理文章 1580/5000\n",
      "处理文章 1581/5000\n",
      "处理文章 1582/5000\n",
      "处理文章 1583/5000\n",
      "处理文章 1584/5000\n",
      "处理文章 1585/5000\n",
      "处理文章 1587/5000\n",
      "处理文章 1586/5000\n",
      "处理文章 1588/5000\n",
      "处理文章 1589/5000\n",
      "处理文章 1590/5000\n",
      "处理文章 1591/5000\n",
      "处理文章 1592/5000\n",
      "处理文章 1593/5000\n",
      "处理文章 1596/5000\n",
      "处理文章 1594/5000\n",
      "处理文章 1595/5000\n",
      "处理文章 1598/5000\n",
      "处理文章 1597/5000\n",
      "处理文章 1599/5000\n",
      "处理文章 1600/5000\n",
      "处理文章 1601/5000\n",
      "处理文章 1602/5000\n",
      "处理文章 1603/5000\n",
      "处理文章 1605/5000\n",
      "处理文章 1604/5000\n",
      "处理文章 1606/5000\n",
      "处理文章 1607/5000\n",
      "处理文章 1608/5000\n",
      "处理文章 1609/5000\n",
      "处理文章 1610/5000\n",
      "处理文章 1611/5000\n",
      "处理文章 1612/5000\n",
      "处理文章 1613/5000\n",
      "处理文章 1614/5000\n",
      "处理文章 1615/5000\n",
      "处理文章 1616/5000\n",
      "处理文章 1618/5000\n",
      "处理文章 1617/5000\n",
      "处理文章 1619/5000\n",
      "处理文章 1621/5000\n",
      "处理文章 1620/5000\n",
      "处理文章 1622/5000\n",
      "处理文章 1623/5000\n",
      "处理文章 1624/5000\n",
      "处理文章 1625/5000\n",
      "处理文章 1626/5000\n",
      "处理文章 1627/5000\n",
      "处理文章 1628/5000\n",
      "处理文章 1629/5000\n",
      "处理文章 1630/5000\n",
      "处理文章 1631/5000\n",
      "处理文章 1632/5000\n",
      "处理文章 1633/5000\n",
      "处理文章 1634/5000\n",
      "处理文章 1635/5000\n",
      "处理文章 1636/5000\n",
      "处理文章 1637/5000\n",
      "处理文章 1638/5000\n",
      "处理文章 1639/5000\n",
      "处理文章 1640/5000\n",
      "处理文章 1641/5000\n",
      "处理文章 1642/5000\n",
      "处理文章 1643/5000\n",
      "处理文章 1644/5000\n",
      "处理文章 1646/5000\n",
      "处理文章 1645/5000\n",
      "处理文章 1647/5000\n",
      "处理文章 1648/5000\n",
      "处理文章 1649/5000\n",
      "处理文章 1650/5000\n",
      "处理文章 1651/5000\n",
      "处理文章 1652/5000\n",
      "处理文章 1654/5000\n",
      "处理文章 1653/5000\n",
      "处理文章 1655/5000\n",
      "处理文章 1656/5000\n",
      "处理文章 1657/5000\n",
      "处理文章 1658/5000\n",
      "处理文章 1659/5000\n",
      "处理文章 1660/5000\n",
      "处理文章 1661/5000\n",
      "处理文章 1662/5000\n",
      "处理文章 1663/5000\n",
      "处理文章 1664/5000\n",
      "处理文章 1665/5000\n",
      "处理文章 1666/5000\n",
      "处理文章 1667/5000\n",
      "处理文章 1668/5000\n",
      "处理文章 1670/5000\n",
      "处理文章 1669/5000\n",
      "处理文章 1671/5000\n",
      "处理文章 1674/5000\n",
      "处理文章 1673/5000\n",
      "处理文章 1672/5000\n",
      "处理文章 1675/5000\n",
      "处理文章 1676/5000\n",
      "处理文章 1678/5000\n",
      "处理文章 1679/5000\n",
      "处理文章 1677/5000\n",
      "处理文章 1680/5000\n",
      "处理文章 1682/5000\n",
      "处理文章 1681/5000\n",
      "处理文章 1684/5000\n",
      "处理文章 1683/5000\n",
      "处理文章 1685/5000\n",
      "处理文章 1686/5000\n",
      "处理文章 1688/5000\n",
      "处理文章 1689/5000\n",
      "处理文章 1687/5000\n",
      "处理文章 1691/5000\n",
      "处理文章 1690/5000\n",
      "处理文章 1692/5000\n",
      "处理文章 1694/5000\n",
      "处理文章 1693/5000\n",
      "处理文章 1695/5000\n",
      "处理文章 1696/5000\n",
      "处理文章 1699/5000\n",
      "处理文章 1698/5000\n",
      "处理文章 1697/5000\n",
      "处理文章 1700/5000\n",
      "处理文章 1701/5000\n",
      "处理文章 1702/5000\n",
      "处理文章 1703/5000\n",
      "处理文章 1704/5000\n",
      "处理文章 1705/5000\n",
      "处理文章 1707/5000\n",
      "处理文章 1706/5000\n",
      "处理文章 1708/5000\n",
      "处理文章 1709/5000\n",
      "处理文章 1710/5000\n",
      "处理文章 1711/5000\n",
      "处理文章 1712/5000\n",
      "处理文章 1713/5000\n",
      "处理文章 1715/5000\n",
      "处理文章 1714/5000\n",
      "处理文章 1716/5000\n",
      "处理文章 1717/5000\n",
      "处理文章 1718/5000\n",
      "处理文章 1719/5000\n",
      "处理文章 1720/5000\n",
      "处理文章 1721/5000\n",
      "处理文章 1722/5000\n",
      "处理文章 1723/5000\n",
      "处理文章 1724/5000\n",
      "处理文章 1727/5000\n",
      "处理文章 1725/5000\n",
      "处理文章 1726/5000\n",
      "处理文章 1729/5000\n",
      "处理文章 1728/5000\n",
      "处理文章 1730/5000\n",
      "处理文章 1731/5000\n",
      "处理文章 1732/5000\n",
      "处理文章 1733/5000\n",
      "处理文章 1735/5000\n",
      "处理文章 1734/5000\n",
      "处理文章 1737/5000\n",
      "处理文章 1738/5000\n",
      "处理文章 1736/5000\n",
      "处理文章 1739/5000\n",
      "处理文章 1740/5000\n",
      "处理文章 1741/5000\n",
      "处理文章 1744/5000\n",
      "处理文章 1743/5000\n",
      "处理文章 1742/5000\n",
      "处理文章 1745/5000\n",
      "处理文章 1746/5000\n",
      "处理文章 1748/5000\n",
      "处理文章 1747/5000\n",
      "处理文章 1749/5000\n",
      "处理文章 1750/5000\n",
      "处理文章 1752/5000\n",
      "处理文章 1751/5000\n",
      "处理文章 1753/5000\n",
      "处理文章 1754/5000\n",
      "处理文章 1755/5000\n",
      "处理文章 1756/5000\n",
      "处理文章 1757/5000\n",
      "处理文章 1758/5000\n",
      "处理文章 1759/5000\n",
      "处理文章 1760/5000\n",
      "处理文章 1761/5000\n",
      "处理文章 1764/5000\n",
      "处理文章 1763/5000\n",
      "处理文章 1762/5000\n",
      "处理文章 1765/5000\n",
      "处理文章 1766/5000\n",
      "处理文章 1767/5000\n",
      "处理文章 1768/5000\n",
      "处理文章 1769/5000\n",
      "处理文章 1770/5000\n",
      "处理文章 1771/5000\n",
      "处理文章 1772/5000\n",
      "处理文章 1773/5000\n",
      "处理文章 1774/5000\n",
      "处理文章 1775/5000\n",
      "处理文章 1776/5000\n",
      "处理文章 1777/5000\n",
      "处理文章 1778/5000\n",
      "处理文章 1779/5000\n",
      "处理文章 1780/5000\n",
      "处理文章 1781/5000\n",
      "处理文章 1782/5000\n",
      "处理文章 1783/5000\n",
      "处理文章 1784/5000\n",
      "处理文章 1785/5000\n",
      "处理文章 1786/5000\n",
      "处理文章 1788/5000\n",
      "处理文章 1787/5000\n",
      "处理文章 1789/5000\n",
      "处理文章 1791/5000\n",
      "处理文章 1790/5000\n",
      "处理文章 1792/5000\n",
      "处理文章 1793/5000\n",
      "处理文章 1794/5000\n",
      "处理文章 1795/5000\n",
      "处理文章 1796/5000\n",
      "处理文章 1798/5000\n",
      "处理文章 1797/5000\n",
      "处理文章 1799/5000\n",
      "处理文章 1800/5000\n",
      "处理文章 1801/5000\n",
      "处理文章 1802/5000\n",
      "处理文章 1804/5000\n",
      "处理文章 1803/5000\n",
      "处理文章 1805/5000\n",
      "处理文章 1806/5000\n",
      "处理文章 1807/5000\n",
      "处理文章 1808/5000\n",
      "处理文章 1809/5000\n",
      "处理文章 1810/5000\n",
      "处理文章 1811/5000\n",
      "处理文章 1812/5000\n",
      "处理文章 1813/5000\n",
      "处理文章 1814/5000\n",
      "处理文章 1815/5000\n",
      "处理文章 1816/5000\n",
      "处理文章 1817/5000\n",
      "处理文章 1818/5000\n",
      "处理文章 1819/5000\n",
      "处理文章 1820/5000\n",
      "处理文章 1821/5000\n",
      "处理文章 1823/5000\n",
      "处理文章 1822/5000\n",
      "处理文章 1826/5000\n",
      "处理文章 1827/5000\n",
      "处理文章 1825/5000\n",
      "处理文章 1824/5000\n",
      "处理文章 1828/5000\n",
      "处理文章 1829/5000\n",
      "处理文章 1830/5000\n",
      "处理文章 1831/5000\n",
      "处理文章 1832/5000\n",
      "处理文章 1833/5000\n",
      "处理文章 1834/5000\n",
      "处理文章 1835/5000\n",
      "处理文章 1836/5000\n",
      "处理文章 1837/5000\n",
      "处理文章 1838/5000\n",
      "处理文章 1839/5000\n",
      "处理文章 1840/5000\n",
      "处理文章 1841/5000\n",
      "处理文章 1842/5000\n",
      "处理文章 1843/5000\n",
      "处理文章 1844/5000\n",
      "处理文章 1845/5000\n",
      "处理文章 1846/5000\n",
      "处理文章 1847/5000\n",
      "处理文章 1848/5000\n",
      "处理文章 1849/5000\n",
      "处理文章 1850/5000\n",
      "处理文章 1851/5000\n",
      "处理文章 1852/5000\n",
      "处理文章 1853/5000\n",
      "处理文章 1854/5000\n",
      "处理文章 1856/5000\n",
      "处理文章 1855/5000\n",
      "处理文章 1857/5000\n",
      "处理文章 1859/5000\n",
      "处理文章 1858/5000\n",
      "处理文章 1860/5000\n",
      "处理文章 1861/5000\n",
      "处理文章 1862/5000\n",
      "处理文章 1863/5000\n",
      "处理文章 1865/5000\n",
      "处理文章 1864/5000\n",
      "处理文章 1867/5000\n",
      "处理文章 1866/5000\n",
      "处理文章 1868/5000\n",
      "处理文章 1869/5000\n",
      "处理文章 1870/5000\n",
      "处理文章 1871/5000\n",
      "处理文章 1872/5000\n",
      "处理文章 1873/5000\n",
      "处理文章 1875/5000\n",
      "处理文章 1874/5000\n",
      "处理文章 1877/5000\n",
      "处理文章 1876/5000\n",
      "处理文章 1878/5000\n",
      "处理文章 1880/5000\n",
      "处理文章 1879/5000\n",
      "处理文章 1881/5000\n",
      "处理文章 1882/5000\n",
      "处理文章 1883/5000\n",
      "处理文章 1884/5000\n",
      "处理文章 1886/5000\n",
      "处理文章 1885/5000\n",
      "处理文章 1887/5000\n",
      "处理文章 1888/5000\n",
      "处理文章 1889/5000\n",
      "处理文章 1891/5000\n",
      "处理文章 1890/5000\n",
      "处理文章 1893/5000\n",
      "处理文章 1892/5000\n",
      "处理文章 1894/5000\n",
      "处理文章 1895/5000\n",
      "处理文章 1896/5000\n",
      "处理文章 1897/5000\n",
      "处理文章 1898/5000\n",
      "处理文章 1899/5000\n",
      "处理文章 1900/5000\n",
      "处理文章 1902/5000\n",
      "处理文章 1901/5000\n",
      "处理文章 1903/5000\n",
      "处理文章 1904/5000\n",
      "处理文章 1905/5000\n",
      "处理文章 1906/5000\n",
      "处理文章 1907/5000\n",
      "处理文章 1908/5000\n",
      "处理文章 1909/5000\n",
      "处理文章 1910/5000\n",
      "处理文章 1911/5000\n",
      "处理文章 1912/5000\n",
      "处理文章 1913/5000\n",
      "处理文章 1914/5000\n",
      "处理文章 1915/5000\n",
      "处理文章 1916/5000\n",
      "处理文章 1917/5000\n",
      "处理文章 1920/5000\n",
      "处理文章 1919/5000\n",
      "处理文章 1918/5000\n",
      "处理文章 1921/5000\n",
      "处理文章 1922/5000\n",
      "处理文章 1923/5000\n",
      "处理文章 1924/5000\n",
      "处理文章 1925/5000\n",
      "处理文章 1926/5000\n",
      "处理文章 1927/5000\n",
      "处理文章 1929/5000\n",
      "处理文章 1930/5000\n",
      "处理文章 1928/5000\n",
      "处理文章 1932/5000\n",
      "处理文章 1931/5000\n",
      "处理文章 1933/5000\n",
      "处理文章 1934/5000\n",
      "处理文章 1935/5000\n",
      "处理文章 1936/5000\n",
      "处理文章 1937/5000\n",
      "处理文章 1938/5000\n",
      "处理文章 1940/5000\n",
      "处理文章 1939/5000\n",
      "处理文章 1942/5000\n",
      "处理文章 1941/5000\n",
      "处理文章 1943/5000\n",
      "处理文章 1944/5000\n",
      "处理文章 1945/5000\n",
      "处理文章 1946/5000\n",
      "处理文章 1947/5000\n",
      "处理文章 1948/5000\n",
      "处理文章 1949/5000\n",
      "处理文章 1950/5000\n",
      "处理文章 1951/5000\n",
      "处理文章 1953/5000\n",
      "处理文章 1954/5000\n",
      "处理文章 1952/5000\n",
      "处理文章 1955/5000\n",
      "处理文章 1956/5000\n",
      "处理文章 1957/5000\n",
      "处理文章 1958/5000\n",
      "处理文章 1959/5000\n",
      "处理文章 1961/5000\n",
      "处理文章 1960/5000\n",
      "处理文章 1962/5000\n",
      "处理文章 1963/5000\n",
      "处理文章 1964/5000\n",
      "处理文章 1966/5000\n",
      "处理文章 1965/5000\n",
      "处理文章 1968/5000\n",
      "处理文章 1967/5000\n",
      "处理文章 1969/5000\n",
      "处理文章 1970/5000\n",
      "处理文章 1971/5000\n",
      "处理文章 1973/5000\n",
      "处理文章 1972/5000\n",
      "处理文章 1974/5000\n",
      "处理文章 1976/5000\n",
      "处理文章 1975/5000\n",
      "处理文章 1977/5000\n",
      "处理文章 1978/5000\n",
      "处理文章 1979/5000\n",
      "处理文章 1981/5000\n",
      "处理文章 1982/5000\n",
      "处理文章 1980/5000\n",
      "处理文章 1983/5000\n",
      "处理文章 1984/5000\n",
      "处理文章 1985/5000\n",
      "处理文章 1986/5000\n",
      "处理文章 1987/5000\n",
      "处理文章 1988/5000\n",
      "处理文章 1989/5000\n",
      "处理文章 1990/5000\n",
      "处理文章 1991/5000\n",
      "处理文章 1992/5000\n",
      "处理文章 1993/5000\n",
      "处理文章 1994/5000\n",
      "处理文章 1995/5000\n",
      "处理文章 1996/5000\n",
      "处理文章 1997/5000\n",
      "处理文章 1998/5000\n",
      "处理文章 1999/5000\n",
      "处理文章 2000/5000\n",
      "处理文章 2001/5000\n",
      "处理文章 2002/5000\n",
      "处理文章 2003/5000\n",
      "处理文章 2004/5000\n",
      "处理文章 2006/5000\n",
      "处理文章 2005/5000\n",
      "处理文章 2007/5000\n",
      "处理文章 2008/5000\n",
      "处理文章 2009/5000\n",
      "处理文章 2010/5000\n",
      "处理文章 2011/5000\n",
      "处理文章 2013/5000\n",
      "处理文章 2012/5000\n",
      "处理文章 2014/5000\n",
      "处理文章 2015/5000\n",
      "处理文章 2016/5000\n",
      "处理文章 2017/5000\n",
      "处理文章 2018/5000\n",
      "处理文章 2019/5000\n",
      "处理文章 2020/5000\n",
      "处理文章 2021/5000\n",
      "处理文章 2022/5000\n",
      "处理文章 2023/5000\n",
      "处理文章 2025/5000\n",
      "处理文章 2024/5000\n",
      "处理文章 2026/5000\n",
      "处理文章 2027/5000\n",
      "处理文章 2028/5000\n",
      "处理文章 2029/5000\n",
      "处理文章 2030/5000\n",
      "处理文章 2031/5000\n",
      "处理文章 2032/5000\n",
      "处理文章 2033/5000\n",
      "处理文章 2034/5000\n",
      "处理文章 2035/5000\n",
      "处理文章 2036/5000\n",
      "处理文章 2038/5000\n",
      "处理文章 2039/5000\n",
      "处理文章 2037/5000\n",
      "处理文章 2040/5000\n",
      "处理文章 2041/5000\n",
      "处理文章 2042/5000\n",
      "处理文章 2043/5000\n",
      "处理文章 2044/5000\n",
      "处理文章 2045/5000\n",
      "处理文章 2046/5000\n",
      "处理文章 2048/5000\n",
      "处理文章 2047/5000\n",
      "处理文章 2049/5000\n",
      "处理文章 2050/5000\n",
      "处理文章 2051/5000\n",
      "处理文章 2052/5000\n",
      "处理文章 2053/5000\n",
      "处理文章 2054/5000\n",
      "处理文章 2055/5000\n",
      "处理文章 2056/5000\n",
      "处理文章 2057/5000\n",
      "处理文章 2060/5000\n",
      "处理文章 2058/5000\n",
      "处理文章 2059/5000\n",
      "处理文章 2061/5000\n",
      "处理文章 2063/5000\n",
      "处理文章 2062/5000\n",
      "处理文章 2064/5000\n",
      "处理文章 2065/5000\n",
      "处理文章 2066/5000\n",
      "处理文章 2067/5000\n",
      "处理文章 2068/5000\n",
      "处理文章 2069/5000\n",
      "处理文章 2070/5000\n",
      "处理文章 2071/5000\n",
      "处理文章 2072/5000\n",
      "处理文章 2073/5000\n",
      "处理文章 2074/5000\n",
      "处理文章 2075/5000\n",
      "处理文章 2076/5000\n",
      "处理文章 2078/5000\n",
      "处理文章 2077/5000\n",
      "处理文章 2079/5000\n",
      "处理文章 2080/5000\n",
      "处理文章 2081/5000\n",
      "处理文章 2082/5000\n",
      "处理文章 2083/5000\n",
      "处理文章 2084/5000\n",
      "处理文章 2085/5000\n",
      "处理文章 2086/5000\n",
      "处理文章 2087/5000\n",
      "处理文章 2088/5000\n",
      "处理文章 2089/5000\n",
      "处理文章 2090/5000\n",
      "处理文章 2091/5000\n",
      "处理文章 2092/5000\n",
      "处理文章 2093/5000\n",
      "处理文章 2094/5000\n",
      "处理文章 2095/5000\n",
      "处理文章 2096/5000\n",
      "处理文章 2098/5000\n",
      "处理文章 2097/5000\n",
      "处理文章 2100/5000\n",
      "处理文章 2099/5000\n",
      "处理文章 2101/5000\n",
      "处理文章 2102/5000\n",
      "处理文章 2103/5000\n",
      "处理文章 2104/5000\n",
      "处理文章 2108/5000\n",
      "处理文章 2105/5000\n",
      "处理文章 2106/5000\n",
      "处理文章 2107/5000\n",
      "处理文章 2110/5000\n",
      "处理文章 2109/5000\n",
      "处理文章 2111/5000\n",
      "处理文章 2114/5000\n",
      "处理文章 2113/5000\n",
      "处理文章 2112/5000\n",
      "处理文章 2115/5000\n",
      "处理文章 2117/5000\n",
      "处理文章 2118/5000\n",
      "处理文章 2116/5000\n",
      "处理文章 2119/5000\n",
      "处理文章 2120/5000\n",
      "处理文章 2121/5000\n",
      "处理文章 2122/5000\n",
      "处理文章 2124/5000\n",
      "处理文章 2123/5000\n",
      "处理文章 2125/5000\n",
      "处理文章 2126/5000\n",
      "处理文章 2127/5000\n",
      "处理文章 2128/5000\n",
      "处理文章 2129/5000\n",
      "处理文章 2130/5000\n",
      "处理文章 2131/5000\n",
      "处理文章 2132/5000\n",
      "处理文章 2133/5000\n",
      "处理文章 2134/5000\n",
      "处理文章 2135/5000\n",
      "处理文章 2136/5000\n",
      "处理文章 2137/5000\n",
      "处理文章 2140/5000\n",
      "处理文章 2138/5000\n",
      "处理文章 2139/5000\n",
      "处理文章 2141/5000\n",
      "处理文章 2142/5000\n",
      "处理文章 2143/5000\n",
      "处理文章 2145/5000\n",
      "处理文章 2146/5000\n",
      "处理文章 2144/5000\n",
      "处理文章 2148/5000\n",
      "处理文章 2147/5000\n",
      "处理文章 2149/5000\n",
      "处理文章 2150/5000\n",
      "处理文章 2151/5000\n",
      "处理文章 2152/5000\n",
      "处理文章 2154/5000\n",
      "处理文章 2153/5000\n",
      "处理文章 2155/5000\n",
      "处理文章 2156/5000\n",
      "处理文章 2157/5000\n",
      "处理文章 2158/5000\n",
      "处理文章 2159/5000\n",
      "处理文章 2160/5000\n",
      "处理文章 2161/5000\n",
      "处理文章 2162/5000\n",
      "处理文章 2163/5000\n",
      "处理文章 2164/5000\n",
      "处理文章 2165/5000\n",
      "处理文章 2168/5000\n",
      "处理文章 2167/5000\n",
      "处理文章 2166/5000\n",
      "处理文章 2169/5000\n",
      "处理文章 2170/5000\n",
      "处理文章 2171/5000\n",
      "处理文章 2172/5000\n",
      "处理文章 2173/5000\n",
      "处理文章 2174/5000\n",
      "处理文章 2175/5000\n",
      "处理文章 2176/5000\n",
      "处理文章 2179/5000\n",
      "处理文章 2178/5000\n",
      "处理文章 2182/5000\n",
      "处理文章 2177/5000\n",
      "处理文章 2181/5000\n",
      "处理文章 2180/5000\n",
      "处理文章 2184/5000\n",
      "处理文章 2183/5000\n",
      "处理文章 2185/5000\n",
      "处理文章 2186/5000\n",
      "处理文章 2187/5000\n",
      "处理文章 2188/5000\n",
      "处理文章 2189/5000\n",
      "处理文章 2190/5000\n",
      "处理文章 2191/5000\n",
      "处理文章 2192/5000\n",
      "处理文章 2193/5000\n",
      "处理文章 2194/5000\n",
      "处理文章 2195/5000\n",
      "处理文章 2196/5000\n",
      "处理文章 2197/5000\n",
      "处理文章 2198/5000\n",
      "处理文章 2199/5000\n",
      "处理文章 2201/5000\n",
      "处理文章 2200/5000\n",
      "处理文章 2202/5000\n",
      "处理文章 2204/5000\n",
      "处理文章 2203/5000\n",
      "处理文章 2205/5000\n",
      "处理文章 2207/5000\n",
      "处理文章 2206/5000\n",
      "处理文章 2208/5000\n",
      "处理文章 2209/5000\n",
      "处理文章 2211/5000\n",
      "处理文章 2210/5000\n",
      "处理文章 2213/5000\n",
      "处理文章 2212/5000\n",
      "处理文章 2214/5000\n",
      "处理文章 2216/5000\n",
      "处理文章 2215/5000\n",
      "处理文章 2218/5000\n",
      "处理文章 2217/5000\n",
      "处理文章 2219/5000\n",
      "处理文章 2220/5000\n",
      "处理文章 2221/5000\n",
      "处理文章 2222/5000\n",
      "处理文章 2223/5000\n",
      "处理文章 2224/5000\n",
      "处理文章 2225/5000\n",
      "处理文章 2226/5000\n",
      "处理文章 2227/5000\n",
      "处理文章 2228/5000\n",
      "处理文章 2229/5000\n",
      "处理文章 2230/5000\n",
      "处理文章 2231/5000\n",
      "处理文章 2232/5000\n",
      "处理文章 2233/5000\n",
      "处理文章 2234/5000\n",
      "处理文章 2235/5000\n",
      "处理文章 2236/5000\n",
      "处理文章 2238/5000\n",
      "处理文章 2237/5000\n",
      "处理文章 2239/5000\n",
      "处理文章 2240/5000\n",
      "处理文章 2241/5000\n",
      "处理文章 2242/5000\n",
      "处理文章 2243/5000\n",
      "处理文章 2244/5000\n",
      "处理文章 2245/5000\n",
      "处理文章 2246/5000\n",
      "处理文章 2247/5000\n",
      "处理文章 2248/5000\n",
      "处理文章 2250/5000\n",
      "处理文章 2249/5000\n",
      "处理文章 2251/5000\n",
      "处理文章 2252/5000\n",
      "处理文章 2253/5000\n",
      "处理文章 2255/5000\n",
      "处理文章 2256/5000\n",
      "处理文章 2254/5000\n",
      "处理文章 2257/5000\n",
      "处理文章 2258/5000\n",
      "处理文章 2259/5000\n",
      "处理文章 2260/5000\n",
      "处理文章 2261/5000\n",
      "处理文章 2262/5000\n",
      "处理文章 2263/5000\n",
      "处理文章 2264/5000\n",
      "处理文章 2265/5000\n",
      "处理文章 2266/5000\n",
      "处理文章 2267/5000\n",
      "处理文章 2268/5000\n",
      "处理文章 2270/5000\n",
      "处理文章 2269/5000\n",
      "处理文章 2272/5000\n",
      "处理文章 2271/5000\n",
      "处理文章 2273/5000\n",
      "处理文章 2274/5000\n",
      "处理文章 2275/5000\n",
      "处理文章 2276/5000\n",
      "处理文章 2277/5000\n",
      "处理文章 2278/5000\n",
      "处理文章 2279/5000\n",
      "处理文章 2280/5000\n",
      "处理文章 2281/5000\n",
      "处理文章 2282/5000\n",
      "处理文章 2286/5000\n",
      "处理文章 2283/5000\n",
      "处理文章 2285/5000\n",
      "处理文章 2284/5000\n",
      "处理文章 2287/5000\n",
      "处理文章 2288/5000\n",
      "处理文章 2289/5000\n",
      "处理文章 2290/5000\n",
      "处理文章 2291/5000\n",
      "处理文章 2292/5000\n",
      "处理文章 2293/5000\n",
      "处理文章 2294/5000\n",
      "处理文章 2295/5000\n",
      "处理文章 2296/5000\n",
      "处理文章 2297/5000\n",
      "处理文章 2299/5000\n",
      "处理文章 2298/5000\n",
      "处理文章 2301/5000\n",
      "处理文章 2300/5000\n",
      "处理文章 2302/5000\n",
      "处理文章 2303/5000\n",
      "处理文章 2305/5000\n",
      "处理文章 2304/5000\n",
      "处理文章 2306/5000\n",
      "处理文章 2307/5000\n",
      "处理文章 2308/5000\n",
      "处理文章 2309/5000\n",
      "处理文章 2311/5000\n",
      "处理文章 2310/5000\n",
      "处理文章 2312/5000\n",
      "处理文章 2313/5000\n",
      "处理文章 2314/5000\n",
      "处理文章 2315/5000\n",
      "处理文章 2316/5000\n",
      "处理文章 2317/5000\n",
      "处理文章 2318/5000\n",
      "处理文章 2319/5000\n",
      "处理文章 2321/5000\n",
      "处理文章 2320/5000\n",
      "处理文章 2324/5000\n",
      "处理文章 2323/5000\n",
      "处理文章 2322/5000\n",
      "处理文章 2325/5000\n",
      "处理文章 2326/5000\n",
      "处理文章 2328/5000\n",
      "处理文章 2327/5000\n",
      "处理文章 2329/5000\n",
      "处理文章 2331/5000\n",
      "处理文章 2330/5000\n",
      "处理文章 2333/5000\n",
      "处理文章 2332/5000\n",
      "处理文章 2334/5000\n",
      "处理文章 2335/5000\n",
      "处理文章 2337/5000\n",
      "处理文章 2336/5000\n",
      "处理文章 2339/5000\n",
      "处理文章 2338/5000\n",
      "处理文章 2340/5000\n",
      "处理文章 2341/5000\n",
      "处理文章 2342/5000\n",
      "处理文章 2343/5000\n",
      "处理文章 2347/5000\n",
      "处理文章 2344/5000\n",
      "处理文章 2345/5000\n",
      "处理文章 2346/5000\n",
      "处理文章 2348/5000\n",
      "处理文章 2351/5000\n",
      "处理文章 2349/5000\n",
      "处理文章 2350/5000\n",
      "处理文章 2352/5000\n",
      "处理文章 2353/5000\n",
      "处理文章 2354/5000\n",
      "处理文章 2355/5000\n",
      "处理文章 2356/5000\n",
      "处理文章 2357/5000\n",
      "处理文章 2359/5000\n",
      "处理文章 2358/5000\n",
      "处理文章 2360/5000\n",
      "处理文章 2361/5000\n",
      "处理文章 2363/5000\n",
      "处理文章 2362/5000\n",
      "处理文章 2364/5000\n",
      "处理文章 2365/5000\n",
      "处理文章 2366/5000\n",
      "处理文章 2367/5000\n",
      "处理文章 2368/5000\n",
      "处理文章 2369/5000\n",
      "处理文章 2370/5000\n",
      "处理文章 2371/5000\n",
      "处理文章 2373/5000\n",
      "处理文章 2372/5000\n",
      "处理文章 2374/5000\n",
      "处理文章 2375/5000\n",
      "处理文章 2376/5000\n",
      "处理文章 2377/5000\n",
      "处理文章 2379/5000\n",
      "处理文章 2378/5000\n",
      "处理文章 2380/5000\n",
      "处理文章 2381/5000\n",
      "无法在响应中找到有效的JSON\n",
      "文章 2366/5000: 问题生成失败\n",
      "处理文章 2383/5000\n",
      "处理文章 2382/5000\n",
      "处理文章 2384/5000\n",
      "处理文章 2385/5000\n",
      "处理文章 2386/5000\n",
      "处理文章 2387/5000\n",
      "处理文章 2388/5000\n",
      "处理文章 2389/5000\n",
      "处理文章 2390/5000\n",
      "处理文章 2393/5000\n",
      "处理文章 2392/5000\n",
      "处理文章 2391/5000\n",
      "处理文章 2394/5000\n",
      "处理文章 2396/5000\n",
      "处理文章 2395/5000\n",
      "处理文章 2398/5000\n",
      "处理文章 2397/5000\n",
      "处理文章 2400/5000\n",
      "处理文章 2399/5000\n",
      "处理文章 2401/5000\n",
      "处理文章 2402/5000\n",
      "处理文章 2403/5000\n",
      "处理文章 2404/5000\n",
      "处理文章 2405/5000\n",
      "处理文章 2406/5000\n",
      "处理文章 2407/5000\n",
      "处理文章 2408/5000\n",
      "处理文章 2409/5000\n",
      "处理文章 2410/5000\n",
      "处理文章 2411/5000\n",
      "处理文章 2412/5000\n",
      "处理文章 2414/5000\n",
      "处理文章 2413/5000\n",
      "处理文章 2416/5000\n",
      "处理文章 2415/5000\n",
      "处理文章 2417/5000\n",
      "处理文章 2418/5000\n",
      "处理文章 2420/5000\n",
      "处理文章 2419/5000\n",
      "处理文章 2421/5000\n",
      "处理文章 2422/5000\n",
      "处理文章 2423/5000\n",
      "处理文章 2425/5000\n",
      "处理文章 2424/5000\n",
      "处理文章 2426/5000\n",
      "处理文章 2427/5000\n",
      "处理文章 2428/5000\n",
      "处理文章 2430/5000\n",
      "处理文章 2429/5000\n",
      "处理文章 2431/5000\n",
      "处理文章 2432/5000\n",
      "处理文章 2433/5000\n",
      "处理文章 2434/5000\n",
      "处理文章 2436/5000\n",
      "处理文章 2435/5000\n",
      "处理文章 2437/5000\n",
      "处理文章 2440/5000\n",
      "处理文章 2438/5000\n",
      "处理文章 2439/5000\n",
      "处理文章 2441/5000\n",
      "处理文章 2442/5000\n",
      "处理文章 2444/5000\n",
      "处理文章 2443/5000\n",
      "处理文章 2445/5000\n",
      "处理文章 2446/5000\n",
      "处理文章 2447/5000\n",
      "处理文章 2449/5000\n",
      "处理文章 2448/5000\n",
      "处理文章 2450/5000\n",
      "处理文章 2451/5000\n",
      "处理文章 2452/5000\n",
      "处理文章 2453/5000\n",
      "处理文章 2454/5000\n",
      "处理文章 2455/5000\n",
      "处理文章 2456/5000\n",
      "处理文章 2457/5000\n",
      "处理文章 2458/5000\n",
      "处理文章 2459/5000\n",
      "处理文章 2460/5000\n",
      "处理文章 2461/5000\n",
      "处理文章 2462/5000\n",
      "处理文章 2463/5000\n",
      "处理文章 2464/5000\n",
      "处理文章 2465/5000\n",
      "处理文章 2466/5000\n",
      "处理文章 2467/5000\n",
      "处理文章 2469/5000\n",
      "处理文章 2468/5000\n",
      "处理文章 2470/5000\n",
      "处理文章 2471/5000\n",
      "处理文章 2472/5000\n",
      "处理文章 2474/5000\n",
      "处理文章 2473/5000\n",
      "处理文章 2475/5000\n",
      "处理文章 2476/5000\n",
      "处理文章 2477/5000\n",
      "处理文章 2478/5000\n",
      "处理文章 2479/5000\n",
      "处理文章 2480/5000\n",
      "处理文章 2482/5000\n",
      "处理文章 2481/5000\n",
      "处理文章 2483/5000\n",
      "处理文章 2484/5000\n",
      "处理文章 2485/5000\n",
      "处理文章 2486/5000\n",
      "处理文章 2487/5000\n",
      "处理文章 2488/5000\n",
      "处理文章 2489/5000\n",
      "处理文章 2490/5000\n",
      "处理文章 2491/5000\n",
      "处理文章 2492/5000\n",
      "处理文章 2493/5000\n",
      "处理文章 2494/5000\n",
      "处理文章 2496/5000\n",
      "处理文章 2495/5000\n",
      "处理文章 2497/5000\n",
      "处理文章 2498/5000\n",
      "处理文章 2500/5000\n",
      "处理文章 2499/5000\n",
      "处理文章 2502/5000\n",
      "处理文章 2501/5000\n",
      "处理文章 2503/5000\n",
      "处理文章 2504/5000\n",
      "处理文章 2505/5000\n",
      "处理文章 2508/5000\n",
      "处理文章 2507/5000\n",
      "处理文章 2506/5000\n",
      "处理文章 2509/5000\n",
      "处理文章 2510/5000\n",
      "处理文章 2511/5000\n",
      "处理文章 2512/5000\n",
      "处理文章 2513/5000\n",
      "处理文章 2514/5000\n",
      "处理文章 2515/5000\n",
      "处理文章 2516/5000\n",
      "处理文章 2517/5000\n",
      "处理文章 2518/5000\n",
      "处理文章 2519/5000\n",
      "处理文章 2520/5000\n",
      "处理文章 2521/5000\n",
      "处理文章 2523/5000\n",
      "处理文章 2522/5000\n",
      "处理文章 2524/5000\n",
      "处理文章 2525/5000\n",
      "处理文章 2526/5000\n",
      "处理文章 2527/5000\n",
      "处理文章 2528/5000\n",
      "处理文章 2530/5000\n",
      "处理文章 2529/5000\n",
      "处理文章 2531/5000\n",
      "处理文章 2532/5000\n",
      "处理文章 2533/5000\n",
      "处理文章 2534/5000\n",
      "处理文章 2535/5000\n",
      "处理文章 2537/5000\n",
      "处理文章 2536/5000\n",
      "处理文章 2538/5000\n",
      "处理文章 2540/5000\n",
      "处理文章 2539/5000\n",
      "处理文章 2542/5000\n",
      "处理文章 2541/5000\n",
      "处理文章 2543/5000\n",
      "处理文章 2544/5000\n",
      "处理文章 2545/5000\n",
      "处理文章 2548/5000\n",
      "处理文章 2546/5000\n",
      "处理文章 2547/5000\n",
      "处理文章 2549/5000\n",
      "处理文章 2550/5000\n",
      "处理文章 2551/5000\n",
      "处理文章 2552/5000\n",
      "处理文章 2553/5000\n",
      "处理文章 2554/5000\n",
      "处理文章 2555/5000\n",
      "处理文章 2556/5000\n",
      "处理文章 2557/5000\n",
      "处理文章 2559/5000\n",
      "处理文章 2560/5000\n",
      "处理文章 2558/5000\n",
      "处理文章 2561/5000\n",
      "处理文章 2564/5000\n",
      "处理文章 2562/5000\n",
      "处理文章 2563/5000\n",
      "处理文章 2565/5000\n",
      "处理文章 2566/5000\n",
      "处理文章 2567/5000\n",
      "处理文章 2569/5000\n",
      "处理文章 2568/5000\n",
      "处理文章 2570/5000\n",
      "处理文章 2571/5000\n",
      "处理文章 2574/5000\n",
      "处理文章 2572/5000\n",
      "处理文章 2573/5000\n",
      "处理文章 2575/5000\n",
      "处理文章 2576/5000\n",
      "处理文章 2577/5000\n",
      "处理文章 2578/5000\n",
      "处理文章 2580/5000\n",
      "处理文章 2579/5000\n",
      "处理文章 2582/5000\n",
      "处理文章 2581/5000\n",
      "处理文章 2583/5000\n",
      "处理文章 2584/5000\n",
      "处理文章 2585/5000\n",
      "处理文章 2586/5000\n",
      "处理文章 2587/5000\n",
      "处理文章 2588/5000\n",
      "处理文章 2589/5000\n",
      "处理文章 2590/5000\n",
      "处理文章 2591/5000\n",
      "处理文章 2592/5000\n",
      "处理文章 2594/5000\n",
      "处理文章 2593/5000\n",
      "处理文章 2595/5000\n",
      "处理文章 2597/5000\n",
      "处理文章 2596/5000\n",
      "处理文章 2598/5000\n",
      "处理文章 2599/5000\n",
      "处理文章 2600/5000\n",
      "处理文章 2601/5000\n",
      "处理文章 2602/5000\n",
      "处理文章 2604/5000\n",
      "处理文章 2603/5000\n",
      "处理文章 2605/5000\n",
      "处理文章 2606/5000\n",
      "处理文章 2608/5000\n",
      "处理文章 2607/5000\n",
      "处理文章 2609/5000\n",
      "处理文章 2610/5000\n",
      "处理文章 2611/5000\n",
      "处理文章 2612/5000\n",
      "处理文章 2614/5000\n",
      "处理文章 2613/5000\n",
      "处理文章 2617/5000\n",
      "处理文章 2615/5000\n",
      "处理文章 2616/5000\n",
      "处理文章 2618/5000\n",
      "处理文章 2620/5000\n",
      "处理文章 2619/5000\n",
      "处理文章 2621/5000\n",
      "处理文章 2622/5000\n",
      "处理文章 2623/5000\n",
      "处理文章 2625/5000\n",
      "处理文章 2624/5000\n",
      "处理文章 2626/5000\n",
      "处理文章 2627/5000\n",
      "处理文章 2628/5000\n",
      "处理文章 2630/5000\n",
      "处理文章 2629/5000\n",
      "处理文章 2633/5000\n",
      "处理文章 2634/5000\n",
      "处理文章 2631/5000\n",
      "处理文章 2632/5000\n",
      "处理文章 2635/5000\n",
      "处理文章 2636/5000\n",
      "处理文章 2637/5000\n",
      "处理文章 2639/5000\n",
      "处理文章 2638/5000\n",
      "处理文章 2642/5000\n",
      "处理文章 2640/5000\n",
      "处理文章 2641/5000\n",
      "处理文章 2643/5000\n",
      "处理文章 2645/5000\n",
      "处理文章 2644/5000\n",
      "处理文章 2646/5000\n",
      "处理文章 2647/5000\n",
      "处理文章 2648/5000\n",
      "处理文章 2649/5000\n",
      "处理文章 2650/5000\n",
      "处理文章 2651/5000\n",
      "处理文章 2652/5000\n",
      "处理文章 2653/5000\n",
      "处理文章 2654/5000\n",
      "处理文章 2655/5000\n",
      "处理文章 2657/5000\n",
      "处理文章 2656/5000\n",
      "处理文章 2659/5000\n",
      "处理文章 2658/5000\n",
      "处理文章 2660/5000\n",
      "处理文章 2661/5000\n",
      "处理文章 2662/5000\n",
      "处理文章 2665/5000\n",
      "处理文章 2663/5000\n",
      "处理文章 2666/5000\n",
      "处理文章 2664/5000\n",
      "处理文章 2667/5000\n",
      "处理文章 2668/5000\n",
      "处理文章 2669/5000\n",
      "处理文章 2671/5000\n",
      "处理文章 2670/5000\n",
      "处理文章 2673/5000\n",
      "处理文章 2672/5000\n",
      "处理文章 2674/5000\n",
      "处理文章 2675/5000\n",
      "处理文章 2676/5000\n",
      "处理文章 2677/5000\n",
      "处理文章 2678/5000\n",
      "处理文章 2679/5000\n",
      "处理文章 2681/5000\n",
      "处理文章 2680/5000\n",
      "处理文章 2683/5000\n",
      "处理文章 2682/5000\n",
      "处理文章 2684/5000\n",
      "处理文章 2685/5000\n",
      "处理文章 2686/5000\n",
      "处理文章 2687/5000\n",
      "处理文章 2688/5000\n",
      "处理文章 2689/5000\n",
      "处理文章 2690/5000\n",
      "处理文章 2692/5000\n",
      "处理文章 2691/5000\n",
      "处理文章 2693/5000\n",
      "处理文章 2694/5000\n",
      "处理文章 2696/5000\n",
      "处理文章 2695/5000\n",
      "处理文章 2697/5000\n",
      "处理文章 2699/5000\n",
      "处理文章 2698/5000\n",
      "处理文章 2700/5000\n",
      "处理文章 2701/5000\n",
      "处理文章 2702/5000\n",
      "处理文章 2704/5000\n",
      "处理文章 2703/5000\n",
      "处理文章 2706/5000\n",
      "处理文章 2705/5000\n",
      "处理文章 2707/5000\n",
      "处理文章 2708/5000\n",
      "处理文章 2709/5000\n",
      "处理文章 2710/5000\n",
      "处理文章 2711/5000\n",
      "处理文章 2712/5000\n",
      "处理文章 2713/5000\n",
      "处理文章 2714/5000\n",
      "处理文章 2715/5000\n",
      "处理文章 2716/5000\n",
      "处理文章 2717/5000\n",
      "处理文章 2718/5000\n",
      "处理文章 2719/5000\n",
      "处理文章 2721/5000\n",
      "处理文章 2722/5000\n",
      "处理文章 2720/5000\n",
      "处理文章 2723/5000\n",
      "处理文章 2724/5000\n",
      "处理文章 2725/5000\n",
      "处理文章 2726/5000\n",
      "处理文章 2727/5000\n",
      "处理文章 2728/5000\n",
      "处理文章 2729/5000\n",
      "处理文章 2730/5000\n",
      "处理文章 2731/5000\n",
      "处理文章 2732/5000\n",
      "处理文章 2733/5000\n",
      "处理文章 2734/5000\n",
      "处理文章 2736/5000\n",
      "处理文章 2737/5000\n",
      "处理文章 2735/5000\n",
      "处理文章 2738/5000\n",
      "处理文章 2739/5000\n",
      "处理文章 2741/5000\n",
      "处理文章 2742/5000\n",
      "处理文章 2740/5000\n",
      "处理文章 2743/5000\n",
      "处理文章 2744/5000\n",
      "处理文章 2745/5000\n",
      "处理文章 2746/5000\n",
      "处理文章 2750/5000\n",
      "处理文章 2749/5000\n",
      "处理文章 2747/5000\n",
      "处理文章 2748/5000\n",
      "处理文章 2751/5000\n",
      "处理文章 2752/5000\n",
      "处理文章 2753/5000\n",
      "处理文章 2754/5000\n",
      "处理文章 2755/5000\n",
      "处理文章 2756/5000\n",
      "处理文章 2758/5000\n",
      "处理文章 2757/5000\n",
      "处理文章 2759/5000\n",
      "处理文章 2760/5000\n",
      "处理文章 2761/5000\n",
      "处理文章 2762/5000\n",
      "处理文章 2764/5000\n",
      "处理文章 2763/5000\n",
      "处理文章 2765/5000\n",
      "处理文章 2766/5000\n",
      "处理文章 2767/5000\n",
      "处理文章 2769/5000\n",
      "处理文章 2768/5000\n",
      "处理文章 2770/5000\n",
      "处理文章 2771/5000\n",
      "处理文章 2772/5000\n",
      "处理文章 2773/5000\n",
      "处理文章 2774/5000\n",
      "处理文章 2777/5000\n",
      "处理文章 2775/5000\n",
      "处理文章 2778/5000\n",
      "处理文章 2776/5000\n",
      "处理文章 2779/5000\n",
      "处理文章 2781/5000\n",
      "处理文章 2780/5000\n",
      "处理文章 2783/5000\n",
      "处理文章 2782/5000\n",
      "处理文章 2784/5000\n",
      "处理文章 2785/5000\n",
      "处理文章 2786/5000\n",
      "处理文章 2789/5000\n",
      "处理文章 2787/5000\n",
      "处理文章 2788/5000\n",
      "处理文章 2791/5000\n",
      "处理文章 2792/5000\n",
      "处理文章 2790/5000\n",
      "处理文章 2793/5000\n",
      "处理文章 2794/5000\n",
      "处理文章 2795/5000\n",
      "处理文章 2796/5000\n",
      "处理文章 2797/5000\n",
      "处理文章 2798/5000\n",
      "处理文章 2799/5000\n",
      "处理文章 2800/5000\n",
      "处理文章 2801/5000\n",
      "处理文章 2802/5000\n",
      "处理文章 2804/5000\n",
      "处理文章 2805/5000\n",
      "处理文章 2803/5000\n",
      "处理文章 2806/5000\n",
      "处理文章 2807/5000\n",
      "处理文章 2808/5000\n",
      "处理文章 2809/5000\n",
      "处理文章 2810/5000\n",
      "处理文章 2811/5000\n",
      "处理文章 2812/5000\n",
      "处理文章 2813/5000\n",
      "处理文章 2814/5000\n",
      "处理文章 2815/5000\n",
      "处理文章 2817/5000\n",
      "处理文章 2818/5000\n",
      "处理文章 2816/5000\n",
      "处理文章 2819/5000\n",
      "处理文章 2821/5000\n",
      "处理文章 2822/5000\n",
      "处理文章 2820/5000\n",
      "处理文章 2823/5000\n",
      "处理文章 2825/5000\n",
      "处理文章 2824/5000\n",
      "处理文章 2827/5000\n",
      "处理文章 2826/5000\n",
      "处理文章 2828/5000\n",
      "处理文章 2829/5000\n",
      "处理文章 2830/5000\n",
      "处理文章 2831/5000\n",
      "处理文章 2832/5000\n",
      "处理文章 2833/5000\n",
      "处理文章 2834/5000\n",
      "处理文章 2835/5000\n",
      "处理文章 2836/5000\n",
      "处理文章 2837/5000\n",
      "处理文章 2838/5000\n",
      "处理文章 2839/5000\n",
      "处理文章 2840/5000\n",
      "处理文章 2841/5000\n",
      "处理文章 2843/5000\n",
      "处理文章 2842/5000\n",
      "处理文章 2844/5000\n",
      "处理文章 2846/5000\n",
      "处理文章 2845/5000\n",
      "处理文章 2847/5000\n",
      "处理文章 2848/5000\n",
      "处理文章 2849/5000\n",
      "处理文章 2851/5000\n",
      "处理文章 2850/5000\n",
      "处理文章 2852/5000\n",
      "处理文章 2855/5000\n",
      "处理文章 2853/5000\n",
      "处理文章 2854/5000\n",
      "处理文章 2856/5000\n",
      "处理文章 2857/5000\n",
      "处理文章 2858/5000\n",
      "处理文章 2859/5000\n",
      "处理文章 2860/5000\n",
      "处理文章 2862/5000\n",
      "处理文章 2861/5000\n",
      "处理文章 2865/5000\n",
      "处理文章 2864/5000\n",
      "处理文章 2863/5000\n",
      "处理文章 2866/5000\n",
      "处理文章 2868/5000\n",
      "处理文章 2867/5000\n",
      "处理文章 2870/5000\n",
      "处理文章 2869/5000\n",
      "处理文章 2871/5000\n",
      "处理文章 2872/5000\n",
      "处理文章 2873/5000\n",
      "处理文章 2874/5000\n",
      "处理文章 2875/5000\n",
      "处理文章 2876/5000\n",
      "处理文章 2877/5000\n",
      "处理文章 2878/5000\n",
      "处理文章 2879/5000\n",
      "处理文章 2880/5000\n",
      "处理文章 2881/5000\n",
      "处理文章 2882/5000\n",
      "处理文章 2883/5000\n",
      "处理文章 2884/5000\n",
      "处理文章 2885/5000\n",
      "处理文章 2886/5000\n",
      "处理文章 2887/5000\n",
      "处理文章 2888/5000\n",
      "处理文章 2891/5000\n",
      "处理文章 2889/5000\n",
      "处理文章 2890/5000\n",
      "处理文章 2892/5000\n",
      "处理文章 2893/5000\n",
      "处理文章 2894/5000\n",
      "处理文章 2895/5000\n",
      "处理文章 2897/5000\n",
      "处理文章 2896/5000\n",
      "处理文章 2899/5000\n",
      "处理文章 2898/5000\n",
      "处理文章 2900/5000\n",
      "处理文章 2902/5000\n",
      "处理文章 2901/5000\n",
      "处理文章 2904/5000\n",
      "处理文章 2903/5000\n",
      "处理文章 2905/5000\n",
      "处理文章 2906/5000\n",
      "处理文章 2907/5000\n",
      "处理文章 2908/5000\n",
      "处理文章 2909/5000\n",
      "处理文章 2910/5000\n",
      "处理文章 2911/5000\n",
      "处理文章 2912/5000\n",
      "处理文章 2913/5000\n",
      "处理文章 2914/5000\n",
      "处理文章 2915/5000\n",
      "处理文章 2916/5000\n",
      "处理文章 2917/5000\n",
      "处理文章 2918/5000\n",
      "处理文章 2919/5000\n",
      "处理文章 2920/5000\n",
      "处理文章 2921/5000\n",
      "处理文章 2922/5000\n",
      "处理文章 2923/5000\n",
      "处理文章 2925/5000\n",
      "处理文章 2924/5000\n",
      "处理文章 2926/5000\n",
      "处理文章 2927/5000\n",
      "处理文章 2928/5000\n",
      "处理文章 2929/5000\n",
      "处理文章 2930/5000\n",
      "处理文章 2931/5000\n",
      "处理文章 2933/5000\n",
      "处理文章 2932/5000\n",
      "处理文章 2934/5000\n",
      "处理文章 2936/5000\n",
      "处理文章 2935/5000\n",
      "处理文章 2937/5000\n",
      "处理文章 2938/5000\n",
      "处理文章 2939/5000\n",
      "处理文章 2940/5000\n",
      "处理文章 2942/5000\n",
      "处理文章 2941/5000\n",
      "处理文章 2944/5000\n",
      "处理文章 2943/5000\n",
      "处理文章 2945/5000\n",
      "处理文章 2946/5000\n",
      "处理文章 2947/5000\n",
      "处理文章 2950/5000\n",
      "处理文章 2948/5000\n",
      "处理文章 2951/5000\n",
      "处理文章 2949/5000\n",
      "处理文章 2952/5000\n",
      "处理文章 2953/5000\n",
      "处理文章 2954/5000\n",
      "处理文章 2955/5000\n",
      "处理文章 2956/5000\n",
      "处理文章 2957/5000\n",
      "处理文章 2958/5000\n",
      "处理文章 2959/5000\n",
      "处理文章 2960/5000\n",
      "处理文章 2961/5000\n",
      "处理文章 2962/5000\n",
      "处理文章 2963/5000\n",
      "处理文章 2964/5000\n",
      "处理文章 2965/5000\n",
      "处理文章 2966/5000\n",
      "处理文章 2967/5000\n",
      "处理文章 2969/5000\n",
      "处理文章 2968/5000\n",
      "处理文章 2970/5000\n",
      "处理文章 2971/5000\n",
      "处理文章 2972/5000\n",
      "处理文章 2973/5000\n",
      "处理文章 2974/5000\n",
      "处理文章 2975/5000\n",
      "处理文章 2976/5000\n",
      "处理文章 2978/5000\n",
      "处理文章 2980/5000\n",
      "处理文章 2979/5000\n",
      "处理文章 2977/5000\n",
      "处理文章 2981/5000\n",
      "处理文章 2982/5000\n",
      "处理文章 2983/5000\n",
      "处理文章 2984/5000\n",
      "处理文章 2985/5000\n",
      "处理文章 2986/5000\n",
      "处理文章 2987/5000\n",
      "处理文章 2988/5000\n",
      "处理文章 2989/5000\n",
      "处理文章 2990/5000\n",
      "处理文章 2991/5000\n",
      "处理文章 2992/5000\n",
      "处理文章 2993/5000\n",
      "处理文章 2994/5000\n",
      "处理文章 2995/5000\n",
      "处理文章 2997/5000\n",
      "处理文章 2996/5000\n",
      "处理文章 2998/5000\n",
      "处理文章 2999/5000\n",
      "处理文章 3000/5000\n",
      "处理文章 3001/5000\n",
      "处理文章 3002/5000\n",
      "处理文章 3003/5000\n",
      "处理文章 3005/5000\n",
      "处理文章 3004/5000\n",
      "处理文章 3006/5000\n",
      "处理文章 3007/5000\n",
      "处理文章 3008/5000\n",
      "处理文章 3010/5000\n",
      "处理文章 3011/5000\n",
      "处理文章 3009/5000\n",
      "处理文章 3012/5000\n",
      "处理文章 3013/5000\n",
      "处理文章 3014/5000\n",
      "处理文章 3015/5000\n",
      "处理文章 3016/5000\n",
      "处理文章 3017/5000\n",
      "处理文章 3018/5000\n",
      "处理文章 3019/5000\n",
      "处理文章 3020/5000\n",
      "处理文章 3022/5000\n",
      "处理文章 3021/5000\n",
      "处理文章 3024/5000\n",
      "处理文章 3023/5000\n",
      "处理文章 3025/5000\n",
      "处理文章 3027/5000\n",
      "处理文章 3026/5000\n",
      "处理文章 3028/5000\n",
      "处理文章 3029/5000\n",
      "处理文章 3030/5000\n",
      "处理文章 3031/5000\n",
      "处理文章 3032/5000\n",
      "处理文章 3033/5000\n",
      "处理文章 3034/5000\n",
      "处理文章 3035/5000\n",
      "处理文章 3036/5000\n",
      "处理文章 3039/5000\n",
      "处理文章 3037/5000\n",
      "处理文章 3038/5000\n",
      "处理文章 3040/5000\n",
      "处理文章 3041/5000\n",
      "处理文章 3042/5000\n",
      "处理文章 3043/5000\n",
      "处理文章 3044/5000\n",
      "处理文章 3045/5000\n",
      "处理文章 3046/5000\n",
      "处理文章 3047/5000\n",
      "处理文章 3048/5000\n",
      "处理文章 3049/5000\n",
      "处理文章 3050/5000\n",
      "处理文章 3052/5000\n",
      "处理文章 3051/5000\n",
      "处理文章 3053/5000\n",
      "处理文章 3054/5000\n",
      "处理文章 3056/5000\n",
      "处理文章 3055/5000\n",
      "处理文章 3057/5000\n",
      "处理文章 3058/5000\n",
      "处理文章 3059/5000\n",
      "处理文章 3060/5000\n",
      "处理文章 3061/5000\n",
      "处理文章 3063/5000\n",
      "处理文章 3062/5000\n",
      "处理文章 3064/5000\n",
      "处理文章 3065/5000\n",
      "处理文章 3067/5000\n",
      "处理文章 3066/5000\n",
      "处理文章 3068/5000\n",
      "处理文章 3069/5000\n",
      "处理文章 3070/5000\n",
      "处理文章 3071/5000\n",
      "处理文章 3072/5000\n",
      "处理文章 3073/5000\n",
      "处理文章 3075/5000\n",
      "处理文章 3074/5000\n",
      "处理文章 3077/5000\n",
      "处理文章 3076/5000\n",
      "处理文章 3078/5000\n",
      "处理文章 3079/5000\n",
      "处理文章 3080/5000\n",
      "处理文章 3081/5000\n",
      "处理文章 3082/5000\n",
      "处理文章 3084/5000\n",
      "处理文章 3083/5000\n",
      "处理文章 3085/5000\n",
      "处理文章 3087/5000\n",
      "处理文章 3086/5000\n",
      "处理文章 3089/5000\n",
      "处理文章 3088/5000\n",
      "处理文章 3090/5000\n",
      "处理文章 3091/5000\n",
      "处理文章 3092/5000\n",
      "处理文章 3093/5000\n",
      "处理文章 3095/5000\n",
      "处理文章 3096/5000\n",
      "处理文章 3094/5000\n",
      "处理文章 3097/5000\n",
      "处理文章 3098/5000\n",
      "处理文章 3099/5000\n",
      "处理文章 3100/5000\n",
      "处理文章 3101/5000\n",
      "处理文章 3102/5000\n",
      "处理文章 3103/5000\n",
      "处理文章 3104/5000\n",
      "处理文章 3105/5000\n",
      "处理文章 3106/5000\n",
      "处理文章 3107/5000\n",
      "处理文章 3108/5000\n",
      "处理文章 3109/5000\n",
      "处理文章 3111/5000\n",
      "处理文章 3113/5000\n",
      "处理文章 3112/5000\n",
      "处理文章 3110/5000\n",
      "处理文章 3114/5000\n",
      "处理文章 3115/5000\n",
      "处理文章 3116/5000\n",
      "处理文章 3117/5000\n",
      "处理文章 3118/5000\n",
      "处理文章 3120/5000\n",
      "处理文章 3119/5000\n",
      "处理文章 3122/5000\n",
      "处理文章 3121/5000\n",
      "处理文章 3123/5000\n",
      "处理文章 3124/5000\n",
      "处理文章 3125/5000\n",
      "处理文章 3126/5000\n",
      "处理文章 3127/5000\n",
      "处理文章 3128/5000\n",
      "处理文章 3129/5000\n",
      "处理文章 3130/5000\n",
      "处理文章 3131/5000\n",
      "处理文章 3132/5000\n",
      "处理文章 3133/5000\n",
      "处理文章 3134/5000\n",
      "处理文章 3135/5000\n",
      "处理文章 3138/5000\n",
      "处理文章 3136/5000\n",
      "处理文章 3137/5000\n",
      "处理文章 3139/5000\n",
      "处理文章 3140/5000\n",
      "处理文章 3141/5000\n",
      "处理文章 3142/5000\n",
      "处理文章 3144/5000\n",
      "处理文章 3143/5000\n",
      "处理文章 3146/5000\n",
      "处理文章 3147/5000\n",
      "处理文章 3145/5000\n",
      "处理文章 3148/5000\n",
      "处理文章 3149/5000\n",
      "处理文章 3150/5000\n",
      "处理文章 3151/5000\n",
      "处理文章 3153/5000\n",
      "处理文章 3154/5000\n",
      "处理文章 3156/5000\n",
      "处理文章 3152/5000\n",
      "处理文章 3155/5000\n",
      "处理文章 3157/5000\n",
      "处理文章 3159/5000\n",
      "处理文章 3158/5000\n",
      "处理文章 3161/5000\n",
      "处理文章 3160/5000\n",
      "处理文章 3162/5000\n",
      "处理文章 3163/5000\n",
      "处理文章 3165/5000\n",
      "处理文章 3164/5000\n",
      "处理文章 3166/5000\n",
      "处理文章 3167/5000\n",
      "处理文章 3170/5000\n",
      "处理文章 3168/5000\n",
      "处理文章 3169/5000\n",
      "处理文章 3172/5000\n",
      "处理文章 3171/5000\n",
      "处理文章 3173/5000\n",
      "处理文章 3174/5000\n",
      "处理文章 3175/5000\n",
      "处理文章 3176/5000\n",
      "处理文章 3177/5000\n",
      "处理文章 3178/5000\n",
      "处理文章 3179/5000\n",
      "处理文章 3180/5000\n",
      "处理文章 3181/5000\n",
      "处理文章 3182/5000\n",
      "处理文章 3183/5000\n",
      "处理文章 3185/5000\n",
      "处理文章 3186/5000\n",
      "处理文章 3184/5000\n",
      "处理文章 3187/5000\n",
      "处理文章 3188/5000\n",
      "处理文章 3189/5000\n",
      "处理文章 3191/5000\n",
      "处理文章 3190/5000\n",
      "处理文章 3192/5000\n",
      "处理文章 3193/5000\n",
      "处理文章 3194/5000\n",
      "处理文章 3195/5000\n",
      "处理文章 3196/5000\n",
      "处理文章 3197/5000\n",
      "处理文章 3199/5000\n",
      "处理文章 3198/5000\n",
      "处理文章 3200/5000\n",
      "处理文章 3202/5000\n",
      "处理文章 3201/5000\n",
      "处理文章 3203/5000\n",
      "处理文章 3204/5000\n",
      "处理文章 3206/5000\n",
      "处理文章 3207/5000\n",
      "处理文章 3205/5000\n",
      "处理文章 3208/5000\n",
      "处理文章 3209/5000\n",
      "处理文章 3210/5000\n",
      "处理文章 3211/5000\n",
      "处理文章 3212/5000\n",
      "处理文章 3213/5000\n",
      "处理文章 3214/5000\n",
      "处理文章 3215/5000\n",
      "处理文章 3216/5000\n",
      "处理文章 3217/5000\n",
      "处理文章 3218/5000\n",
      "处理文章 3219/5000\n",
      "处理文章 3220/5000\n",
      "处理文章 3221/5000\n",
      "处理文章 3223/5000\n",
      "处理文章 3222/5000\n",
      "处理文章 3224/5000\n",
      "处理文章 3225/5000\n",
      "处理文章 3227/5000\n",
      "处理文章 3226/5000\n",
      "处理文章 3228/5000\n",
      "处理文章 3230/5000\n",
      "处理文章 3229/5000\n",
      "处理文章 3231/5000\n",
      "处理文章 3233/5000\n",
      "处理文章 3234/5000\n",
      "处理文章 3232/5000\n",
      "处理文章 3235/5000\n",
      "处理文章 3236/5000\n",
      "处理文章 3238/5000\n",
      "处理文章 3237/5000\n",
      "处理文章 3242/5000\n",
      "处理文章 3239/5000\n",
      "处理文章 3240/5000\n",
      "处理文章 3241/5000\n",
      "处理文章 3243/5000\n",
      "处理文章 3244/5000\n",
      "处理文章 3245/5000\n",
      "处理文章 3247/5000\n",
      "处理文章 3246/5000\n",
      "处理文章 3248/5000\n",
      "处理文章 3249/5000\n",
      "处理文章 3250/5000\n",
      "处理文章 3251/5000\n",
      "处理文章 3252/5000\n",
      "处理文章 3253/5000\n",
      "处理文章 3254/5000\n",
      "处理文章 3255/5000\n",
      "处理文章 3256/5000\n",
      "处理文章 3258/5000\n",
      "处理文章 3257/5000\n",
      "处理文章 3260/5000\n",
      "处理文章 3261/5000\n",
      "处理文章 3262/5000\n",
      "处理文章 3259/5000\n",
      "处理文章 3263/5000\n",
      "处理文章 3264/5000\n",
      "处理文章 3265/5000\n",
      "处理文章 3268/5000\n",
      "处理文章 3266/5000\n",
      "处理文章 3267/5000\n",
      "处理文章 3269/5000\n",
      "处理文章 3270/5000\n",
      "处理文章 3271/5000\n",
      "处理文章 3273/5000\n",
      "处理文章 3272/5000\n",
      "处理文章 3275/5000\n",
      "处理文章 3274/5000\n",
      "处理文章 3276/5000\n",
      "处理文章 3277/5000\n",
      "处理文章 3278/5000\n",
      "处理文章 3279/5000\n",
      "处理文章 3281/5000\n",
      "处理文章 3280/5000\n",
      "处理文章 3282/5000\n",
      "处理文章 3283/5000\n",
      "处理文章 3284/5000\n",
      "处理文章 3285/5000\n",
      "处理文章 3286/5000\n",
      "处理文章 3287/5000\n",
      "处理文章 3288/5000\n",
      "处理文章 3289/5000\n",
      "处理文章 3290/5000\n",
      "处理文章 3291/5000\n",
      "处理文章 3292/5000\n",
      "处理文章 3293/5000\n",
      "处理文章 3294/5000\n",
      "处理文章 3295/5000\n",
      "处理文章 3296/5000\n",
      "处理文章 3297/5000\n",
      "处理文章 3298/5000\n",
      "处理文章 3299/5000\n",
      "处理文章 3300/5000\n",
      "处理文章 3302/5000\n",
      "处理文章 3301/5000\n",
      "处理文章 3303/5000\n",
      "处理文章 3305/5000\n",
      "处理文章 3304/5000\n",
      "处理文章 3308/5000\n",
      "处理文章 3306/5000\n",
      "处理文章 3307/5000\n",
      "处理文章 3309/5000\n",
      "处理文章 3310/5000\n",
      "处理文章 3311/5000\n",
      "处理文章 3312/5000\n",
      "处理文章 3313/5000\n",
      "处理文章 3314/5000\n",
      "处理文章 3315/5000\n",
      "处理文章 3316/5000\n",
      "处理文章 3318/5000\n",
      "处理文章 3317/5000\n",
      "处理文章 3319/5000\n",
      "处理文章 3321/5000\n",
      "处理文章 3320/5000\n",
      "处理文章 3322/5000\n",
      "处理文章 3323/5000\n",
      "处理文章 3325/5000\n",
      "处理文章 3326/5000\n",
      "处理文章 3327/5000\n",
      "处理文章 3324/5000\n",
      "处理文章 3328/5000\n",
      "处理文章 3329/5000\n",
      "处理文章 3331/5000\n",
      "处理文章 3330/5000\n",
      "处理文章 3332/5000\n",
      "处理文章 3334/5000\n",
      "处理文章 3335/5000\n",
      "处理文章 3333/5000\n",
      "处理文章 3336/5000\n",
      "处理文章 3337/5000\n",
      "处理文章 3338/5000\n",
      "处理文章 3339/5000\n",
      "处理文章 3340/5000\n",
      "处理文章 3341/5000\n",
      "处理文章 3342/5000\n",
      "处理文章 3343/5000\n",
      "处理文章 3344/5000\n",
      "处理文章 3345/5000\n",
      "处理文章 3346/5000\n",
      "处理文章 3347/5000\n",
      "处理文章 3349/5000\n",
      "处理文章 3348/5000\n",
      "处理文章 3350/5000\n",
      "处理文章 3351/5000\n",
      "处理文章 3352/5000\n",
      "处理文章 3353/5000\n",
      "处理文章 3354/5000\n",
      "处理文章 3356/5000\n",
      "处理文章 3355/5000\n",
      "处理文章 3357/5000\n",
      "处理文章 3358/5000\n",
      "处理文章 3360/5000\n",
      "处理文章 3359/5000\n",
      "处理文章 3361/5000\n",
      "处理文章 3362/5000\n",
      "处理文章 3363/5000\n",
      "处理文章 3365/5000\n",
      "处理文章 3364/5000\n",
      "处理文章 3366/5000\n",
      "处理文章 3367/5000\n",
      "处理文章 3368/5000\n",
      "处理文章 3370/5000\n",
      "处理文章 3369/5000\n",
      "处理文章 3371/5000\n",
      "处理文章 3372/5000\n",
      "处理文章 3373/5000\n",
      "处理文章 3374/5000\n",
      "处理文章 3375/5000\n",
      "处理文章 3376/5000\n",
      "处理文章 3378/5000\n",
      "处理文章 3377/5000\n",
      "处理文章 3380/5000\n",
      "处理文章 3379/5000\n",
      "处理文章 3381/5000\n",
      "处理文章 3382/5000\n",
      "处理文章 3384/5000\n",
      "处理文章 3383/5000\n",
      "处理文章 3385/5000\n",
      "处理文章 3386/5000\n",
      "处理文章 3387/5000\n",
      "处理文章 3388/5000\n",
      "处理文章 3390/5000\n",
      "处理文章 3389/5000\n",
      "处理文章 3391/5000\n",
      "处理文章 3392/5000\n",
      "处理文章 3393/5000\n",
      "处理文章 3395/5000\n",
      "处理文章 3394/5000\n",
      "处理文章 3396/5000\n",
      "处理文章 3397/5000\n",
      "处理文章 3399/5000\n",
      "处理文章 3398/5000\n",
      "处理文章 3400/5000\n",
      "处理文章 3401/5000\n",
      "处理文章 3402/5000\n",
      "处理文章 3403/5000\n",
      "处理文章 3404/5000\n",
      "处理文章 3405/5000\n",
      "处理文章 3406/5000\n",
      "处理文章 3407/5000\n",
      "处理文章 3408/5000\n",
      "处理文章 3409/5000\n",
      "处理文章 3410/5000\n",
      "处理文章 3411/5000\n",
      "处理文章 3412/5000\n",
      "处理文章 3413/5000\n",
      "处理文章 3414/5000\n",
      "处理文章 3415/5000\n",
      "处理文章 3417/5000\n",
      "处理文章 3416/5000\n",
      "处理文章 3418/5000\n",
      "处理文章 3419/5000\n",
      "处理文章 3420/5000\n",
      "处理文章 3421/5000\n",
      "处理文章 3422/5000\n",
      "处理文章 3423/5000\n",
      "处理文章 3424/5000\n",
      "处理文章 3425/5000\n",
      "处理文章 3426/5000\n",
      "处理文章 3428/5000\n",
      "处理文章 3427/5000\n",
      "处理文章 3429/5000\n",
      "处理文章 3430/5000\n",
      "处理文章 3431/5000\n",
      "处理文章 3432/5000\n",
      "处理文章 3433/5000\n",
      "处理文章 3434/5000\n",
      "处理文章 3435/5000\n",
      "处理文章 3436/5000\n",
      "处理文章 3437/5000\n",
      "处理文章 3438/5000\n",
      "处理文章 3439/5000\n",
      "处理文章 3441/5000\n",
      "处理文章 3443/5000\n",
      "处理文章 3440/5000\n",
      "处理文章 3442/5000\n",
      "处理文章 3445/5000\n",
      "处理文章 3444/5000\n",
      "处理文章 3446/5000\n",
      "处理文章 3447/5000\n",
      "处理文章 3448/5000\n",
      "处理文章 3449/5000\n",
      "处理文章 3450/5000\n",
      "处理文章 3451/5000\n",
      "处理文章 3452/5000\n",
      "处理文章 3453/5000\n",
      "处理文章 3454/5000\n",
      "处理文章 3455/5000\n",
      "处理文章 3456/5000\n",
      "处理文章 3457/5000\n",
      "处理文章 3458/5000\n",
      "处理文章 3459/5000\n",
      "处理文章 3460/5000\n",
      "处理文章 3461/5000\n",
      "处理文章 3462/5000\n",
      "处理文章 3464/5000\n",
      "处理文章 3463/5000\n",
      "处理文章 3465/5000\n",
      "处理文章 3466/5000\n",
      "处理文章 3467/5000\n",
      "处理文章 3468/5000\n",
      "处理文章 3470/5000\n",
      "处理文章 3469/5000\n",
      "处理文章 3473/5000\n",
      "处理文章 3472/5000\n",
      "处理文章 3471/5000\n",
      "处理文章 3474/5000\n",
      "处理文章 3475/5000\n",
      "处理文章 3476/5000\n",
      "处理文章 3477/5000\n",
      "处理文章 3478/5000\n",
      "处理文章 3479/5000\n",
      "处理文章 3481/5000\n",
      "处理文章 3480/5000\n",
      "处理文章 3482/5000\n",
      "处理文章 3485/5000\n",
      "处理文章 3483/5000\n",
      "处理文章 3484/5000\n",
      "处理文章 3486/5000\n",
      "处理文章 3488/5000\n",
      "处理文章 3487/5000\n",
      "处理文章 3489/5000\n",
      "处理文章 3490/5000\n",
      "处理文章 3491/5000\n",
      "处理文章 3492/5000\n",
      "处理文章 3493/5000\n",
      "处理文章 3494/5000\n",
      "处理文章 3495/5000\n",
      "处理文章 3497/5000\n",
      "处理文章 3496/5000\n",
      "处理文章 3498/5000\n",
      "处理文章 3500/5000\n",
      "处理文章 3499/5000\n",
      "处理文章 3501/5000\n",
      "处理文章 3502/5000\n",
      "处理文章 3503/5000\n",
      "处理文章 3504/5000\n",
      "处理文章 3506/5000\n",
      "处理文章 3505/5000\n",
      "处理文章 3507/5000\n",
      "处理文章 3508/5000\n",
      "处理文章 3510/5000\n",
      "处理文章 3509/5000\n",
      "处理文章 3511/5000\n",
      "处理文章 3512/5000\n",
      "处理文章 3513/5000\n",
      "处理文章 3514/5000\n",
      "处理文章 3516/5000\n",
      "处理文章 3515/5000\n",
      "处理文章 3517/5000\n",
      "处理文章 3518/5000\n",
      "处理文章 3519/5000\n",
      "处理文章 3521/5000\n",
      "处理文章 3520/5000\n",
      "处理文章 3523/5000\n",
      "处理文章 3522/5000\n",
      "处理文章 3524/5000\n",
      "处理文章 3526/5000\n",
      "处理文章 3525/5000\n",
      "处理文章 3527/5000\n",
      "处理文章 3528/5000\n",
      "处理文章 3529/5000\n",
      "处理文章 3531/5000\n",
      "处理文章 3530/5000\n",
      "处理文章 3533/5000\n",
      "处理文章 3532/5000\n",
      "处理文章 3535/5000\n",
      "处理文章 3534/5000\n",
      "处理文章 3538/5000\n",
      "处理文章 3536/5000\n",
      "处理文章 3537/5000\n",
      "处理文章 3539/5000\n",
      "处理文章 3540/5000\n",
      "处理文章 3541/5000\n",
      "处理文章 3543/5000\n",
      "处理文章 3542/5000\n",
      "处理文章 3544/5000\n",
      "处理文章 3545/5000\n",
      "处理文章 3546/5000\n",
      "处理文章 3547/5000\n",
      "处理文章 3548/5000\n",
      "处理文章 3549/5000\n",
      "处理文章 3550/5000\n",
      "处理文章 3551/5000\n",
      "处理文章 3553/5000\n",
      "处理文章 3552/5000\n",
      "处理文章 3554/5000\n",
      "处理文章 3556/5000\n",
      "处理文章 3555/5000\n",
      "处理文章 3557/5000\n",
      "处理文章 3559/5000\n",
      "处理文章 3558/5000\n",
      "处理文章 3560/5000\n",
      "处理文章 3561/5000\n",
      "处理文章 3562/5000\n",
      "处理文章 3564/5000\n",
      "处理文章 3563/5000\n",
      "处理文章 3565/5000\n",
      "处理文章 3566/5000\n",
      "处理文章 3567/5000\n",
      "处理文章 3568/5000\n",
      "处理文章 3569/5000\n",
      "处理文章 3570/5000\n",
      "处理文章 3571/5000\n",
      "处理文章 3572/5000\n",
      "处理文章 3573/5000\n",
      "处理文章 3574/5000\n",
      "处理文章 3575/5000\n",
      "处理文章 3576/5000\n",
      "处理文章 3577/5000\n",
      "处理文章 3578/5000\n",
      "处理文章 3580/5000\n",
      "处理文章 3579/5000\n",
      "处理文章 3581/5000\n",
      "处理文章 3582/5000\n",
      "处理文章 3583/5000\n",
      "处理文章 3584/5000\n",
      "处理文章 3585/5000\n",
      "处理文章 3586/5000\n",
      "处理文章 3587/5000\n",
      "处理文章 3588/5000\n",
      "处理文章 3589/5000\n",
      "处理文章 3590/5000\n",
      "处理文章 3591/5000\n",
      "处理文章 3592/5000\n",
      "处理文章 3593/5000\n",
      "处理文章 3596/5000\n",
      "处理文章 3594/5000\n",
      "处理文章 3595/5000\n",
      "处理文章 3597/5000\n",
      "处理文章 3599/5000\n",
      "处理文章 3598/5000\n",
      "处理文章 3600/5000\n",
      "处理文章 3601/5000\n",
      "处理文章 3602/5000\n",
      "处理文章 3603/5000\n",
      "处理文章 3604/5000\n",
      "处理文章 3606/5000\n",
      "处理文章 3605/5000\n",
      "处理文章 3607/5000\n",
      "处理文章 3609/5000\n",
      "处理文章 3608/5000\n",
      "处理文章 3610/5000\n",
      "处理文章 3611/5000\n",
      "处理文章 3612/5000\n",
      "处理文章 3613/5000\n",
      "处理文章 3614/5000\n",
      "处理文章 3615/5000\n",
      "处理文章 3616/5000\n",
      "处理文章 3617/5000\n",
      "处理文章 3618/5000\n",
      "处理文章 3619/5000\n",
      "处理文章 3620/5000\n",
      "处理文章 3621/5000\n",
      "处理文章 3622/5000\n",
      "处理文章 3623/5000\n",
      "处理文章 3627/5000\n",
      "处理文章 3625/5000\n",
      "处理文章 3626/5000\n",
      "处理文章 3624/5000\n",
      "处理文章 3628/5000\n",
      "处理文章 3630/5000\n",
      "处理文章 3629/5000\n",
      "处理文章 3632/5000\n",
      "处理文章 3631/5000\n",
      "处理文章 3633/5000\n",
      "处理文章 3635/5000\n",
      "处理文章 3634/5000\n",
      "处理文章 3636/5000\n",
      "处理文章 3638/5000\n",
      "处理文章 3640/5000\n",
      "处理文章 3637/5000\n",
      "处理文章 3641/5000\n",
      "处理文章 3639/5000\n",
      "处理文章 3642/5000\n",
      "处理文章 3643/5000\n",
      "处理文章 3644/5000\n",
      "处理文章 3645/5000\n",
      "处理文章 3646/5000\n",
      "处理文章 3647/5000\n",
      "处理文章 3648/5000\n",
      "处理文章 3649/5000\n",
      "处理文章 3650/5000\n",
      "处理文章 3651/5000\n",
      "处理文章 3652/5000\n",
      "处理文章 3653/5000\n",
      "处理文章 3654/5000\n",
      "处理文章 3655/5000\n",
      "处理文章 3656/5000\n",
      "处理文章 3657/5000\n",
      "处理文章 3658/5000\n",
      "处理文章 3659/5000\n",
      "处理文章 3660/5000\n",
      "处理文章 3661/5000\n",
      "处理文章 3663/5000\n",
      "处理文章 3662/5000\n",
      "处理文章 3664/5000\n",
      "处理文章 3665/5000\n",
      "处理文章 3666/5000\n",
      "处理文章 3667/5000\n",
      "处理文章 3668/5000\n",
      "处理文章 3669/5000\n",
      "处理文章 3670/5000\n",
      "处理文章 3671/5000\n",
      "处理文章 3672/5000\n",
      "处理文章 3675/5000\n",
      "处理文章 3673/5000\n",
      "处理文章 3674/5000\n",
      "处理文章 3676/5000\n",
      "处理文章 3677/5000\n",
      "处理文章 3678/5000\n",
      "处理文章 3679/5000\n",
      "处理文章 3681/5000\n",
      "处理文章 3680/5000\n",
      "处理文章 3682/5000\n",
      "处理文章 3683/5000\n",
      "处理文章 3684/5000\n",
      "处理文章 3685/5000\n",
      "处理文章 3686/5000\n",
      "处理文章 3687/5000\n",
      "处理文章 3688/5000\n",
      "处理文章 3689/5000\n",
      "处理文章 3690/5000\n",
      "处理文章 3691/5000\n",
      "处理文章 3692/5000\n",
      "处理文章 3693/5000\n",
      "处理文章 3694/5000\n",
      "处理文章 3696/5000\n",
      "处理文章 3695/5000\n",
      "处理文章 3698/5000\n",
      "处理文章 3699/5000\n",
      "处理文章 3697/5000\n",
      "处理文章 3700/5000\n",
      "处理文章 3701/5000\n",
      "处理文章 3702/5000\n",
      "处理文章 3703/5000\n",
      "处理文章 3704/5000\n",
      "处理文章 3705/5000\n",
      "处理文章 3706/5000\n",
      "处理文章 3708/5000\n",
      "处理文章 3707/5000\n",
      "处理文章 3709/5000\n",
      "处理文章 3710/5000\n",
      "处理文章 3711/5000\n",
      "处理文章 3712/5000\n",
      "处理文章 3713/5000\n",
      "处理文章 3714/5000\n",
      "处理文章 3715/5000\n",
      "处理文章 3716/5000\n",
      "处理文章 3717/5000\n",
      "处理文章 3720/5000\n",
      "处理文章 3718/5000\n",
      "处理文章 3719/5000\n",
      "处理文章 3721/5000\n",
      "处理文章 3722/5000\n",
      "处理文章 3723/5000\n",
      "处理文章 3726/5000\n",
      "处理文章 3725/5000\n",
      "处理文章 3724/5000\n",
      "处理文章 3727/5000\n",
      "处理文章 3728/5000\n",
      "处理文章 3729/5000\n",
      "处理文章 3730/5000\n",
      "处理文章 3732/5000\n",
      "处理文章 3731/5000\n",
      "处理文章 3733/5000\n",
      "处理文章 3734/5000\n",
      "处理文章 3735/5000\n",
      "处理文章 3737/5000\n",
      "处理文章 3736/5000\n",
      "处理文章 3738/5000\n",
      "处理文章 3739/5000\n",
      "处理文章 3740/5000\n",
      "处理文章 3741/5000\n",
      "处理文章 3742/5000\n",
      "处理文章 3743/5000\n",
      "处理文章 3744/5000\n",
      "处理文章 3745/5000\n",
      "处理文章 3746/5000\n",
      "处理文章 3747/5000\n",
      "处理文章 3749/5000\n",
      "处理文章 3748/5000\n",
      "处理文章 3750/5000\n",
      "处理文章 3751/5000\n",
      "处理文章 3753/5000\n",
      "处理文章 3754/5000\n",
      "处理文章 3752/5000\n",
      "处理文章 3756/5000\n",
      "处理文章 3755/5000\n",
      "处理文章 3757/5000\n",
      "处理文章 3758/5000\n",
      "处理文章 3759/5000\n",
      "处理文章 3760/5000\n",
      "处理文章 3761/5000\n",
      "处理文章 3762/5000\n",
      "处理文章 3763/5000\n",
      "处理文章 3764/5000\n",
      "处理文章 3765/5000\n",
      "处理文章 3766/5000\n",
      "处理文章 3767/5000\n",
      "处理文章 3768/5000\n",
      "处理文章 3769/5000\n",
      "处理文章 3770/5000\n",
      "处理文章 3772/5000\n",
      "处理文章 3771/5000\n",
      "处理文章 3773/5000\n",
      "处理文章 3774/5000\n",
      "处理文章 3775/5000\n",
      "处理文章 3777/5000\n",
      "处理文章 3776/5000\n",
      "处理文章 3778/5000\n",
      "处理文章 3779/5000\n",
      "处理文章 3780/5000\n",
      "处理文章 3781/5000\n",
      "处理文章 3782/5000\n",
      "处理文章 3784/5000\n",
      "处理文章 3785/5000\n",
      "处理文章 3783/5000\n",
      "处理文章 3786/5000\n",
      "处理文章 3787/5000\n",
      "处理文章 3788/5000\n",
      "处理文章 3790/5000\n",
      "处理文章 3789/5000\n",
      "处理文章 3791/5000\n",
      "处理文章 3792/5000\n",
      "处理文章 3793/5000\n",
      "处理文章 3794/5000\n",
      "处理文章 3796/5000\n",
      "处理文章 3795/5000\n",
      "处理文章 3797/5000\n",
      "处理文章 3798/5000\n",
      "处理文章 3799/5000\n",
      "处理文章 3800/5000\n",
      "处理文章 3801/5000\n",
      "处理文章 3802/5000\n",
      "处理文章 3803/5000\n",
      "处理文章 3804/5000\n",
      "处理文章 3806/5000\n",
      "处理文章 3805/5000\n",
      "处理文章 3807/5000\n",
      "处理文章 3808/5000\n",
      "处理文章 3809/5000\n",
      "处理文章 3810/5000\n",
      "处理文章 3811/5000\n",
      "处理文章 3814/5000\n",
      "处理文章 3812/5000\n",
      "处理文章 3813/5000\n",
      "处理文章 3815/5000\n",
      "处理文章 3816/5000\n",
      "处理文章 3817/5000\n",
      "处理文章 3818/5000\n",
      "处理文章 3819/5000\n",
      "处理文章 3820/5000\n",
      "处理文章 3821/5000\n",
      "处理文章 3822/5000\n",
      "处理文章 3823/5000\n",
      "处理文章 3824/5000\n",
      "处理文章 3825/5000\n",
      "处理文章 3826/5000\n",
      "处理文章 3827/5000\n",
      "处理文章 3828/5000\n",
      "处理文章 3829/5000\n",
      "处理文章 3830/5000\n",
      "处理文章 3831/5000\n",
      "处理文章 3832/5000\n",
      "处理文章 3833/5000\n",
      "处理文章 3835/5000\n",
      "处理文章 3834/5000\n",
      "处理文章 3838/5000\n",
      "处理文章 3836/5000\n",
      "处理文章 3837/5000\n",
      "处理文章 3840/5000\n",
      "处理文章 3839/5000\n",
      "处理文章 3841/5000\n",
      "处理文章 3843/5000\n",
      "处理文章 3842/5000\n",
      "处理文章 3844/5000\n",
      "处理文章 3845/5000\n",
      "处理文章 3846/5000\n",
      "处理文章 3848/5000\n",
      "处理文章 3847/5000\n",
      "处理文章 3849/5000\n",
      "处理文章 3850/5000\n",
      "处理文章 3851/5000\n",
      "处理文章 3853/5000\n",
      "处理文章 3852/5000\n",
      "处理文章 3855/5000\n",
      "处理文章 3854/5000\n",
      "处理文章 3856/5000\n",
      "处理文章 3857/5000\n",
      "处理文章 3858/5000\n",
      "处理文章 3859/5000\n",
      "处理文章 3860/5000\n",
      "处理文章 3861/5000\n",
      "处理文章 3862/5000\n",
      "处理文章 3863/5000\n",
      "处理文章 3865/5000\n",
      "处理文章 3864/5000\n",
      "处理文章 3866/5000\n",
      "处理文章 3867/5000\n",
      "处理文章 3868/5000\n",
      "处理文章 3869/5000\n",
      "处理文章 3870/5000\n",
      "处理文章 3871/5000\n",
      "处理文章 3872/5000\n",
      "处理文章 3873/5000\n",
      "处理文章 3874/5000\n",
      "处理文章 3875/5000\n",
      "处理文章 3876/5000\n",
      "处理文章 3877/5000\n",
      "处理文章 3878/5000\n",
      "处理文章 3879/5000\n",
      "处理文章 3880/5000\n",
      "处理文章 3882/5000\n",
      "处理文章 3881/5000\n",
      "处理文章 3883/5000\n",
      "处理文章 3884/5000\n",
      "处理文章 3885/5000\n",
      "处理文章 3887/5000\n",
      "处理文章 3886/5000\n",
      "处理文章 3888/5000\n",
      "处理文章 3889/5000\n",
      "处理文章 3890/5000\n",
      "处理文章 3891/5000\n",
      "处理文章 3892/5000\n",
      "处理文章 3894/5000\n",
      "处理文章 3893/5000\n",
      "处理文章 3895/5000\n",
      "处理文章 3896/5000\n",
      "处理文章 3897/5000\n",
      "处理文章 3899/5000\n",
      "处理文章 3898/5000\n",
      "处理文章 3900/5000\n",
      "处理文章 3901/5000\n",
      "处理文章 3902/5000\n",
      "处理文章 3903/5000\n",
      "处理文章 3904/5000\n",
      "处理文章 3905/5000\n",
      "处理文章 3906/5000\n",
      "处理文章 3907/5000\n",
      "处理文章 3908/5000\n",
      "处理文章 3910/5000\n",
      "处理文章 3909/5000\n",
      "处理文章 3911/5000\n",
      "处理文章 3912/5000\n",
      "处理文章 3913/5000\n",
      "处理文章 3914/5000\n",
      "处理文章 3915/5000\n",
      "处理文章 3916/5000\n",
      "处理文章 3917/5000\n",
      "处理文章 3918/5000\n",
      "处理文章 3919/5000\n",
      "处理文章 3922/5000\n",
      "处理文章 3921/5000\n",
      "处理文章 3920/5000\n",
      "处理文章 3923/5000\n",
      "处理文章 3924/5000\n",
      "处理文章 3925/5000\n",
      "处理文章 3926/5000\n",
      "处理文章 3927/5000\n",
      "处理文章 3928/5000\n",
      "处理文章 3929/5000\n",
      "处理文章 3930/5000\n",
      "处理文章 3931/5000\n",
      "处理文章 3933/5000\n",
      "处理文章 3932/5000\n",
      "处理文章 3934/5000\n",
      "处理文章 3936/5000\n",
      "处理文章 3935/5000\n",
      "处理文章 3938/5000\n",
      "处理文章 3937/5000\n",
      "处理文章 3939/5000\n",
      "处理文章 3940/5000\n",
      "处理文章 3941/5000\n",
      "处理文章 3942/5000\n",
      "处理文章 3944/5000\n",
      "处理文章 3943/5000\n",
      "处理文章 3945/5000\n",
      "处理文章 3946/5000\n",
      "处理文章 3948/5000\n",
      "处理文章 3947/5000\n",
      "处理文章 3949/5000\n",
      "处理文章 3951/5000\n",
      "处理文章 3950/5000\n",
      "处理文章 3952/5000\n",
      "处理文章 3953/5000\n",
      "处理文章 3956/5000\n",
      "处理文章 3954/5000\n",
      "处理文章 3955/5000\n",
      "处理文章 3957/5000\n",
      "处理文章 3958/5000\n",
      "处理文章 3960/5000\n",
      "处理文章 3959/5000\n",
      "处理文章 3961/5000\n",
      "处理文章 3962/5000\n",
      "处理文章 3963/5000\n",
      "处理文章 3966/5000\n",
      "处理文章 3967/5000\n",
      "处理文章 3965/5000\n",
      "处理文章 3964/5000\n",
      "处理文章 3968/5000\n",
      "处理文章 3969/5000\n",
      "处理文章 3970/5000\n",
      "处理文章 3971/5000\n",
      "处理文章 3972/5000\n",
      "处理文章 3973/5000\n",
      "处理文章 3974/5000\n",
      "处理文章 3975/5000\n",
      "处理文章 3976/5000\n",
      "处理文章 3977/5000\n",
      "处理文章 3979/5000\n",
      "处理文章 3978/5000\n",
      "处理文章 3980/5000\n",
      "处理文章 3982/5000\n",
      "处理文章 3981/5000\n",
      "处理文章 3984/5000\n",
      "处理文章 3983/5000\n",
      "处理文章 3985/5000\n",
      "处理文章 3986/5000\n",
      "处理文章 3987/5000\n",
      "处理文章 3988/5000\n",
      "处理文章 3990/5000\n",
      "处理文章 3989/5000\n",
      "处理文章 3991/5000\n",
      "处理文章 3992/5000\n",
      "处理文章 3993/5000\n",
      "处理文章 3994/5000\n",
      "处理文章 3995/5000\n",
      "处理文章 3996/5000\n",
      "处理文章 3998/5000\n",
      "处理文章 3997/5000\n",
      "处理文章 4000/5000\n",
      "处理文章 3999/5000\n",
      "处理文章 4001/5000\n",
      "处理文章 4003/5000\n",
      "处理文章 4002/5000\n",
      "处理文章 4004/5000\n",
      "处理文章 4006/5000\n",
      "处理文章 4005/5000\n",
      "处理文章 4007/5000\n",
      "处理文章 4008/5000\n",
      "处理文章 4009/5000\n",
      "处理文章 4010/5000\n",
      "处理文章 4011/5000\n",
      "处理文章 4012/5000\n",
      "处理文章 4013/5000\n",
      "处理文章 4014/5000\n",
      "处理文章 4015/5000\n",
      "处理文章 4016/5000\n",
      "处理文章 4018/5000\n",
      "处理文章 4017/5000\n",
      "处理文章 4019/5000\n",
      "处理文章 4021/5000\n",
      "处理文章 4020/5000\n",
      "处理文章 4022/5000\n",
      "处理文章 4024/5000\n",
      "处理文章 4023/5000\n",
      "处理文章 4025/5000\n",
      "处理文章 4026/5000\n",
      "处理文章 4027/5000\n",
      "处理文章 4028/5000\n",
      "处理文章 4029/5000\n",
      "处理文章 4030/5000\n",
      "处理文章 4031/5000\n",
      "处理文章 4032/5000\n",
      "处理文章 4033/5000\n",
      "处理文章 4036/5000\n",
      "处理文章 4034/5000\n",
      "处理文章 4035/5000\n",
      "处理文章 4037/5000\n",
      "处理文章 4038/5000\n",
      "处理文章 4039/5000\n",
      "处理文章 4040/5000\n",
      "处理文章 4041/5000\n",
      "处理文章 4042/5000\n",
      "处理文章 4043/5000\n",
      "处理文章 4044/5000\n",
      "处理文章 4045/5000\n",
      "处理文章 4046/5000\n",
      "处理文章 4047/5000\n",
      "处理文章 4049/5000\n",
      "处理文章 4050/5000\n",
      "处理文章 4048/5000\n",
      "处理文章 4051/5000\n",
      "处理文章 4052/5000\n",
      "处理文章 4054/5000\n",
      "处理文章 4053/5000\n",
      "处理文章 4055/5000\n",
      "处理文章 4056/5000\n",
      "处理文章 4057/5000\n",
      "处理文章 4058/5000\n",
      "处理文章 4059/5000\n",
      "处理文章 4060/5000\n",
      "处理文章 4061/5000\n",
      "处理文章 4062/5000\n",
      "处理文章 4064/5000\n",
      "处理文章 4065/5000\n",
      "处理文章 4063/5000\n",
      "处理文章 4067/5000\n",
      "处理文章 4066/5000\n",
      "处理文章 4068/5000\n",
      "处理文章 4069/5000\n",
      "处理文章 4070/5000\n",
      "处理文章 4071/5000\n",
      "处理文章 4072/5000\n",
      "处理文章 4073/5000\n",
      "处理文章 4074/5000\n",
      "处理文章 4075/5000\n",
      "处理文章 4076/5000\n",
      "处理文章 4077/5000\n",
      "处理文章 4078/5000\n",
      "处理文章 4079/5000\n",
      "处理文章 4080/5000\n",
      "处理文章 4081/5000\n",
      "处理文章 4082/5000\n",
      "处理文章 4083/5000\n",
      "处理文章 4084/5000\n",
      "处理文章 4085/5000\n",
      "处理文章 4087/5000\n",
      "处理文章 4086/5000\n",
      "处理文章 4089/5000\n",
      "处理文章 4088/5000\n",
      "处理文章 4090/5000\n",
      "处理文章 4091/5000\n",
      "处理文章 4092/5000\n",
      "处理文章 4094/5000\n",
      "处理文章 4093/5000\n",
      "处理文章 4095/5000\n",
      "处理文章 4096/5000\n",
      "处理文章 4097/5000\n",
      "处理文章 4098/5000\n",
      "处理文章 4099/5000\n",
      "处理文章 4100/5000\n",
      "处理文章 4101/5000\n",
      "处理文章 4102/5000\n",
      "处理文章 4103/5000\n",
      "处理文章 4104/5000\n",
      "处理文章 4105/5000\n",
      "处理文章 4106/5000\n",
      "处理文章 4107/5000\n",
      "处理文章 4108/5000\n",
      "处理文章 4111/5000\n",
      "处理文章 4109/5000\n",
      "处理文章 4110/5000\n",
      "处理文章 4112/5000\n",
      "处理文章 4115/5000\n",
      "处理文章 4113/5000\n",
      "处理文章 4114/5000\n",
      "处理文章 4116/5000\n",
      "处理文章 4117/5000\n",
      "处理文章 4119/5000\n",
      "处理文章 4118/5000\n",
      "处理文章 4120/5000\n",
      "处理文章 4122/5000\n",
      "处理文章 4123/5000\n",
      "处理文章 4121/5000\n",
      "处理文章 4124/5000\n",
      "处理文章 4125/5000\n",
      "处理文章 4127/5000\n",
      "处理文章 4126/5000\n",
      "处理文章 4129/5000\n",
      "处理文章 4128/5000\n",
      "处理文章 4130/5000\n",
      "处理文章 4132/5000\n",
      "处理文章 4131/5000\n",
      "处理文章 4133/5000\n",
      "处理文章 4134/5000\n",
      "处理文章 4135/5000\n",
      "处理文章 4136/5000\n",
      "处理文章 4139/5000\n",
      "处理文章 4137/5000\n",
      "处理文章 4138/5000\n",
      "处理文章 4140/5000\n",
      "处理文章 4141/5000\n",
      "处理文章 4143/5000\n",
      "处理文章 4144/5000\n",
      "处理文章 4142/5000\n",
      "处理文章 4145/5000\n",
      "处理文章 4146/5000\n",
      "处理文章 4148/5000\n",
      "处理文章 4147/5000\n",
      "处理文章 4149/5000\n",
      "处理文章 4152/5000\n",
      "处理文章 4151/5000\n",
      "处理文章 4150/5000\n",
      "处理文章 4153/5000\n",
      "处理文章 4154/5000\n",
      "处理文章 4156/5000\n",
      "处理文章 4155/5000\n",
      "处理文章 4157/5000\n",
      "处理文章 4158/5000\n",
      "处理文章 4160/5000\n",
      "处理文章 4159/5000\n",
      "处理文章 4163/5000\n",
      "处理文章 4161/5000\n",
      "处理文章 4162/5000\n",
      "处理文章 4164/5000\n",
      "处理文章 4165/5000\n",
      "处理文章 4167/5000\n",
      "处理文章 4166/5000\n",
      "处理文章 4168/5000\n",
      "处理文章 4169/5000\n",
      "处理文章 4171/5000\n",
      "处理文章 4170/5000\n",
      "处理文章 4175/5000\n",
      "处理文章 4172/5000\n",
      "处理文章 4174/5000\n",
      "处理文章 4173/5000\n",
      "处理文章 4176/5000\n",
      "处理文章 4177/5000\n",
      "处理文章 4181/5000\n",
      "处理文章 4178/5000\n",
      "处理文章 4180/5000\n",
      "处理文章 4179/5000\n",
      "处理文章 4182/5000\n",
      "处理文章 4183/5000\n",
      "处理文章 4184/5000\n",
      "处理文章 4185/5000\n",
      "处理文章 4187/5000\n",
      "处理文章 4186/5000\n",
      "处理文章 4188/5000\n",
      "处理文章 4189/5000\n",
      "处理文章 4191/5000\n",
      "处理文章 4190/5000\n",
      "处理文章 4192/5000\n",
      "处理文章 4193/5000\n",
      "处理文章 4194/5000\n",
      "处理文章 4195/5000\n",
      "处理文章 4196/5000\n",
      "处理文章 4197/5000\n",
      "处理文章 4198/5000\n",
      "处理文章 4199/5000\n",
      "处理文章 4200/5000\n",
      "处理文章 4201/5000\n",
      "处理文章 4203/5000\n",
      "处理文章 4202/5000\n",
      "处理文章 4205/5000\n",
      "处理文章 4204/5000\n",
      "处理文章 4206/5000\n",
      "处理文章 4207/5000\n",
      "处理文章 4209/5000\n",
      "处理文章 4208/5000\n",
      "处理文章 4210/5000\n",
      "处理文章 4211/5000\n",
      "处理文章 4212/5000\n",
      "处理文章 4213/5000\n",
      "处理文章 4214/5000\n",
      "处理文章 4216/5000\n",
      "处理文章 4215/5000\n",
      "处理文章 4217/5000\n",
      "处理文章 4218/5000\n",
      "处理文章 4219/5000\n",
      "处理文章 4220/5000\n",
      "处理文章 4221/5000\n",
      "处理文章 4222/5000\n",
      "处理文章 4224/5000\n",
      "处理文章 4223/5000\n",
      "处理文章 4225/5000\n",
      "处理文章 4226/5000\n",
      "处理文章 4227/5000\n",
      "处理文章 4228/5000\n",
      "处理文章 4229/5000\n",
      "处理文章 4230/5000\n",
      "处理文章 4231/5000\n",
      "处理文章 4232/5000\n",
      "处理文章 4233/5000\n",
      "处理文章 4234/5000\n",
      "处理文章 4235/5000\n",
      "处理文章 4237/5000\n",
      "处理文章 4236/5000\n",
      "处理文章 4240/5000\n",
      "处理文章 4239/5000\n",
      "处理文章 4238/5000\n",
      "处理文章 4241/5000\n",
      "处理文章 4243/5000\n",
      "处理文章 4242/5000\n",
      "处理文章 4244/5000\n",
      "处理文章 4246/5000\n",
      "处理文章 4245/5000\n",
      "处理文章 4249/5000\n",
      "处理文章 4247/5000\n",
      "处理文章 4248/5000\n",
      "处理文章 4250/5000\n",
      "处理文章 4251/5000\n",
      "处理文章 4252/5000\n",
      "处理文章 4253/5000\n",
      "处理文章 4255/5000\n",
      "处理文章 4254/5000\n",
      "处理文章 4256/5000\n",
      "处理文章 4257/5000\n",
      "处理文章 4259/5000\n",
      "处理文章 4258/5000\n",
      "处理文章 4262/5000\n",
      "处理文章 4261/5000\n",
      "处理文章 4260/5000\n",
      "处理文章 4263/5000\n",
      "处理文章 4264/5000\n",
      "处理文章 4266/5000\n",
      "处理文章 4269/5000\n",
      "处理文章 4265/5000\n",
      "处理文章 4268/5000\n",
      "处理文章 4267/5000\n",
      "处理文章 4270/5000\n",
      "处理文章 4271/5000\n",
      "处理文章 4272/5000\n",
      "处理文章 4273/5000\n",
      "处理文章 4274/5000\n",
      "处理文章 4275/5000\n",
      "处理文章 4276/5000\n",
      "处理文章 4277/5000\n",
      "处理文章 4279/5000\n",
      "处理文章 4278/5000\n",
      "处理文章 4280/5000\n",
      "处理文章 4281/5000\n",
      "处理文章 4283/5000\n",
      "处理文章 4282/5000\n",
      "处理文章 4284/5000\n",
      "处理文章 4285/5000\n",
      "处理文章 4286/5000\n",
      "处理文章 4287/5000\n",
      "处理文章 4290/5000\n",
      "处理文章 4288/5000\n",
      "处理文章 4289/5000\n",
      "处理文章 4294/5000\n",
      "处理文章 4292/5000\n",
      "处理文章 4291/5000\n",
      "处理文章 4293/5000\n",
      "处理文章 4295/5000\n",
      "处理文章 4297/5000\n",
      "处理文章 4296/5000\n",
      "处理文章 4298/5000\n",
      "处理文章 4299/5000\n",
      "处理文章 4300/5000\n",
      "处理文章 4301/5000\n",
      "处理文章 4302/5000\n",
      "处理文章 4304/5000\n",
      "处理文章 4303/5000\n",
      "处理文章 4306/5000\n",
      "处理文章 4305/5000\n",
      "处理文章 4308/5000\n",
      "处理文章 4307/5000\n",
      "处理文章 4309/5000\n",
      "处理文章 4310/5000\n",
      "处理文章 4312/5000\n",
      "处理文章 4311/5000\n",
      "处理文章 4313/5000\n",
      "处理文章 4314/5000\n",
      "处理文章 4315/5000\n",
      "处理文章 4316/5000\n",
      "处理文章 4317/5000\n",
      "处理文章 4318/5000\n",
      "处理文章 4319/5000\n",
      "处理文章 4320/5000\n",
      "处理文章 4321/5000\n",
      "处理文章 4322/5000\n",
      "处理文章 4323/5000\n",
      "处理文章 4324/5000\n",
      "处理文章 4325/5000\n",
      "处理文章 4326/5000\n",
      "处理文章 4327/5000\n",
      "处理文章 4329/5000\n",
      "处理文章 4328/5000\n",
      "处理文章 4330/5000\n",
      "处理文章 4331/5000\n",
      "处理文章 4333/5000\n",
      "处理文章 4332/5000\n",
      "处理文章 4334/5000\n",
      "处理文章 4335/5000\n",
      "处理文章 4337/5000\n",
      "处理文章 4336/5000\n",
      "处理文章 4339/5000\n",
      "处理文章 4338/5000\n",
      "处理文章 4340/5000\n",
      "处理文章 4341/5000\n",
      "处理文章 4342/5000\n",
      "处理文章 4343/5000\n",
      "处理文章 4344/5000\n",
      "处理文章 4345/5000\n",
      "处理文章 4346/5000\n",
      "处理文章 4348/5000\n",
      "处理文章 4347/5000\n",
      "处理文章 4349/5000\n",
      "处理文章 4351/5000\n",
      "处理文章 4352/5000\n",
      "处理文章 4350/5000\n",
      "处理文章 4353/5000\n",
      "处理文章 4354/5000\n",
      "处理文章 4355/5000\n",
      "处理文章 4356/5000\n",
      "处理文章 4357/5000\n",
      "处理文章 4359/5000\n",
      "处理文章 4358/5000\n",
      "处理文章 4360/5000\n",
      "处理文章 4364/5000\n",
      "处理文章 4361/5000\n",
      "处理文章 4365/5000\n",
      "处理文章 4363/5000\n",
      "处理文章 4362/5000\n",
      "处理文章 4366/5000\n",
      "处理文章 4367/5000\n",
      "处理文章 4368/5000\n",
      "处理文章 4369/5000\n",
      "处理文章 4370/5000\n",
      "处理文章 4371/5000\n",
      "处理文章 4372/5000\n",
      "处理文章 4373/5000\n",
      "处理文章 4374/5000\n",
      "处理文章 4375/5000\n",
      "处理文章 4376/5000\n",
      "处理文章 4377/5000\n",
      "处理文章 4378/5000\n",
      "处理文章 4379/5000\n",
      "处理文章 4380/5000\n",
      "处理文章 4381/5000\n",
      "处理文章 4382/5000\n",
      "处理文章 4383/5000\n",
      "处理文章 4384/5000\n",
      "处理文章 4385/5000\n",
      "处理文章 4387/5000\n",
      "处理文章 4386/5000\n",
      "处理文章 4388/5000\n",
      "处理文章 4390/5000\n",
      "处理文章 4389/5000\n",
      "处理文章 4391/5000\n",
      "处理文章 4392/5000\n",
      "处理文章 4393/5000\n",
      "处理文章 4394/5000\n",
      "处理文章 4395/5000\n",
      "处理文章 4396/5000\n",
      "处理文章 4397/5000\n",
      "处理文章 4398/5000\n",
      "处理文章 4399/5000\n",
      "处理文章 4400/5000\n",
      "处理文章 4401/5000\n",
      "处理文章 4402/5000\n",
      "处理文章 4403/5000\n",
      "处理文章 4404/5000\n",
      "处理文章 4405/5000\n",
      "处理文章 4406/5000\n",
      "处理文章 4408/5000\n",
      "处理文章 4407/5000\n",
      "处理文章 4409/5000\n",
      "处理文章 4411/5000\n",
      "处理文章 4412/5000\n",
      "处理文章 4410/5000\n",
      "处理文章 4413/5000\n",
      "处理文章 4415/5000\n",
      "处理文章 4416/5000\n",
      "处理文章 4414/5000\n",
      "处理文章 4417/5000\n",
      "处理文章 4418/5000\n",
      "处理文章 4419/5000\n",
      "处理文章 4420/5000\n",
      "处理文章 4421/5000\n",
      "处理文章 4422/5000\n",
      "处理文章 4423/5000\n",
      "处理文章 4425/5000\n",
      "处理文章 4424/5000\n",
      "处理文章 4426/5000\n",
      "处理文章 4427/5000\n",
      "处理文章 4428/5000\n",
      "处理文章 4429/5000\n",
      "处理文章 4431/5000\n",
      "处理文章 4432/5000\n",
      "处理文章 4430/5000\n",
      "处理文章 4433/5000\n",
      "处理文章 4434/5000\n",
      "处理文章 4435/5000\n",
      "处理文章 4436/5000\n",
      "处理文章 4438/5000\n",
      "处理文章 4437/5000\n",
      "处理文章 4440/5000\n",
      "处理文章 4439/5000\n",
      "处理文章 4441/5000\n",
      "处理文章 4442/5000\n",
      "处理文章 4443/5000\n",
      "处理文章 4444/5000\n",
      "处理文章 4445/5000\n",
      "处理文章 4446/5000\n",
      "处理文章 4447/5000\n",
      "处理文章 4449/5000\n",
      "处理文章 4448/5000\n",
      "处理文章 4450/5000\n",
      "处理文章 4451/5000\n",
      "处理文章 4452/5000\n",
      "处理文章 4453/5000\n",
      "处理文章 4454/5000\n",
      "处理文章 4455/5000\n",
      "处理文章 4456/5000\n",
      "处理文章 4457/5000\n",
      "处理文章 4458/5000\n",
      "处理文章 4461/5000\n",
      "处理文章 4459/5000\n",
      "处理文章 4460/5000\n",
      "处理文章 4462/5000\n",
      "处理文章 4464/5000\n",
      "处理文章 4463/5000\n",
      "处理文章 4465/5000\n",
      "处理文章 4466/5000\n",
      "处理文章 4467/5000\n",
      "处理文章 4468/5000\n",
      "处理文章 4469/5000\n",
      "处理文章 4470/5000\n",
      "处理文章 4471/5000\n",
      "处理文章 4472/5000\n",
      "处理文章 4473/5000\n",
      "处理文章 4474/5000\n",
      "处理文章 4475/5000\n",
      "处理文章 4476/5000\n",
      "处理文章 4477/5000\n",
      "处理文章 4478/5000\n",
      "处理文章 4479/5000\n",
      "处理文章 4480/5000\n",
      "处理文章 4481/5000\n",
      "处理文章 4482/5000\n",
      "处理文章 4484/5000\n",
      "处理文章 4485/5000\n",
      "处理文章 4483/5000\n",
      "处理文章 4486/5000\n",
      "处理文章 4487/5000\n",
      "处理文章 4488/5000\n",
      "处理文章 4489/5000\n",
      "处理文章 4491/5000\n",
      "处理文章 4490/5000\n",
      "处理文章 4492/5000\n",
      "处理文章 4493/5000\n",
      "处理文章 4494/5000\n",
      "处理文章 4495/5000\n",
      "处理文章 4496/5000\n",
      "处理文章 4497/5000\n",
      "处理文章 4498/5000\n",
      "处理文章 4499/5000\n",
      "处理文章 4500/5000\n",
      "处理文章 4503/5000\n",
      "处理文章 4501/5000\n",
      "处理文章 4502/5000\n",
      "处理文章 4504/5000\n",
      "处理文章 4505/5000\n",
      "处理文章 4506/5000\n",
      "处理文章 4507/5000\n",
      "处理文章 4508/5000\n",
      "处理文章 4509/5000\n",
      "处理文章 4510/5000\n",
      "处理文章 4511/5000\n",
      "处理文章 4512/5000\n",
      "处理文章 4514/5000\n",
      "处理文章 4513/5000\n",
      "处理文章 4515/5000\n",
      "处理文章 4516/5000\n",
      "处理文章 4517/5000\n",
      "处理文章 4520/5000\n",
      "处理文章 4518/5000\n",
      "处理文章 4519/5000\n",
      "处理文章 4521/5000\n",
      "处理文章 4522/5000\n",
      "处理文章 4523/5000\n",
      "处理文章 4525/5000\n",
      "处理文章 4524/5000\n",
      "处理文章 4527/5000\n",
      "处理文章 4526/5000\n",
      "处理文章 4529/5000\n",
      "处理文章 4528/5000\n",
      "处理文章 4532/5000\n",
      "处理文章 4531/5000\n",
      "处理文章 4530/5000\n",
      "处理文章 4534/5000\n",
      "处理文章 4533/5000\n",
      "处理文章 4536/5000\n",
      "处理文章 4535/5000\n",
      "处理文章 4537/5000\n",
      "处理文章 4538/5000\n",
      "处理文章 4539/5000\n",
      "处理文章 4540/5000\n",
      "处理文章 4541/5000\n",
      "处理文章 4542/5000\n",
      "处理文章 4544/5000\n",
      "处理文章 4543/5000\n",
      "处理文章 4545/5000\n",
      "处理文章 4546/5000\n",
      "处理文章 4548/5000\n",
      "处理文章 4549/5000\n",
      "处理文章 4547/5000\n",
      "处理文章 4550/5000\n",
      "处理文章 4551/5000\n",
      "处理文章 4552/5000\n",
      "处理文章 4554/5000\n",
      "处理文章 4553/5000\n",
      "处理文章 4555/5000\n",
      "处理文章 4556/5000\n",
      "处理文章 4557/5000\n",
      "处理文章 4558/5000\n",
      "处理文章 4559/5000\n",
      "处理文章 4560/5000\n",
      "处理文章 4561/5000\n",
      "处理文章 4562/5000\n",
      "处理文章 4563/5000\n",
      "处理文章 4564/5000\n",
      "处理文章 4565/5000\n",
      "处理文章 4566/5000\n",
      "处理文章 4567/5000\n",
      "处理文章 4568/5000\n",
      "处理文章 4570/5000\n",
      "处理文章 4569/5000\n",
      "处理文章 4573/5000\n",
      "处理文章 4571/5000\n",
      "处理文章 4572/5000\n",
      "处理文章 4574/5000\n",
      "处理文章 4575/5000\n",
      "处理文章 4576/5000\n",
      "处理文章 4577/5000\n",
      "处理文章 4578/5000\n",
      "处理文章 4579/5000\n",
      "处理文章 4580/5000\n",
      "处理文章 4581/5000\n",
      "处理文章 4582/5000\n",
      "处理文章 4583/5000\n",
      "处理文章 4584/5000\n",
      "处理文章 4585/5000\n",
      "处理文章 4586/5000\n",
      "处理文章 4587/5000\n",
      "处理文章 4589/5000\n",
      "处理文章 4590/5000\n",
      "处理文章 4588/5000\n",
      "处理文章 4591/5000\n",
      "处理文章 4592/5000\n",
      "处理文章 4593/5000\n",
      "处理文章 4594/5000\n",
      "处理文章 4595/5000\n",
      "处理文章 4596/5000\n",
      "处理文章 4597/5000\n",
      "处理文章 4598/5000\n",
      "处理文章 4599/5000\n",
      "处理文章 4600/5000\n",
      "处理文章 4605/5000\n",
      "处理文章 4601/5000\n",
      "处理文章 4604/5000\n",
      "处理文章 4603/5000\n",
      "处理文章 4602/5000\n",
      "处理文章 4606/5000\n",
      "处理文章 4608/5000\n",
      "处理文章 4607/5000\n",
      "处理文章 4609/5000\n",
      "处理文章 4610/5000\n",
      "处理文章 4611/5000\n",
      "处理文章 4612/5000\n",
      "处理文章 4613/5000\n",
      "处理文章 4614/5000\n",
      "处理文章 4616/5000\n",
      "处理文章 4617/5000\n",
      "处理文章 4615/5000\n",
      "处理文章 4618/5000\n",
      "处理文章 4619/5000\n",
      "处理文章 4620/5000\n",
      "处理文章 4621/5000\n",
      "处理文章 4622/5000\n",
      "处理文章 4624/5000\n",
      "处理文章 4623/5000\n",
      "处理文章 4626/5000\n",
      "处理文章 4627/5000\n",
      "处理文章 4625/5000\n",
      "处理文章 4629/5000\n",
      "处理文章 4628/5000\n",
      "处理文章 4630/5000\n",
      "处理文章 4631/5000\n",
      "处理文章 4632/5000\n",
      "处理文章 4634/5000\n",
      "处理文章 4633/5000\n",
      "处理文章 4636/5000\n",
      "处理文章 4635/5000\n",
      "处理文章 4637/5000\n",
      "处理文章 4639/5000\n",
      "处理文章 4640/5000\n",
      "处理文章 4638/5000\n",
      "处理文章 4642/5000\n",
      "处理文章 4641/5000\n",
      "处理文章 4643/5000\n",
      "处理文章 4644/5000\n",
      "处理文章 4646/5000\n",
      "处理文章 4645/5000\n",
      "处理文章 4647/5000\n",
      "处理文章 4648/5000\n",
      "处理文章 4649/5000\n",
      "处理文章 4650/5000\n",
      "处理文章 4655/5000\n",
      "处理文章 4651/5000\n",
      "处理文章 4653/5000\n",
      "处理文章 4652/5000\n",
      "处理文章 4654/5000\n",
      "处理文章 4656/5000\n",
      "处理文章 4657/5000\n",
      "处理文章 4658/5000\n",
      "处理文章 4659/5000\n",
      "处理文章 4660/5000\n",
      "处理文章 4662/5000\n",
      "处理文章 4661/5000\n",
      "处理文章 4664/5000\n",
      "处理文章 4663/5000\n",
      "处理文章 4666/5000\n",
      "处理文章 4665/5000\n",
      "处理文章 4668/5000\n",
      "处理文章 4667/5000\n",
      "处理文章 4669/5000\n",
      "处理文章 4670/5000\n",
      "处理文章 4671/5000\n",
      "处理文章 4672/5000\n",
      "处理文章 4673/5000\n",
      "处理文章 4675/5000\n",
      "处理文章 4674/5000\n",
      "处理文章 4676/5000\n",
      "处理文章 4677/5000\n",
      "处理文章 4678/5000\n",
      "处理文章 4680/5000\n",
      "处理文章 4679/5000\n",
      "处理文章 4681/5000\n",
      "处理文章 4683/5000\n",
      "处理文章 4682/5000\n",
      "处理文章 4684/5000\n",
      "处理文章 4685/5000\n",
      "处理文章 4686/5000\n",
      "处理文章 4687/5000\n",
      "处理文章 4688/5000\n",
      "处理文章 4689/5000\n",
      "处理文章 4691/5000\n",
      "处理文章 4690/5000\n",
      "处理文章 4693/5000\n",
      "处理文章 4694/5000\n",
      "处理文章 4692/5000\n",
      "处理文章 4696/5000\n",
      "处理文章 4695/5000\n",
      "处理文章 4697/5000\n",
      "处理文章 4698/5000\n",
      "处理文章 4699/5000\n",
      "处理文章 4702/5000\n",
      "处理文章 4701/5000\n",
      "处理文章 4700/5000\n",
      "处理文章 4703/5000\n",
      "处理文章 4704/5000\n",
      "处理文章 4705/5000\n",
      "处理文章 4706/5000\n",
      "处理文章 4707/5000\n",
      "处理文章 4708/5000\n",
      "处理文章 4709/5000\n",
      "处理文章 4710/5000\n",
      "处理文章 4711/5000\n",
      "处理文章 4712/5000\n",
      "处理文章 4713/5000\n",
      "处理文章 4714/5000\n",
      "处理文章 4715/5000\n",
      "处理文章 4716/5000\n",
      "处理文章 4717/5000\n",
      "处理文章 4718/5000\n",
      "处理文章 4719/5000\n",
      "处理文章 4720/5000\n",
      "处理文章 4721/5000\n",
      "处理文章 4722/5000\n",
      "处理文章 4723/5000\n",
      "处理文章 4724/5000\n",
      "处理文章 4726/5000\n",
      "处理文章 4725/5000\n",
      "处理文章 4727/5000\n",
      "处理文章 4728/5000\n",
      "处理文章 4729/5000\n",
      "处理文章 4731/5000\n",
      "处理文章 4730/5000\n",
      "处理文章 4732/5000\n",
      "处理文章 4733/5000\n",
      "处理文章 4734/5000\n",
      "处理文章 4735/5000\n",
      "处理文章 4736/5000\n",
      "处理文章 4737/5000\n",
      "处理文章 4738/5000\n",
      "处理文章 4739/5000\n",
      "处理文章 4741/5000\n",
      "处理文章 4740/5000\n",
      "处理文章 4742/5000\n",
      "处理文章 4743/5000\n",
      "处理文章 4744/5000\n",
      "处理文章 4745/5000\n",
      "处理文章 4746/5000\n",
      "处理文章 4747/5000\n",
      "处理文章 4748/5000\n",
      "处理文章 4749/5000\n",
      "处理文章 4751/5000\n",
      "处理文章 4750/5000\n",
      "处理文章 4752/5000\n",
      "处理文章 4753/5000\n",
      "处理文章 4754/5000\n",
      "处理文章 4756/5000\n",
      "处理文章 4755/5000\n",
      "处理文章 4757/5000\n",
      "处理文章 4758/5000\n",
      "处理文章 4759/5000\n",
      "处理文章 4760/5000\n",
      "处理文章 4761/5000\n",
      "处理文章 4762/5000\n",
      "处理文章 4763/5000\n",
      "处理文章 4764/5000\n",
      "处理文章 4766/5000\n",
      "处理文章 4765/5000\n",
      "处理文章 4767/5000\n",
      "处理文章 4768/5000\n",
      "处理文章 4770/5000\n",
      "处理文章 4769/5000\n",
      "处理文章 4771/5000\n",
      "处理文章 4773/5000\n",
      "处理文章 4772/5000\n",
      "处理文章 4774/5000\n",
      "处理文章 4775/5000\n",
      "处理文章 4776/5000\n",
      "处理文章 4777/5000\n",
      "处理文章 4778/5000\n",
      "处理文章 4780/5000\n",
      "处理文章 4779/5000\n",
      "处理文章 4782/5000\n",
      "处理文章 4781/5000\n",
      "处理文章 4783/5000\n",
      "处理文章 4784/5000\n",
      "处理文章 4785/5000\n",
      "处理文章 4786/5000\n",
      "处理文章 4787/5000\n",
      "处理文章 4788/5000\n",
      "处理文章 4789/5000\n",
      "处理文章 4790/5000\n",
      "处理文章 4791/5000\n",
      "处理文章 4792/5000\n",
      "处理文章 4793/5000\n",
      "处理文章 4796/5000\n",
      "处理文章 4794/5000\n",
      "处理文章 4795/5000\n",
      "处理文章 4797/5000\n",
      "处理文章 4801/5000\n",
      "处理文章 4800/5000\n",
      "处理文章 4799/5000\n",
      "处理文章 4798/5000\n",
      "处理文章 4802/5000\n",
      "处理文章 4803/5000\n",
      "处理文章 4805/5000\n",
      "处理文章 4804/5000\n",
      "处理文章 4806/5000\n",
      "处理文章 4807/5000\n",
      "处理文章 4808/5000\n",
      "处理文章 4809/5000\n",
      "处理文章 4811/5000\n",
      "处理文章 4810/5000\n",
      "处理文章 4812/5000\n",
      "处理文章 4813/5000\n",
      "处理文章 4814/5000\n",
      "处理文章 4815/5000\n",
      "处理文章 4816/5000\n",
      "处理文章 4817/5000\n",
      "处理文章 4818/5000\n",
      "处理文章 4819/5000\n",
      "处理文章 4820/5000\n",
      "处理文章 4821/5000\n",
      "处理文章 4822/5000\n",
      "处理文章 4823/5000\n",
      "处理文章 4824/5000\n",
      "处理文章 4825/5000\n",
      "处理文章 4826/5000\n",
      "处理文章 4827/5000\n",
      "处理文章 4828/5000\n",
      "处理文章 4829/5000\n",
      "处理文章 4830/5000\n",
      "处理文章 4831/5000\n",
      "处理文章 4832/5000\n",
      "处理文章 4833/5000\n",
      "处理文章 4834/5000\n",
      "处理文章 4835/5000\n",
      "处理文章 4836/5000\n",
      "处理文章 4837/5000\n",
      "处理文章 4838/5000\n",
      "处理文章 4839/5000\n",
      "处理文章 4840/5000\n",
      "处理文章 4841/5000\n",
      "处理文章 4842/5000\n",
      "处理文章 4843/5000\n",
      "处理文章 4844/5000\n",
      "处理文章 4845/5000\n",
      "处理文章 4846/5000\n",
      "处理文章 4847/5000\n",
      "处理文章 4849/5000\n",
      "处理文章 4850/5000\n",
      "处理文章 4848/5000\n",
      "处理文章 4851/5000\n",
      "处理文章 4852/5000\n",
      "处理文章 4854/5000\n",
      "处理文章 4853/5000\n",
      "处理文章 4855/5000\n",
      "处理文章 4857/5000\n",
      "处理文章 4856/5000\n",
      "处理文章 4858/5000\n",
      "处理文章 4860/5000\n",
      "处理文章 4859/5000\n",
      "处理文章 4861/5000\n",
      "处理文章 4862/5000\n",
      "处理文章 4863/5000\n",
      "处理文章 4864/5000\n",
      "处理文章 4866/5000\n",
      "处理文章 4865/5000\n",
      "处理文章 4867/5000\n",
      "处理文章 4868/5000\n",
      "处理文章 4869/5000\n",
      "处理文章 4870/5000\n",
      "处理文章 4871/5000\n",
      "处理文章 4872/5000\n",
      "处理文章 4876/5000\n",
      "处理文章 4873/5000\n",
      "处理文章 4875/5000\n",
      "处理文章 4877/5000\n",
      "处理文章 4874/5000\n",
      "处理文章 4878/5000\n",
      "处理文章 4879/5000\n",
      "处理文章 4880/5000\n",
      "处理文章 4881/5000\n",
      "处理文章 4882/5000\n",
      "处理文章 4883/5000\n",
      "处理文章 4887/5000\n",
      "处理文章 4884/5000\n",
      "处理文章 4885/5000\n",
      "处理文章 4886/5000\n",
      "处理文章 4888/5000\n",
      "处理文章 4889/5000\n",
      "处理文章 4890/5000\n",
      "处理文章 4892/5000\n",
      "处理文章 4891/5000\n",
      "处理文章 4894/5000\n",
      "处理文章 4893/5000\n",
      "处理文章 4896/5000\n",
      "处理文章 4895/5000\n",
      "处理文章 4897/5000\n",
      "处理文章 4898/5000\n",
      "处理文章 4899/5000\n",
      "处理文章 4900/5000\n",
      "处理文章 4901/5000\n",
      "处理文章 4902/5000\n",
      "处理文章 4903/5000\n",
      "处理文章 4904/5000\n",
      "处理文章 4905/5000\n",
      "处理文章 4906/5000\n",
      "处理文章 4907/5000\n",
      "处理文章 4908/5000\n",
      "处理文章 4909/5000\n",
      "处理文章 4910/5000\n",
      "处理文章 4911/5000\n",
      "处理文章 4912/5000\n",
      "处理文章 4914/5000\n",
      "处理文章 4913/5000\n",
      "处理文章 4915/5000\n",
      "处理文章 4916/5000\n",
      "处理文章 4919/5000\n",
      "处理文章 4917/5000\n",
      "处理文章 4918/5000\n",
      "处理文章 4920/5000\n",
      "处理文章 4921/5000\n",
      "处理文章 4922/5000\n",
      "处理文章 4923/5000\n",
      "处理文章 4924/5000\n",
      "处理文章 4926/5000\n",
      "处理文章 4925/5000\n",
      "处理文章 4927/5000\n",
      "处理文章 4928/5000\n",
      "处理文章 4930/5000\n",
      "处理文章 4931/5000\n",
      "处理文章 4929/5000\n",
      "处理文章 4933/5000\n",
      "处理文章 4932/5000\n",
      "处理文章 4934/5000\n",
      "处理文章 4935/5000\n",
      "处理文章 4936/5000\n",
      "处理文章 4937/5000\n",
      "处理文章 4938/5000\n",
      "处理文章 4939/5000\n",
      "处理文章 4940/5000\n",
      "处理文章 4941/5000\n",
      "处理文章 4942/5000\n",
      "处理文章 4943/5000\n",
      "处理文章 4945/5000\n",
      "处理文章 4944/5000\n",
      "处理文章 4946/5000\n",
      "处理文章 4947/5000\n",
      "处理文章 4948/5000\n",
      "处理文章 4950/5000\n",
      "处理文章 4951/5000\n",
      "处理文章 4949/5000\n",
      "处理文章 4952/5000\n",
      "处理文章 4953/5000\n",
      "处理文章 4954/5000\n",
      "处理文章 4955/5000\n",
      "处理文章 4956/5000\n",
      "处理文章 4957/5000\n",
      "处理文章 4958/5000\n",
      "处理文章 4959/5000\n",
      "处理文章 4960/5000\n",
      "处理文章 4961/5000\n",
      "处理文章 4963/5000\n",
      "处理文章 4962/5000\n",
      "处理文章 4965/5000\n",
      "处理文章 4964/5000\n",
      "处理文章 4966/5000\n",
      "处理文章 4968/5000\n",
      "处理文章 4969/5000\n",
      "处理文章 4967/5000\n",
      "处理文章 4971/5000\n",
      "处理文章 4970/5000\n",
      "处理文章 4972/5000\n",
      "处理文章 4973/5000\n",
      "处理文章 4974/5000\n",
      "处理文章 4975/5000\n",
      "处理文章 4976/5000\n",
      "处理文章 4978/5000\n",
      "处理文章 4977/5000\n",
      "处理文章 4979/5000\n",
      "处理文章 4980/5000\n",
      "处理文章 4981/5000\n",
      "处理文章 4982/5000\n",
      "处理文章 4983/5000\n",
      "处理文章 4984/5000\n",
      "处理文章 4986/5000\n",
      "处理文章 4985/5000\n",
      "处理文章 4988/5000\n",
      "处理文章 4987/5000\n",
      "处理文章 4989/5000\n",
      "处理文章 4990/5000\n",
      "处理文章 4992/5000\n",
      "处理文章 4991/5000\n",
      "处理文章 4993/5000\n",
      "处理文章 4994/5000\n",
      "处理文章 4995/5000\n",
      "处理文章 4996/5000\n",
      "处理文章 4997/5000\n",
      "处理文章 4999/5000\n",
      "处理文章 4998/5000\n",
      "处理文章 5000/5000\n",
      "处理完成，成功处理 4999/5000 条记录\n",
      "结果已保存至 /root/autodl-tmp/data/sft/deepspeek_sft_dataset_5k.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os                    # 操作系统相关功能，如文件路径操作\n",
    "import json                  # 处理JSON格式数据\n",
    "import time                  # 时间相关功能，如延迟等待\n",
    "import random                # 随机数生成和随机采样\n",
    "import openai                # OpenAI API客户端，这里用于调用DeepSeek API\n",
    "from tqdm import tqdm        # 进度条显示库\n",
    "import concurrent.futures    # 并发处理库，用于多线程\n",
    "import threading             # 线程相关功能，用于线程锁\n",
    "\n",
    "# ==================== 配置参数 ====================\n",
    "API_KEY = \"sk-6uvkiohp89g6798e9eda\"  # DeepSeek API的密钥，用于身份验证\n",
    "INPUT_FILE = \"/root/autodl-tmp/data/filtered_financial_news_5k.jsonl\"    # 输入文件路径：过滤后的金融新闻数据\n",
    "OUTPUT_FILE = \"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_5k.jsonl\" # 输出文件路径：生成的SFT训练数据\n",
    "SAMPLE_COUNT = 5000          # 需要采样的记录数量：从输入数据中选择5000条进行处理\n",
    "MAX_WORKERS = 80             # 并行处理的最大线程数：同时运行80个线程来加速处理\n",
    "REQUEST_INTERVAL = 1         # 请求间隔（秒）：避免API调用过于频繁触发限制\n",
    "RANDOM_SEED = 57             # 随机种子：确保每次运行程序时随机采样的结果都相同\n",
    "\n",
    "# ==================== 随机种子设置 ====================\n",
    "# 设置随机种子，确保程序的可重现性\n",
    "# 随机种子的作用：让\"随机\"变得可预测和可重复\n",
    "# 例如：每次运行程序时，random.sample()会选择相同的数据\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==================== 线程安全锁 ====================\n",
    "# 在多线程环境中，多个线程可能同时访问共享资源，导致数据混乱\n",
    "# 使用锁来确保同一时间只有一个线程能访问特定资源\n",
    "print_lock = threading.Lock()   # 用于保护打印输出，避免多个线程同时打印导致输出混乱\n",
    "output_lock = threading.Lock()  # 用于保护文件写入，避免多个线程同时写入同一文件导致数据损坏\n",
    "\n",
    "# ==================== API客户端初始化 ====================\n",
    "# 初始化OpenAI客户端，配置为使用DeepSeek的API服务\n",
    "client = openai.OpenAI(\n",
    "    api_key=API_KEY,                        # 使用上面定义的API密钥\n",
    "    base_url=\"https://api.deepseek.com\"     # DeepSeek的API服务地址\n",
    ")\n",
    "\n",
    "def truncate_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    截断文本以满足API长度限制\n",
    "    \n",
    "    参数:\n",
    "        text: 需要截断的文本\n",
    "        max_length: 最大允许长度，默认5000字符\n",
    "    \n",
    "    返回:\n",
    "        截断后的文本\n",
    "    \n",
    "    作用: API通常对输入文本长度有限制，过长的文本会导致请求失败\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text  # 如果文本长度在限制内，直接返回原文本\n",
    "    return text[:max_length]  # 如果超出限制，只返回前max_length个字符\n",
    "\n",
    "def generate_question(item):\n",
    "    \"\"\"\n",
    "    第一阶段：基于英文金融文章生成中文金融问题\n",
    "    \n",
    "    参数:\n",
    "        item: 包含文章内容的字典，应该有'Article'字段\n",
    "    \n",
    "    返回:\n",
    "        包含生成问题的字典，格式：{\"full_question\": \"问题内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这个函数的特色：\n",
    "    1. 将英文金融文章转换为中文问题\n",
    "    2. 确保问题包含足够的背景信息，让回答者无需看原文就能回答\n",
    "    3. 避免直接引用文章，而是将信息融入问题中\n",
    "    \"\"\"\n",
    "    # 从输入数据中提取文章内容\n",
    "    article = item.get('Article', '')  # 使用get方法安全获取，如果没有'Article'字段则返回空字符串\n",
    "    \n",
    "    # 截断文章以满足API长度限制\n",
    "    article_truncated = truncate_text(article)\n",
    "    \n",
    "    # 构建发送给AI的提示词（prompt）\n",
    "    # 这个提示词告诉AI如何处理文章并生成问题\n",
    "    prompt = f\"\"\"\n",
    "请基于以下英文金融文章，创建一个详细的中文金融问题。\n",
    "\n",
    "英文文章:\n",
    "{article_truncated}\n",
    "\n",
    "请完成以下任务:\n",
    "1. 从文章中提取关键信息、数据、事实和核心观点\n",
    "2. 创建一个针对这些关键信息的专业金融领域问题\n",
    "3. 问题必须包含足够详细的背景信息和事实，确保仅凭问题本身就能够推导出合理的回答\n",
    "\n",
    "输出格式必须是有效的JSON，结构如下:\n",
    "{{\n",
    "  \"full_question\": \"这里是包含详细背景信息的专业金融问题\"\n",
    "}}\n",
    "\n",
    "要求:\n",
    "- 问题必须具体且深入，能够引导出金融专业领域的分析\n",
    "- 必须包含足够丰富的事实信息，使第三方仅通过阅读问题就能回答\n",
    "- 禁止出现\"本文\"、\"文章\"、\"整体基调\"、\"情绪\"等字样\n",
    "- 禁止对文章本身进行评价或总结\n",
    "- 直接以陈述事实的方式提供背景信息\n",
    "- 问题应以客观的方式呈现数据和事实，避免主观评价\n",
    "- 问题要以自然、符合实际提问习惯的方式表达\n",
    "- 问题内容要特别详细，包含文章中所有能够支持回答问题的关键信息\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用DeepSeek API生成问题\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",  # 使用DeepSeek的聊天模型\n",
    "            messages=[\n",
    "                # 系统消息：定义AI的角色和任务\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融数据分析助手，精通英文金融文章翻译和问题构建。你的任务是创建包含充分背景信息的专业金融问题。\"},\n",
    "                # 用户消息：具体的任务指令\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,      # 不使用流式输出，一次性获取完整回复\n",
    "            temperature=0.7    # 控制输出的随机性，0.7表示适中的创造性\n",
    "        )\n",
    "        # 获取AI的回复内容\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # 从AI回复中提取JSON格式的数据\n",
    "        # AI的回复可能包含其他文本，需要找到JSON部分\n",
    "        json_start = result.find('{')        # 找到第一个'{'的位置\n",
    "        json_end = result.rfind('}') + 1     # 找到最后一个'}'的位置\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            # 提取JSON字符串\n",
    "            json_str = result[json_start:json_end]\n",
    "            # 解析JSON\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # 检查JSON是否包含必要的字段\n",
    "            if \"full_question\" in data:\n",
    "                return {\"full_question\": data[\"full_question\"]}\n",
    "            else:\n",
    "                print(\"警告: 返回的JSON缺少必要字段\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"无法在响应中找到有效的JSON\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 如果出现任何错误（网络错误、API错误、JSON解析错误等），打印错误信息\n",
    "        print(f\"生成问题时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_answer(question_data):\n",
    "    \"\"\"\n",
    "    第二阶段：基于生成的问题创建带有思考过程的专业答案\n",
    "    \n",
    "    参数:\n",
    "        question_data: 包含问题的字典，应该有'full_question'字段\n",
    "    \n",
    "    返回:\n",
    "        包含答案的字典，格式：{\"answer\": \"完整答案内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这个函数的特色：\n",
    "    1. 生成包含思考过程的答案，模拟人类专家的分析思路\n",
    "    2. 答案分为两部分：<think>标签内的思考过程 + 最终结论\n",
    "    3. 这种格式有助于训练模型学会逐步推理\n",
    "    \"\"\"\n",
    "    # 从输入数据中提取问题内容\n",
    "    full_question = question_data.get(\"full_question\", \"\")\n",
    "    \n",
    "    # 检查问题是否为空\n",
    "    if not full_question:\n",
    "        print(\"错误: 问题内容为空\")\n",
    "        return None\n",
    "    \n",
    "    # 构建用于生成答案的提示词\n",
    "    prompt = f\"\"\"\n",
    "请针对以下金融问题提供专业、全面的分析和回答。\n",
    "\n",
    "问题:\n",
    "{full_question}\n",
    "\n",
    "请仅基于问题中提供的信息进行回答，不要引入外部知识。你的回答必须包含两部分：\n",
    "1. 使用<think>标签包围的详细思考过程\n",
    "2. 最终的专业回答\n",
    "\n",
    "首先，使用<think>和</think>标签包围你的详细思考过程：\n",
    "<think>\n",
    "在这里，你需要进行非常详细的分析，包括以下几方面：\n",
    "1. 问题背景分析：分析问题中提供的关键信息和数据\n",
    "2. 数据解读：对问题中的数字、百分比等数据进行专业解读\n",
    "3. 原因探究：分析可能的原因和影响因素\n",
    "4. 多角度思考：从多个维度考虑问题\n",
    "5. 推理过程：清晰展示你的推理步骤和逻辑\n",
    "</think>\n",
    "\n",
    "然后，不使用任何标签，直接提供你的最终专业回答。\n",
    "\n",
    "回答要求:\n",
    "- 使用专业的金融术语和表达方式\n",
    "- 提供有价值的见解和结论\n",
    "- 清晰解释原因和影响\n",
    "- 仅基于问题中提供的信息进行回答，不要编造事实\n",
    "- 回答必须使用中文\n",
    "\n",
    "严格注意：\n",
    "- 思考过程必须详细，至少包含300字以上的分析\n",
    "- 最终回答必须放在</think>标签之后，不得包含在思考标签内\n",
    "- 思考过程和最终回答必须严格分开\n",
    "- 禁止在回答中再次使用<think>或</think>标签\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用DeepSeek API生成答案\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                # 系统消息：定义AI作为专业金融分析师的角色\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融分析师，擅长提供深入、全面的金融分析。你的回答必须包含详细的思考过程和最终结论。\"},\n",
    "                # 用户消息：具体的分析任务\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        # 获取AI生成的答案\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # 检查答案是否包含必要的思考过程标签\n",
    "        if \"<think>\" in answer and \"</think>\" in answer:\n",
    "            return {\"answer\": answer}\n",
    "        else:\n",
    "            # 如果AI没有按要求使用标签，尝试自动添加标签\n",
    "            # 假设前面部分是思考过程，后面部分是结论\n",
    "            thinking_end = answer.find(\"\\n\\n\")  # 寻找段落分隔符\n",
    "            if thinking_end != -1:\n",
    "                thinking = answer[:thinking_end]      # 前面部分作为思考过程\n",
    "                conclusion = answer[thinking_end+2:]  # 后面部分作为最终答案\n",
    "                # 重新格式化答案\n",
    "                formatted_answer = f\"<think>\\n{thinking}\\n</think>\\n\\n{conclusion}\"\n",
    "                return {\"answer\": formatted_answer}\n",
    "            else:\n",
    "                print(\"警告: 无法在回答中识别思考过程\")\n",
    "                return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"生成回答时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_sft_data(item, index, total):\n",
    "    \"\"\"\n",
    "    两阶段处理：先生成问题，再生成答案，最后合并为SFT格式\n",
    "    \n",
    "    参数:\n",
    "        item: 原始文章数据\n",
    "        index: 当前处理的文章索引（用于显示进度）\n",
    "        total: 总文章数量\n",
    "    \n",
    "    返回:\n",
    "        SFT格式的训练数据字典，包含instruction、input、output三个字段\n",
    "        如果失败返回None\n",
    "    \n",
    "    SFT (Supervised Fine-Tuning) 格式说明：\n",
    "    - instruction: 任务指令，告诉模型要做什么\n",
    "    - input: 具体的输入内容（这里是金融问题）\n",
    "    - output: 期望的输出内容（这里是带思考过程的答案）\n",
    "    \"\"\"\n",
    "    # 使用线程锁确保打印输出不会混乱\n",
    "    with print_lock:\n",
    "        print(f\"处理文章 {index+1}/{total}\")\n",
    "    \n",
    "    # 第一阶段：生成问题\n",
    "    question_data = generate_question(item)\n",
    "    if not question_data:\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 问题生成失败\")\n",
    "        return None\n",
    "    \n",
    "    # 添加随机延迟，避免API请求过于集中\n",
    "    # 这样可以避免触发API的频率限制\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 第二阶段：生成答案\n",
    "    answer_data = generate_answer(question_data)\n",
    "    if not answer_data:\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 答案生成失败\")\n",
    "        return None\n",
    "    \n",
    "    # 合并为SFT训练数据格式\n",
    "    sft_data = {\n",
    "        \"instruction\": \"以下是一个关于金融领域的问题，请提供详细的分析和见解。\",  # 统一的任务指令\n",
    "        \"input\": question_data[\"full_question\"],    # 生成的金融问题\n",
    "        \"output\": answer_data[\"answer\"]             # 生成的带思考过程的答案\n",
    "    }\n",
    "    \n",
    "    return sft_data\n",
    "\n",
    "def process_article(args):\n",
    "    \"\"\"\n",
    "    处理单篇文章的包装函数，专门用于并行处理\n",
    "    \n",
    "    参数:\n",
    "        args: 包含(item, index, total, output_file)的元组\n",
    "    \n",
    "    返回:\n",
    "        True表示处理成功，False表示处理失败\n",
    "    \n",
    "    这个函数的作用：\n",
    "    1. 解包参数\n",
    "    2. 调用create_sft_data处理文章\n",
    "    3. 将结果写入文件（使用线程锁保证安全）\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    item, index, total, output_file = args\n",
    "    \n",
    "    # 添加随机延迟，避免所有线程同时发起API请求\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 处理文章生成SFT数据\n",
    "    result = create_sft_data(item, index, total)\n",
    "    if result:\n",
    "        # 使用文件写入锁，确保多个线程不会同时写入同一文件\n",
    "        with output_lock:\n",
    "            # 以追加模式打开文件，将结果写入\n",
    "            with open(output_file, 'a', encoding='utf-8') as out_f:\n",
    "                # 将结果转换为JSON格式并写入文件，每行一个JSON对象\n",
    "                out_f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "        return True  # 返回成功标志\n",
    "    return False     # 返回失败标志\n",
    "\n",
    "def sample_evenly(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    均匀采样指定数量的数据\n",
    "    \n",
    "    参数:\n",
    "        data_list: 原始数据列表\n",
    "        sample_count: 需要采样的数量\n",
    "    \n",
    "    返回:\n",
    "        采样后的数据列表\n",
    "    \n",
    "    均匀采样的原理：\n",
    "    - 将数据按固定间隔选择，确保选择的数据在原始数据中分布均匀\n",
    "    - 例如：有1000条数据，要选100条，就每隔10条选一条\n",
    "    \"\"\"\n",
    "    total_count = len(data_list)\n",
    "    \n",
    "    # 如果原始数据不够，返回全部数据\n",
    "    if total_count <= sample_count:\n",
    "        print(f\"数据总量({total_count})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 计算采样间隔\n",
    "    step = total_count / sample_count\n",
    "    # 生成采样索引列表\n",
    "    indices = [int(i * step) for i in range(sample_count)]\n",
    "    \n",
    "    # 确保最后一个元素是最后一个索引，保证覆盖到数据的末尾\n",
    "    if indices[-1] != total_count - 1:\n",
    "        indices[-1] = total_count - 1\n",
    "        \n",
    "    # 根据索引返回采样数据\n",
    "    return [data_list[i] for i in indices]\n",
    "\n",
    "def random_sample(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    随机采样指定数量的数据，使用固定随机种子确保可复现\n",
    "    \n",
    "    参数:\n",
    "        data_list: 原始数据列表\n",
    "        sample_count: 需要采样的数量\n",
    "    \n",
    "    返回:\n",
    "        随机采样后的数据列表\n",
    "    \n",
    "    随机采样的特点：\n",
    "    - 每条数据被选中的概率相等\n",
    "    - 由于使用了固定随机种子，每次运行结果相同\n",
    "    \"\"\"\n",
    "    if len(data_list) <= sample_count:\n",
    "        print(f\"数据总量({len(data_list)})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 由于在程序开始时已设置了随机种子，这里的随机采样将是可复现的\n",
    "    # 即：每次运行程序，random.sample会选择相同的数据\n",
    "    sampled_items = random.sample(data_list, sample_count)\n",
    "    return sampled_items\n",
    "\n",
    "def stratified_random_sample(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    分层随机采样 - 结合均匀采样的覆盖性和随机采样的随机性\n",
    "    \n",
    "    参数:\n",
    "        data_list: 原始数据列表\n",
    "        sample_count: 需要采样的数量\n",
    "    \n",
    "    返回:\n",
    "        分层随机采样后的数据列表\n",
    "    \n",
    "    分层随机采样的原理：\n",
    "    1. 将原始数据分成若干个区间（层）\n",
    "    2. 从每个区间中随机选择一定数量的数据\n",
    "    3. 这样既保证了数据的代表性（覆盖所有区间），又保持了随机性\n",
    "    \n",
    "    优势：\n",
    "    - 比纯随机采样更有代表性\n",
    "    - 比均匀采样更有随机性\n",
    "    - 适合大规模数据集的采样\n",
    "    \"\"\"\n",
    "    total_count = len(data_list)\n",
    "    \n",
    "    if total_count <= sample_count:\n",
    "        print(f\"数据总量({total_count})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 计算需要划分的区间数量\n",
    "    # 如果样本数量太小，至少划分为样本数量的区间\n",
    "    num_strata = min(sample_count, total_count // 10 + 1)\n",
    "    \n",
    "    # 计算每个区间的大小\n",
    "    stratum_size = total_count // num_strata\n",
    "    \n",
    "    sampled_items = []  # 存储采样结果\n",
    "    for i in range(num_strata):\n",
    "        # 计算当前区间的起止索引\n",
    "        start_idx = i * stratum_size\n",
    "        end_idx = start_idx + stratum_size if i < num_strata - 1 else total_count\n",
    "        \n",
    "        # 当前区间的数据\n",
    "        stratum_data = data_list[start_idx:end_idx]\n",
    "        \n",
    "        # 计算当前区间需要采样的数量\n",
    "        # 按比例分配采样数量\n",
    "        stratum_sample_count = max(1, int((end_idx - start_idx) / total_count * sample_count))\n",
    "        \n",
    "        # 确保总采样数不超过要求\n",
    "        if len(sampled_items) + stratum_sample_count > sample_count:\n",
    "            stratum_sample_count = sample_count - len(sampled_items)\n",
    "        \n",
    "        # 如果区间样本数少于要求采样数，全部选择\n",
    "        if len(stratum_data) <= stratum_sample_count:\n",
    "            sampled_items.extend(stratum_data)\n",
    "        else:\n",
    "            # 从当前区间随机采样\n",
    "            stratum_samples = random.sample(stratum_data, stratum_sample_count)\n",
    "            sampled_items.extend(stratum_samples)\n",
    "        \n",
    "        # 如果已经达到所需样本数，结束循环\n",
    "        if len(sampled_items) >= sample_count:\n",
    "            break\n",
    "    \n",
    "    # 处理边界情况：最终样本数小于要求数\n",
    "    if len(sampled_items) < sample_count:\n",
    "        # 从未选择的数据中随机补充\n",
    "        remaining = [item for item in data_list if item not in sampled_items]\n",
    "        additional = random.sample(remaining, sample_count - len(sampled_items))\n",
    "        sampled_items.extend(additional)\n",
    "    \n",
    "    return sampled_items\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：协调整个数据处理流程\n",
    "    \n",
    "    主要步骤：\n",
    "    1. 检查输入文件\n",
    "    2. 读取和解析数据\n",
    "    3. 采样数据\n",
    "    4. 并行处理生成SFT数据\n",
    "    5. 统计处理结果\n",
    "    \"\"\"\n",
    "    # 确保输出目录存在\n",
    "    # os.path.dirname()获取文件路径的目录部分\n",
    "    # exist_ok=True表示如果目录已存在不会报错\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"错误: 输入文件 '{INPUT_FILE}' 不存在!\")\n",
    "        return\n",
    "    \n",
    "    # 检查文件大小，确保文件不为空\n",
    "    file_size = os.path.getsize(INPUT_FILE)\n",
    "    print(f\"输入文件大小: {file_size} 字节\")\n",
    "    if file_size == 0:\n",
    "        print(\"错误: 输入文件为空!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"使用随机种子: {RANDOM_SEED} 确保可复现性\")\n",
    "    \n",
    "    # ==================== 读取和解析输入文件 ====================\n",
    "    # 读取输入文件，保留完整的JSON对象\n",
    "    items = []          # 存储有效的数据记录\n",
    "    line_count = 0      # 总行数计数器\n",
    "    error_count = 0     # 错误行数计数器\n",
    "    valid_count = 0     # 有效记录计数器\n",
    "    \n",
    "    try:\n",
    "        # 逐行读取JSONL文件（每行一个JSON对象）\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):  # enumerate从1开始计数\n",
    "                line_count += 1\n",
    "                line = line.strip()  # 去除行首行尾的空白字符\n",
    "                if not line:         # 跳过空行\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # 尝试解析JSON\n",
    "                    item = json.loads(line)\n",
    "                    \n",
    "                    # 检查必要的字段是否存在\n",
    "                    # 支持两种字段名格式：'Article'/'article' 和 'Summary'/'summary'\n",
    "                    if 'Article' in item:\n",
    "                        if 'Summary' in item:\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'Summary'字段\")\n",
    "                    elif 'article' in item:\n",
    "                        if 'summary' in item:\n",
    "                            # 标准化字段名，统一使用大写开头\n",
    "                            item['Article'] = item['article']\n",
    "                            item['Summary'] = item['summary']\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'summary'字段\")\n",
    "                    else:\n",
    "                        print(f\"警告: 第 {line_num} 行没有'Article'或'article'字段\")\n",
    "                        print(f\"可用字段: {list(item.keys())}\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    # JSON解析失败\n",
    "                    error_count += 1\n",
    "                    print(f\"错误: 第 {line_num} 行JSON解析失败: {e}\")\n",
    "                    print(f\"问题行内容: {line[:100]}...\")  # 只显示前100个字符\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "    \n",
    "    # 打印文件读取统计信息\n",
    "    print(f\"文件共有 {line_count} 行\")\n",
    "    print(f\"解析错误: {error_count} 行\")\n",
    "    print(f\"成功读取: {valid_count} 条有效记录\")\n",
    "    print(f\"最终收集: {len(items)} 条记录\")\n",
    "    \n",
    "    # 如果没有读取到任何有效数据，显示调试信息\n",
    "    if len(items) == 0:\n",
    "        print(\"\\n尝试显示文件前5行内容进行调试:\")\n",
    "        try:\n",
    "            with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= 5:  # 只显示前5行\n",
    "                        break\n",
    "                    print(f\"第 {i+1} 行: {line.strip()[:200]}...\")  # 显示前200个字符\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        print(f\"JSON键: {list(data.keys())}\")  # 显示JSON的所有键\n",
    "                    except:\n",
    "                        pass  # 忽略JSON解析错误\n",
    "        except Exception as e:\n",
    "            print(f\"显示文件内容时发生错误: {e}\")\n",
    "        return  # 没有数据就退出程序\n",
    "    \n",
    "    # ==================== 数据采样 ====================\n",
    "    # 使用分层随机采样替代简单随机采样\n",
    "    # 这样可以获得更有代表性的数据子集\n",
    "    sampled_items = stratified_random_sample(items, SAMPLE_COUNT)\n",
    "    print(f\"分层随机采样了 {len(sampled_items)}/{len(items)} 条记录\")\n",
    "    \n",
    "    # 创建或清空输出文件\n",
    "    # 'w'模式会清空文件内容，确保输出文件是干净的\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        pass  # 只是创建/清空文件，不写入任何内容\n",
    "    \n",
    "    # ==================== 并行处理 ====================\n",
    "    print(f\"开始并行处理，最大线程数: {MAX_WORKERS}\")\n",
    "    \n",
    "    # 准备传递给每个线程的参数\n",
    "    # 每个元组包含：(文章数据, 索引, 总数, 输出文件路径)\n",
    "    args_list = [(item, i, len(sampled_items), OUTPUT_FILE) for i, item in enumerate(sampled_items)]\n",
    "    \n",
    "    # 使用线程池并行处理所有文章\n",
    "    success_count = 0  # 成功处理的文章数量\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # executor.map会将args_list中的每个元素传递给process_article函数\n",
    "        # 并在多个线程中并行执行\n",
    "        results = list(executor.map(process_article, args_list))\n",
    "        # 统计成功处理的数量（process_article返回True表示成功）\n",
    "        success_count = sum(1 for r in results if r)\n",
    "    \n",
    "    # 打印最终处理结果\n",
    "    print(f\"处理完成，成功处理 {success_count}/{len(sampled_items)} 条记录\")\n",
    "    print(f\"结果已保存至 {OUTPUT_FILE}\")\n",
    "\n",
    "# ==================== 程序入口 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # 运行主函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa160b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526202018120.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8661f7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528113624512.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21e492",
   "metadata": {},
   "source": [
    "## 3.SFT多轮数据集对话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a782b96",
   "metadata": {},
   "source": [
    "### 1. **核心思路差异**\n",
    "\n",
    "| 对比维度 | 单轮对话版本 | 多轮对话版本 |\n",
    "|---------|-------------|-------------|\n",
    "| **数据格式** | instruction/input/output | messages数组格式 |\n",
    "| **对话结构** | 一问一答 | 1-3轮连续对话 |\n",
    "| **训练目标** | 单次回答能力 | 多轮对话能力 |\n",
    "| **复杂度** | 相对简单 | 显著增加 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35673f8f",
   "metadata": {},
   "source": [
    "\n",
    "### 2. **功能实现差异**\n",
    "\n",
    "#### **新增核心功能：**\n",
    "```python\n",
    "# 1. 多轮对话生成\n",
    "- generate_follow_up_question()：生成后续问题\n",
    "- generate_follow_up_answer()：生成后续回答\n",
    "- extract_final_answer()：提取答案核心部分\n",
    "\n",
    "# 2. 对话历史管理\n",
    "- conversation = [(问题1, 答案1), (问题2, 答案2), ...]\n",
    "- 保持对话的连贯性和一致性\n",
    "\n",
    "# 3. 随机轮数控制\n",
    "- actual_turns = random.randint(1, MAX_TURNS)\n",
    "- 增加数据的多样性\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eeffd8",
   "metadata": {},
   "source": [
    "\n",
    "#### **输出格式变化：**\n",
    "```python\n",
    "# 单轮版本输出：\n",
    "{\n",
    "  \"instruction\": \"任务指令\",\n",
    "  \"input\": \"问题内容\", \n",
    "  \"output\": \"答案内容\"\n",
    "}\n",
    "\n",
    "# 多轮版本输出：\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"系统消息\"},\n",
    "    {\"role\": \"user\", \"content\": \"问题1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"答案1\"},\n",
    "    {\"role\": \"user\", \"content\": \"问题2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"答案2\"}\n",
    "  ],\n",
    "  \"turns\": 2\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ea73f",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **技术实现差异**\n",
    "\n",
    "#### **API调用策略：**\n",
    "```python\n",
    "# 单轮版本：每篇文章2次API调用\n",
    "1. 生成问题\n",
    "2. 生成答案\n",
    "\n",
    "# 多轮版本：每篇文章2-6次API调用\n",
    "1. 生成初始问题\n",
    "2. 生成第一轮答案\n",
    "3. 生成后续问题（可能多次）\n",
    "4. 生成后续答案（可能多次）\n",
    "```\n",
    "\n",
    "#### **内存和复杂度：**\n",
    "```python\n",
    "# 单轮版本：\n",
    "- 内存占用较小\n",
    "- 处理逻辑简单\n",
    "- 错误处理相对简单\n",
    "\n",
    "# 多轮版本：\n",
    "- 需要维护对话历史\n",
    "- 处理逻辑复杂（需要考虑上下文）\n",
    "- 错误处理更复杂（任何一轮失败都可能影响整个对话）\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31d196",
   "metadata": {},
   "source": [
    "### 4. **数据质量差异**\n",
    "\n",
    "#### **训练数据复杂度：**\n",
    "```python\n",
    "# 单轮版本数据特点：\n",
    "- 每条数据独立完整\n",
    "- 问答逻辑相对简单\n",
    "- 适合基础对话能力训练\n",
    "\n",
    "# 多轮版本数据特点：\n",
    "- 数据间存在上下文依赖\n",
    "- 对话逻辑更复杂\n",
    "- 适合高级对话能力训练\n",
    "```\n",
    "\n",
    "#### **数据一致性要求：**\n",
    "```python\n",
    "# 单轮版本：\n",
    "- 只需保证单个问答对的质量\n",
    "- 错误影响范围有限\n",
    "\n",
    "# 多轮版本：\n",
    "- 需要保证整个对话的连贯性\n",
    "- 任何一轮的错误都可能影响后续对话\n",
    "- 对AI模型的稳定性要求更高\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455636bb",
   "metadata": {},
   "source": [
    "\n",
    "### 5. **性能和资源消耗差异**\n",
    "\n",
    "#### **API调用成本：**\n",
    "```python\n",
    "# 成本对比（以10条数据为例）\n",
    "单轮版本：10 × 2 = 20次API调用\n",
    "多轮版本：10 × (2-6) = 20-60次API调用\n",
    "\n",
    "# 实际成本差异：\n",
    "- 多轮版本的API成本是单轮版本的1.5-3倍\n",
    "- 处理时间也相应增加\n",
    "```\n",
    "\n",
    "#### **错误处理复杂度：**\n",
    "```python\n",
    "# 单轮版本错误处理：\n",
    "if not question_data or not answer_data:\n",
    "    return None  # 简单失败处理\n",
    "\n",
    "# 多轮版本错误处理：\n",
    "for turn in range(1, max_turns):\n",
    "    if not generate_question() or not generate_answer():\n",
    "        break  # 部分成功也可以保留\n",
    "    # 需要考虑部分对话的价值\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139026d",
   "metadata": {},
   "source": [
    "\n",
    "### 6. **适用场景差异**\n",
    "\n",
    "#### **训练目标不同：**\n",
    "```python\n",
    "# 单轮版本适合：\n",
    "- 基础问答能力训练\n",
    "- 知识检索型任务\n",
    "- 快速原型验证\n",
    "- 资源受限的场景\n",
    "\n",
    "# 多轮版本适合：\n",
    "- 复杂对话系统训练\n",
    "- 客服机器人训练\n",
    "- 教育辅导系统\n",
    "- 需要深度交互的应用\n",
    "```\n",
    "\n",
    "#### **数据集规模考虑：**\n",
    "```python\n",
    "# 单轮版本：\n",
    "- 可以处理大规模数据集（如50k+条）\n",
    "- 成本可控，适合批量生产\n",
    "\n",
    "# 多轮版本：\n",
    "- 更适合中小规模高质量数据集（如5k-10k条）\n",
    "- 注重质量而非数量\n",
    "- 每条数据的信息密度更高\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d54799",
   "metadata": {},
   "source": [
    "\n",
    "### 7. **技术架构差异总结**\n",
    "\n",
    "| 维度 | 单轮对话版本 | 多轮对话版本 |\n",
    "|------|-------------|-------------|\n",
    "| **复杂度** | 低 | 高 |\n",
    "| **成本** | 低 | 中-高 |\n",
    "| **数据质量** | 标准 | 高 |\n",
    "| **训练效果** | 基础对话能力 | 高级对话能力 |\n",
    "| **维护难度** | 简单 | 复杂 |\n",
    "| **扩展性** | 好 | 一般 |\n",
    "| **错误容忍度** | 高 | 低 |\n",
    "\n",
    "### 8. **实际应用建议**\n",
    "\n",
    "```python\n",
    "# 选择单轮版本的情况：\n",
    "- 预算有限\n",
    "- 需要大量训练数据\n",
    "- 基础对话能力训练\n",
    "- 快速迭代验证\n",
    "\n",
    "# 选择多轮版本的情况：\n",
    "- 追求高质量对话体验\n",
    "- 有充足的计算资源\n",
    "- 需要复杂交互能力\n",
    "- 对话连贯性要求高\n",
    "```\n",
    "\n",
    "这两个版本代表了不同的设计：**单轮版本追求效率和规模，多轮版本追求质量和深度**。在实际项目中，可以根据具体需求和资源情况选择合适的方案，甚至可以将两种方案结合使用，先用单轮版本生成大量基础数据，再用多轮版本生成少量高质量数据进行补充。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1708c-1fbf-4a53-a85d-3a55e1188da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文件大小: 278687859 字节\n",
      "使用随机种子: 57 确保可复现性\n",
      "最大对话轮数: 3\n",
      "文件共有 70061 行\n",
      "解析错误: 0 行\n",
      "成功读取: 70061 条有效记录\n",
      "最终收集: 70061 条记录\n",
      "分层随机采样了 10/70061 条记录\n",
      "开始并行处理，最大线程数: 10\n",
      "处理文章 10/10\n",
      "处理文章 6/10\n",
      "处理文章 8/10\n",
      "处理文章 4/10\n",
      "处理文章 9/10\n",
      "处理文章 2/10\n",
      "处理文章 3/10\n",
      "处理文章 5/10\n",
      "处理文章 7/10\n",
      "处理文章 1/10\n",
      "处理完成，成功处理 10/10 条记录\n",
      "结果已保存至 /root/autodl-tmp/data/sft/deepspeek_multi_turn_dataset.jsonl\n",
      "对话轮数分布:\n",
      "1轮对话: 4条 (40.00%)\n",
      "2轮对话: 4条 (40.00%)\n",
      "3轮对话: 2条 (20.00%)\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os                    # 操作系统相关功能，如文件路径操作\n",
    "import json                  # 处理JSON格式数据\n",
    "import time                  # 时间相关功能，如延迟等待\n",
    "import random                # 随机数生成和随机采样\n",
    "import openai                # OpenAI API客户端，这里用于调用DeepSeek API\n",
    "from tqdm import tqdm        # 进度条显示库\n",
    "import concurrent.futures    # 并发处理库，用于多线程\n",
    "import threading             # 线程相关功能，用于线程锁\n",
    "\n",
    "# ==================== 配置参数 ====================\n",
    "API_KEY = \"sk-6uvkiohp89g6798e9eda\"  # DeepSeek API的密钥，用于身份验证\n",
    "INPUT_FILE = \"/root/autodl-tmp/data/filtered_financial_news_5k.jsonl\"    # 输入文件路径：过滤后的金融新闻数据\n",
    "OUTPUT_FILE = \"/root/autodl-tmp/data/sft/deepspeek_multi_turn_dataset.jsonl\" # 输出文件路径：生成的多轮对话SFT训练数据\n",
    "SAMPLE_COUNT = 10            # 需要采样的记录数量：从输入数据中选择10条进行处理（测试用小数量）\n",
    "MAX_WORKERS = 10             # 并行处理的最大线程数：同时运行10个线程来加速处理\n",
    "REQUEST_INTERVAL = 1         # 请求间隔（秒）：避免API调用过于频繁触发限制\n",
    "RANDOM_SEED = 57             # 随机种子：确保每次运行程序时随机采样的结果都相同\n",
    "MAX_TURNS = 3                # 最大对话轮数：每个对话最多包含3轮问答（新增参数）\n",
    "\n",
    "# ==================== 随机种子设置 ====================\n",
    "# 设置随机种子，确保程序的可重现性\n",
    "# 这里的随机种子不仅影响数据采样，还影响对话轮数的随机选择\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==================== 线程安全锁 ====================\n",
    "# 在多线程环境中，多个线程可能同时访问共享资源，导致数据混乱\n",
    "# 使用锁来确保同一时间只有一个线程能访问特定资源\n",
    "print_lock = threading.Lock()   # 用于保护打印输出，避免多个线程同时打印导致输出混乱\n",
    "output_lock = threading.Lock()  # 用于保护文件写入，避免多个线程同时写入同一文件导致数据损坏\n",
    "\n",
    "# ==================== API客户端初始化 ====================\n",
    "# 初始化OpenAI客户端，配置为使用DeepSeek的API服务\n",
    "client = openai.OpenAI(\n",
    "    api_key=API_KEY,                        # 使用上面定义的API密钥\n",
    "    base_url=\"https://api.deepseek.com\"     # DeepSeek的API服务地址\n",
    ")\n",
    "\n",
    "def truncate_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    截断文本以满足API长度限制\n",
    "    \n",
    "    参数:\n",
    "        text: 需要截断的文本\n",
    "        max_length: 最大允许长度，默认5000字符\n",
    "    \n",
    "    返回:\n",
    "        截断后的文本\n",
    "    \n",
    "    作用: API通常对输入文本长度有限制，过长的文本会导致请求失败\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text  # 如果文本长度在限制内，直接返回原文本\n",
    "    return text[:max_length]  # 如果超出限制，只返回前max_length个字符\n",
    "\n",
    "def generate_first_question(item):\n",
    "    \"\"\"\n",
    "    第一阶段：基于英文金融文章生成初始中文金融问题\n",
    "    \n",
    "    参数:\n",
    "        item: 包含文章内容的字典，应该有'Article'字段\n",
    "    \n",
    "    返回:\n",
    "        包含生成问题的字典，格式：{\"full_question\": \"问题内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这个函数与之前版本的区别：\n",
    "    - 函数名从generate_question改为generate_first_question，更明确表示这是多轮对话的第一轮\n",
    "    - 功能基本相同，但在多轮对话体系中扮演\"开场问题\"的角色\n",
    "    \"\"\"\n",
    "    # 从输入数据中提取文章内容\n",
    "    article = item.get('Article', '')  # 使用get方法安全获取，如果没有'Article'字段则返回空字符串\n",
    "    \n",
    "    # 截断文章以满足API长度限制\n",
    "    article_truncated = truncate_text(article)\n",
    "    \n",
    "    # 构建发送给AI的提示词（prompt）\n",
    "    # 这个提示词告诉AI如何处理文章并生成问题\n",
    "    prompt = f\"\"\"\n",
    "请基于以下英文金融文章，创建一个详细的中文金融问题。\n",
    "\n",
    "英文文章:\n",
    "{article_truncated}\n",
    "\n",
    "请完成以下任务:\n",
    "1. 从文章中提取关键信息、数据、事实和核心观点\n",
    "2. 创建一个针对这些关键信息的专业金融领域问题\n",
    "3. 问题必须包含足够详细的背景信息和事实，确保仅凭问题本身就能够推导出合理的回答\n",
    "\n",
    "输出格式必须是有效的JSON，结构如下:\n",
    "{{\n",
    "  \"full_question\": \"这里是包含详细背景信息的专业金融问题\"\n",
    "}}\n",
    "\n",
    "要求:\n",
    "- 问题必须具体且深入，能够引导出金融专业领域的分析\n",
    "- 必须包含足够丰富的事实信息，使第三方仅通过阅读问题就能回答\n",
    "- 禁止出现\"本文\"、\"文章\"、\"整体基调\"、\"情绪\"等字样\n",
    "- 禁止对文章本身进行评价或总结\n",
    "- 直接以陈述事实的方式提供背景信息\n",
    "- 问题应以客观的方式呈现数据和事实，避免主观评价\n",
    "- 问题要以自然、符合实际提问习惯的方式表达\n",
    "- 问题内容要特别详细，包含文章中所有能够支持回答问题的关键信息\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用DeepSeek API生成问题\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",  # 使用DeepSeek的聊天模型\n",
    "            messages=[\n",
    "                # 系统消息：定义AI的角色和任务\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融数据分析助手，精通英文金融文章翻译和问题构建。你的任务是创建包含充分背景信息的专业金融问题。\"},\n",
    "                # 用户消息：具体的任务指令\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,      # 不使用流式输出，一次性获取完整回复\n",
    "            temperature=0.7    # 控制输出的随机性，0.7表示适中的创造性\n",
    "        )\n",
    "        # 获取AI的回复内容\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # 从AI回复中提取JSON格式的数据\n",
    "        # AI的回复可能包含其他文本，需要找到JSON部分\n",
    "        json_start = result.find('{')        # 找到第一个'{'的位置\n",
    "        json_end = result.rfind('}') + 1     # 找到最后一个'}'的位置\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            # 提取JSON字符串\n",
    "            json_str = result[json_start:json_end]\n",
    "            # 解析JSON\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # 检查JSON是否包含必要的字段\n",
    "            if \"full_question\" in data:\n",
    "                return {\"full_question\": data[\"full_question\"]}\n",
    "            else:\n",
    "                print(\"警告: 返回的JSON缺少必要字段\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"无法在响应中找到有效的JSON\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 如果出现任何错误（网络错误、API错误、JSON解析错误等），打印错误信息\n",
    "        print(f\"生成问题时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_first_answer(question_data):\n",
    "    \"\"\"\n",
    "    生成第一轮回答\n",
    "    \n",
    "    参数:\n",
    "        question_data: 包含问题的字典，应该有'full_question'字段\n",
    "    \n",
    "    返回:\n",
    "        包含答案的字典，格式：{\"answer\": \"完整答案内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这个函数与之前版本的区别：\n",
    "    - 函数名从generate_answer改为generate_first_answer，明确表示这是第一轮回答\n",
    "    - 在提示词中增加了\"确保回答中保留1-2个可能的后续问题点\"，为后续对话做铺垫\n",
    "    \"\"\"\n",
    "    # 从输入数据中提取问题内容\n",
    "    full_question = question_data.get(\"full_question\", \"\")\n",
    "    \n",
    "    # 检查问题是否为空\n",
    "    if not full_question:\n",
    "        print(\"错误: 问题内容为空\")\n",
    "        return None\n",
    "    \n",
    "    # 构建用于生成答案的提示词\n",
    "    prompt = f\"\"\"\n",
    "请针对以下金融问题提供专业、全面的分析和回答。\n",
    "\n",
    "问题:\n",
    "{full_question}\n",
    "\n",
    "请仅基于问题中提供的信息进行回答，不要引入外部知识。你的回答必须包含两部分：\n",
    "1. 使用<think>标签包围的详细思考过程\n",
    "2. 最终的专业回答\n",
    "\n",
    "首先，使用<think>和</think>标签包围你的详细思考过程：\n",
    "<think>\n",
    "在这里，你需要进行非常详细的分析，包括以下几方面：\n",
    "1. 问题背景分析：分析问题中提供的关键信息和数据\n",
    "2. 数据解读：对问题中的数字、百分比等数据进行专业解读\n",
    "3. 原因探究：分析可能的原因和影响因素\n",
    "4. 多角度思考：从多个维度考虑问题\n",
    "5. 推理过程：清晰展示你的推理步骤和逻辑\n",
    "</think>\n",
    "\n",
    "然后，不使用任何标签，直接提供你的最终专业回答。\n",
    "\n",
    "回答要求:\n",
    "- 使用专业的金融术语和表达方式\n",
    "- 提供有价值的见解和结论\n",
    "- 清晰解释原因和影响\n",
    "- 仅基于问题中提供的信息进行回答，不要编造事实\n",
    "- 回答必须使用中文\n",
    "- 确保回答中保留1-2个可能的后续问题点，以便用户能够继续提问\n",
    "\n",
    "严格注意：\n",
    "- 思考过程必须详细，至少包含300字以上的分析\n",
    "- 最终回答必须放在</think>标签之后，不得包含在思考标签内\n",
    "- 思考过程和最终回答必须严格分开\n",
    "- 禁止在回答中再次使用<think>或</think>标签\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用DeepSeek API生成答案\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                # 系统消息：定义AI作为专业金融分析师的角色\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融分析师，擅长提供深入、全面的金融分析。你的回答必须包含详细的思考过程和最终结论。\"},\n",
    "                # 用户消息：具体的分析任务\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        # 获取AI生成的答案\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # 检查答案是否包含必要的思考过程标签\n",
    "        if \"<think>\" in answer and \"</think>\" in answer:\n",
    "            return {\"answer\": answer}\n",
    "        else:\n",
    "            # 如果AI没有按要求使用标签，尝试自动添加标签\n",
    "            # 假设前面部分是思考过程，后面部分是结论\n",
    "            thinking_end = answer.find(\"\\n\\n\")  # 寻找段落分隔符\n",
    "            if thinking_end != -1:\n",
    "                thinking = answer[:thinking_end]      # 前面部分作为思考过程\n",
    "                conclusion = answer[thinking_end+2:]  # 后面部分作为最终答案\n",
    "                # 重新格式化答案\n",
    "                formatted_answer = f\"<think>\\n{thinking}\\n</think>\\n\\n{conclusion}\"\n",
    "                return {\"answer\": formatted_answer}\n",
    "            else:\n",
    "                print(\"警告: 无法在回答中识别思考过程\")\n",
    "                return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"生成回答时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_follow_up_question(initial_question, previous_answer, turn_num):\n",
    "    \"\"\"\n",
    "    生成后续问题（全新功能）\n",
    "    \n",
    "    参数:\n",
    "        initial_question: 初始问题内容\n",
    "        previous_answer: 前一轮的回答内容\n",
    "        turn_num: 当前轮次编号\n",
    "    \n",
    "    返回:\n",
    "        包含后续问题的字典，格式：{\"follow_up_question\": \"问题内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这是多轮对话版本的核心新功能：\n",
    "    - 基于前一轮的回答内容生成自然的后续问题\n",
    "    - 确保问题的连贯性和相关性\n",
    "    - 模拟真实用户的提问习惯\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "你需要基于下面的初始问题和前一轮对话回答，生成一个自然且相关的后续问题。这是第{turn_num}轮对话。\n",
    "\n",
    "初始问题:\n",
    "{initial_question}\n",
    "\n",
    "前一轮回答:\n",
    "{extract_final_answer(previous_answer)}\n",
    "\n",
    "请生成一个合理的后续问题，满足以下要求:\n",
    "1. 问题必须是对前一轮回答中提到内容的深入探讨\n",
    "2. 应该选择前一轮回答中的某个观点或信息点进行追问\n",
    "3. 不要引入与话题无关的内容\n",
    "4. 问题必须具体且专业，避免过于宽泛的提问\n",
    "5. 问题应该自然，就像是真实用户看到上一轮回答后会提出的疑问\n",
    "\n",
    "输出格式必须是有效的JSON，结构如下:\n",
    "{{\n",
    "  \"follow_up_question\": \"这里是后续提问的内容\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用API生成后续问题\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融数据分析助手，你的任务是根据对话上下文生成自然的后续问题。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0.75  # 稍微提高一点温度，使后续问题更多样化\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        # 提取JSON格式的后续问题\n",
    "        json_start = result.find('{')\n",
    "        json_end = result.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = result[json_start:json_end]\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # 检查必要字段\n",
    "            if \"follow_up_question\" in data:\n",
    "                return {\"follow_up_question\": data[\"follow_up_question\"]}\n",
    "            else:\n",
    "                print(\"警告: 返回的JSON缺少必要字段\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"无法在响应中找到有效的JSON\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"生成后续问题时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_final_answer(answer_with_thinking):\n",
    "    \"\"\"\n",
    "    从带有思考过程的回答中提取最终答案部分（新增辅助函数）\n",
    "    \n",
    "    参数:\n",
    "        answer_with_thinking: 包含<think>标签的完整回答\n",
    "    \n",
    "    返回:\n",
    "        提取出的最终答案部分（不包含思考过程）\n",
    "    \n",
    "    作用:\n",
    "    - 在生成后续问题时，只需要前一轮的最终答案，不需要思考过程\n",
    "    - 避免提示词过长，提高API调用效率\n",
    "    \"\"\"\n",
    "    if \"<think>\" in answer_with_thinking and \"</think>\" in answer_with_thinking:\n",
    "        # 找到</think>标签的结束位置\n",
    "        end_of_thinking = answer_with_thinking.rfind(\"</think>\") + 8\n",
    "        # 提取标签后面的内容作为最终答案\n",
    "        final_answer = answer_with_thinking[end_of_thinking:].strip()\n",
    "        return final_answer\n",
    "    return answer_with_thinking  # 如果没有思考标签，返回原文本\n",
    "\n",
    "def generate_follow_up_answer(conversation_history, current_question, turn_num):\n",
    "    \"\"\"\n",
    "    生成后续回答（全新功能）\n",
    "    \n",
    "    参数:\n",
    "        conversation_history: 之前的对话历史，格式为[(问题1, 答案1), (问题2, 答案2), ...]\n",
    "        current_question: 当前需要回答的问题\n",
    "        turn_num: 当前轮次编号\n",
    "    \n",
    "    返回:\n",
    "        包含答案的字典，格式：{\"answer\": \"完整答案内容\"}\n",
    "        如果失败返回None\n",
    "    \n",
    "    这是多轮对话版本的核心新功能：\n",
    "    - 考虑整个对话历史来生成回答\n",
    "    - 确保回答的连贯性和一致性\n",
    "    - 避免重复之前已经说过的内容\n",
    "    \"\"\"\n",
    "    # 构建对话历史的文本表示\n",
    "    conversation_context = \"\"\n",
    "    for i, (q, a) in enumerate(conversation_history):\n",
    "        conversation_context += f\"第{i+1}轮问题: {q}\\n\"\n",
    "        # 只包含最终答案，不包含思考过程，避免提示词过长\n",
    "        conversation_context += f\"第{i+1}轮回答: {extract_final_answer(a)}\\n\\n\"\n",
    "    \n",
    "    # 构建生成后续回答的提示词\n",
    "    prompt = f\"\"\"\n",
    "请针对以下多轮对话中的最新问题提供专业、全面的金融分析和回答。这是第{turn_num}轮对话。\n",
    "\n",
    "对话历史:\n",
    "{conversation_context}\n",
    "\n",
    "当前问题 (第{turn_num}轮):\n",
    "{current_question}\n",
    "\n",
    "请仅基于对话历史和问题中提供的信息进行回答，不要引入外部知识。你的回答必须包含两部分：\n",
    "1. 使用<think>标签包围的详细思考过程\n",
    "2. 最终的专业回答\n",
    "\n",
    "首先，使用<think>和</think>标签包围你的详细思考过程：\n",
    "<think>\n",
    "在这里，你需要进行非常详细的分析，包括以下几方面：\n",
    "1. 对话历史分析：理解之前的问答内容和上下文\n",
    "2. 当前问题分析：分析当前问题的关键点和需求\n",
    "3. 数据解读：对涉及的数字、百分比等数据进行专业解读\n",
    "4. 原因探究：分析可能的原因和影响因素\n",
    "5. 多角度思考：从多个维度考虑问题\n",
    "6. 推理过程：清晰展示你的推理步骤和逻辑\n",
    "</think>\n",
    "\n",
    "然后，不使用任何标签，直接提供你的最终专业回答。\n",
    "\n",
    "回答要求:\n",
    "- 使用专业的金融术语和表达方式\n",
    "- 提供有价值的见解和结论\n",
    "- 清晰解释原因和影响\n",
    "- 仅基于对话历史和当前问题中提供的信息进行回答，不要编造事实\n",
    "- 回答必须使用中文\n",
    "- 确保回答是对当前问题的直接回应，同时也考虑对话的连贯性\n",
    "\n",
    "严格注意：\n",
    "- 思考过程必须详细，至少包含300字以上的分析\n",
    "- 最终回答必须放在</think>标签之后，不得包含在思考标签内\n",
    "- 思考过程和最终回答必须严格分开\n",
    "- 禁止在回答中再次使用<think>或</think>标签\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用API生成后续回答\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融分析师，擅长提供深入、全面的金融分析。你的回答必须包含详细的思考过程和最终结论。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # 检查是否包含思考过程标签\n",
    "        if \"<think>\" in answer and \"</think>\" in answer:\n",
    "            return {\"answer\": answer}\n",
    "        else:\n",
    "            # 如果没有标签，尝试添加标签\n",
    "            thinking_end = answer.find(\"\\n\\n\")\n",
    "            if thinking_end != -1:\n",
    "                thinking = answer[:thinking_end]\n",
    "                conclusion = answer[thinking_end+2:]\n",
    "                formatted_answer = f\"<think>\\n{thinking}\\n</think>\\n\\n{conclusion}\"\n",
    "                return {\"answer\": formatted_answer}\n",
    "            else:\n",
    "                print(\"警告: 无法在回答中识别思考过程\")\n",
    "                return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"生成回答时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_multi_turn_sft_data(item, index, total):\n",
    "    \"\"\"\n",
    "    多阶段处理：生成多轮对话SFT数据（核心功能重构）\n",
    "    \n",
    "    参数:\n",
    "        item: 原始文章数据\n",
    "        index: 当前处理的文章索引（用于显示进度）\n",
    "        total: 总文章数量\n",
    "    \n",
    "    返回:\n",
    "        多轮对话格式的SFT训练数据字典，包含messages数组和turns计数\n",
    "        如果失败返回None\n",
    "    \n",
    "    这个函数与之前版本的主要区别：\n",
    "    1. 函数名从create_sft_data改为create_multi_turn_sft_data\n",
    "    2. 不再生成单轮问答，而是生成1-3轮的多轮对话\n",
    "    3. 输出格式从instruction/input/output改为messages数组格式\n",
    "    4. 增加了对话轮数的随机性和统计\n",
    "    \"\"\"\n",
    "    with print_lock:\n",
    "        print(f\"处理文章 {index+1}/{total}\")\n",
    "    \n",
    "    # 第一阶段：生成初始问题\n",
    "    question_data = generate_first_question(item)\n",
    "    if not question_data:\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 初始问题生成失败\")\n",
    "        return None\n",
    "    \n",
    "    first_question = question_data[\"full_question\"]\n",
    "    \n",
    "    # 添加随机延迟，避免API请求过于集中\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 第二阶段：生成第一轮回答\n",
    "    answer_data = generate_first_answer(question_data)\n",
    "    if not answer_data:\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 第一轮回答生成失败\")\n",
    "        return None\n",
    "    \n",
    "    first_answer = answer_data[\"answer\"]\n",
    "    \n",
    "    # 确定实际对话轮数，随机1-3轮\n",
    "    # 这里使用随机数决定对话长度，增加数据的多样性\n",
    "    actual_turns = random.randint(1, MAX_TURNS)\n",
    "    \n",
    "    # 存储对话历史，格式为[(问题, 答案), (问题, 答案), ...]\n",
    "    conversation = [(first_question, first_answer)]\n",
    "    \n",
    "    # 如果需要多轮对话，继续生成后续问答\n",
    "    current_turn = 1\n",
    "    while current_turn < actual_turns:\n",
    "        current_turn += 1\n",
    "        \n",
    "        # 添加随机延迟\n",
    "        time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "        \n",
    "        # 生成后续问题\n",
    "        follow_up_question_data = generate_follow_up_question(\n",
    "            first_question,         # 初始问题，用于保持话题一致性\n",
    "            conversation[-1][1],    # 上一轮回答，用于生成相关的后续问题\n",
    "            current_turn            # 当前轮次\n",
    "        )\n",
    "        \n",
    "        if not follow_up_question_data:\n",
    "            with print_lock:\n",
    "                print(f\"文章 {index+1}/{total}: 第{current_turn}轮问题生成失败\")\n",
    "            break  # 如果生成失败，结束对话\n",
    "        \n",
    "        follow_up_question = follow_up_question_data[\"follow_up_question\"]\n",
    "        \n",
    "        # 添加随机延迟\n",
    "        time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "        \n",
    "        # 生成对应回答\n",
    "        follow_up_answer_data = generate_follow_up_answer(\n",
    "            conversation,       # 之前的对话历史，确保回答的连贯性\n",
    "            follow_up_question, # 当前问题\n",
    "            current_turn        # 当前轮次\n",
    "        )\n",
    "        \n",
    "        if not follow_up_answer_data:\n",
    "            with print_lock:\n",
    "                print(f\"文章 {index+1}/{total}: 第{current_turn}轮回答生成失败\")\n",
    "            break  # 如果生成失败，结束对话\n",
    "        \n",
    "        follow_up_answer = follow_up_answer_data[\"answer\"]\n",
    "        \n",
    "        # 添加到对话历史\n",
    "        conversation.append((follow_up_question, follow_up_answer))\n",
    "    \n",
    "    # 构建最终的SFT数据\n",
    "    # 使用messages格式，这是多轮对话训练的标准格式\n",
    "    messages = []\n",
    "    \n",
    "    # 添加系统消息，定义AI的角色\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一个专业的金融分析师，擅长提供深入、全面的金融分析。你的回答必须包含详细的思考过程和最终结论。\"\n",
    "    })\n",
    "    \n",
    "    # 添加对话内容，交替添加用户问题和助手回答\n",
    "    for question, answer in conversation:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",        # 用户角色\n",
    "            \"content\": question    # 问题内容\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",   # 助手角色\n",
    "            \"content\": answer      # 回答内容（包含思考过程）\n",
    "        })\n",
    "    \n",
    "    # 构建最终的SFT数据\n",
    "    sft_data = {\n",
    "        \"messages\": messages,           # 对话消息数组\n",
    "        \"turns\": len(conversation)      # 记录实际对话轮数，用于后续统计\n",
    "    }\n",
    "    \n",
    "    return sft_data\n",
    "\n",
    "def process_article(args):\n",
    "    \"\"\"\n",
    "    处理单篇文章的包装函数，用于并行处理\n",
    "    \n",
    "    参数:\n",
    "        args: 包含(item, index, total, output_file)的元组\n",
    "    \n",
    "    返回:\n",
    "        True表示处理成功，False表示处理失败\n",
    "    \n",
    "    这个函数与之前版本基本相同，只是调用的核心函数改为create_multi_turn_sft_data\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    item, index, total, output_file = args\n",
    "    \n",
    "    # 添加随机延迟，避免所有线程同时发起API请求\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 处理文章生成多轮对话SFT数据\n",
    "    result = create_multi_turn_sft_data(item, index, total)\n",
    "    if result:\n",
    "        # 使用文件写入锁，确保多个线程不会同时写入同一文件\n",
    "        with output_lock:\n",
    "            # 以追加模式打开文件，将结果写入\n",
    "            with open(output_file, 'a', encoding='utf-8') as out_f:\n",
    "                # 将结果转换为JSON格式并写入文件，每行一个JSON对象\n",
    "                out_f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "        return True  # 返回成功标志\n",
    "    return False     # 返回失败标志\n",
    "\n",
    "def stratified_random_sample(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    分层随机采样 - 结合均匀采样的覆盖性和随机采样的随机性\n",
    "    \n",
    "    这个函数与之前版本完全相同，没有任何变化\n",
    "    \"\"\"\n",
    "    total_count = len(data_list)\n",
    "    \n",
    "    if total_count <= sample_count:\n",
    "        print(f\"数据总量({total_count})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 计算需要划分的区间数量\n",
    "    # 如果样本数量太小，至少划分为样本数量的区间\n",
    "    num_strata = min(sample_count, total_count // 10 + 1)\n",
    "    \n",
    "    # 计算每个区间的大小\n",
    "    stratum_size = total_count // num_strata\n",
    "    \n",
    "    sampled_items = []  # 存储采样结果\n",
    "    for i in range(num_strata):\n",
    "        # 计算当前区间的起止索引\n",
    "        start_idx = i * stratum_size\n",
    "        end_idx = start_idx + stratum_size if i < num_strata - 1 else total_count\n",
    "        \n",
    "        # 当前区间的数据\n",
    "        stratum_data = data_list[start_idx:end_idx]\n",
    "        \n",
    "        # 计算当前区间需要采样的数量\n",
    "        # 按比例分配采样数量\n",
    "        stratum_sample_count = max(1, int((end_idx - start_idx) / total_count * sample_count))\n",
    "        \n",
    "        # 确保总采样数不超过要求\n",
    "        if len(sampled_items) + stratum_sample_count > sample_count:\n",
    "            stratum_sample_count = sample_count - len(sampled_items)\n",
    "        \n",
    "        # 如果区间样本数少于要求采样数，全部选择\n",
    "        if len(stratum_data) <= stratum_sample_count:\n",
    "            sampled_items.extend(stratum_data)\n",
    "        else:\n",
    "            # 从当前区间随机采样\n",
    "            stratum_samples = random.sample(stratum_data, stratum_sample_count)\n",
    "            sampled_items.extend(stratum_samples)\n",
    "        \n",
    "        # 如果已经达到所需样本数，结束循环\n",
    "        if len(sampled_items) >= sample_count:\n",
    "            break\n",
    "    \n",
    "    # 处理边界情况：最终样本数小于要求数\n",
    "    if len(sampled_items) < sample_count:\n",
    "        # 从未选择的数据中随机补充\n",
    "        remaining = [item for item in data_list if item not in sampled_items]\n",
    "        additional = random.sample(remaining, sample_count - len(sampled_items))\n",
    "        sampled_items.extend(additional)\n",
    "    \n",
    "    return sampled_items\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：协调整个多轮对话数据处理流程\n",
    "    \n",
    "    主要步骤：\n",
    "    1. 检查输入文件\n",
    "    2. 读取和解析数据\n",
    "    3. 采样数据\n",
    "    4. 并行处理生成多轮对话SFT数据\n",
    "    5. 统计处理结果和对话轮数分布（新增功能）\n",
    "    \"\"\"\n",
    "    # 确保输出目录存在\n",
    "    # os.path.dirname()获取文件路径的目录部分\n",
    "    # exist_ok=True表示如果目录已存在不会报错\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"错误: 输入文件 '{INPUT_FILE}' 不存在!\")\n",
    "        return\n",
    "    \n",
    "    # 检查文件大小，确保文件不为空\n",
    "    file_size = os.path.getsize(INPUT_FILE)\n",
    "    print(f\"输入文件大小: {file_size} 字节\")\n",
    "    if file_size == 0:\n",
    "        print(\"错误: 输入文件为空!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"使用随机种子: {RANDOM_SEED} 确保可复现性\")\n",
    "    print(f\"最大对话轮数: {MAX_TURNS}\")  # 新增：显示最大对话轮数配置\n",
    "    \n",
    "    # ==================== 读取和解析输入文件 ====================\n",
    "    # 这部分代码与之前版本完全相同\n",
    "    items = []          # 存储有效的数据记录\n",
    "    line_count = 0      # 总行数计数器\n",
    "    error_count = 0     # 错误行数计数器\n",
    "    valid_count = 0     # 有效记录计数器\n",
    "    \n",
    "    try:\n",
    "        # 逐行读取JSONL文件（每行一个JSON对象）\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):  # enumerate从1开始计数\n",
    "                line_count += 1\n",
    "                line = line.strip()  # 去除行首行尾的空白字符\n",
    "                if not line:         # 跳过空行\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # 尝试解析JSON\n",
    "                    item = json.loads(line)\n",
    "                    \n",
    "                    # 检查必要的字段是否存在\n",
    "                    # 支持两种字段名格式：'Article'/'article' 和 'Summary'/'summary'\n",
    "                    if 'Article' in item:\n",
    "                        if 'Summary' in item:\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'Summary'字段\")\n",
    "                    elif 'article' in item:\n",
    "                        if 'summary' in item:\n",
    "                            # 标准化字段名，统一使用大写开头\n",
    "                            item['Article'] = item['article']\n",
    "                            item['Summary'] = item['summary']\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'summary'字段\")\n",
    "                    else:\n",
    "                        print(f\"警告: 第 {line_num} 行没有'Article'或'article'字段\")\n",
    "                        print(f\"可用字段: {list(item.keys())}\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    # JSON解析失败\n",
    "                    error_count += 1\n",
    "                    print(f\"错误: 第 {line_num} 行JSON解析失败: {e}\")\n",
    "                    print(f\"问题行内容: {line[:100]}...\")  # 只显示前100个字符\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "    \n",
    "    # 打印文件读取统计信息\n",
    "    print(f\"文件共有 {line_count} 行\")\n",
    "    print(f\"解析错误: {error_count} 行\")\n",
    "    print(f\"成功读取: {valid_count} 条有效记录\")\n",
    "    print(f\"最终收集: {len(items)} 条记录\")\n",
    "    \n",
    "    # 如果没有读取到任何有效数据，显示调试信息\n",
    "    if len(items) == 0:\n",
    "        print(\"\\n尝试显示文件前5行内容进行调试:\")\n",
    "        try:\n",
    "            with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= 5:  # 只显示前5行\n",
    "                        break\n",
    "                    print(f\"第 {i+1} 行: {line.strip()[:200]}...\")  # 显示前200个字符\n",
    "                    try:\n",
    "                        data = json.loads(line.strip())\n",
    "                        print(f\"JSON键: {list(data.keys())}\")  # 显示JSON的所有键\n",
    "                    except:\n",
    "                        pass  # 忽略JSON解析错误\n",
    "        except Exception as e:\n",
    "            print(f\"显示文件内容时发生错误: {e}\")\n",
    "        return  # 没有数据就退出程序\n",
    "    \n",
    "    # ==================== 数据采样 ====================\n",
    "    # 使用分层随机采样\n",
    "    sampled_items = stratified_random_sample(items, SAMPLE_COUNT)\n",
    "    print(f\"分层随机采样了 {len(sampled_items)}/{len(items)} 条记录\")\n",
    "    \n",
    "    # 创建或清空输出文件\n",
    "    # 'w'模式会清空文件内容，确保输出文件是干净的\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        pass  # 只是创建/清空文件，不写入任何内容\n",
    "    \n",
    "    # ==================== 并行处理 ====================\n",
    "    print(f\"开始并行处理，最大线程数: {MAX_WORKERS}\")\n",
    "    \n",
    "    # 准备传递给每个线程的参数\n",
    "    # 每个元组包含：(文章数据, 索引, 总数, 输出文件路径)\n",
    "    args_list = [(item, i, len(sampled_items), OUTPUT_FILE) for i, item in enumerate(sampled_items)]\n",
    "    \n",
    "    # 使用线程池并行处理所有文章\n",
    "    success_count = 0  # 成功处理的文章数量\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # executor.map会将args_list中的每个元素传递给process_article函数\n",
    "        # 并在多个线程中并行执行\n",
    "        results = list(executor.map(process_article, args_list))\n",
    "        # 统计成功处理的数量（process_article返回True表示成功）\n",
    "        success_count = sum(1 for r in results if r)\n",
    "    \n",
    "    # 打印最终处理结果\n",
    "    print(f\"处理完成，成功处理 {success_count}/{len(sampled_items)} 条记录\")\n",
    "    print(f\"结果已保存至 {OUTPUT_FILE}\")\n",
    "    \n",
    "    # ==================== 统计对话轮数分布（新增功能） ====================\n",
    "    # 这是多轮对话版本特有的统计功能\n",
    "    try:\n",
    "        turn_distribution = {1: 0, 2: 0, 3: 0}  # 初始化轮数统计字典\n",
    "        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    turns = data.get(\"turns\", 0)  # 获取对话轮数\n",
    "                    if turns in turn_distribution:\n",
    "                        turn_distribution[turns] += 1\n",
    "                    else:\n",
    "                        turn_distribution[turns] = 1\n",
    "                except:\n",
    "                    pass  # 忽略解析错误\n",
    "        \n",
    "        # 打印对话轮数分布统计\n",
    "        print(\"对话轮数分布:\")\n",
    "        for turns, count in sorted(turn_distribution.items()):\n",
    "            percentage = count/success_count*100 if success_count > 0 else 0\n",
    "            print(f\"{turns}轮对话: {count}条 ({percentage:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"统计对话轮数分布时出错: {e}\")\n",
    "\n",
    "# ==================== 程序入口 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # 运行主函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9d41a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526202311763.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff62f7",
   "metadata": {},
   "source": [
    "## 4.使用qwen3进行问题生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3f7c2",
   "metadata": {},
   "source": [
    "\n",
    "###  两阶段处理流程\n",
    "1. **问题生成阶段**：基于英文金融新闻生成包含详细背景的中文专业问题\n",
    "2. **答案生成阶段**：针对生成的问题创建带有思考过程的专业回答\n",
    "\n",
    "###  核心特性\n",
    "- **并行处理**：使用多线程提高处理效率\n",
    "- **错误处理**：完善的异常处理和格式修复机制\n",
    "- **数据采样**：支持均匀采样，避免处理全部数据\n",
    "- **API限制保护**：通过随机延迟避免触发API频率限制\n",
    "\n",
    "### 输出格式\n",
    "生成标准的SFT训练格式，包含：\n",
    "- `instruction`：固定的任务指令\n",
    "- `input`：生成的专业金融问题\n",
    "- `output`：包含思考过程的专业回答\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b07f39-0330-4ed1-917d-585b5266299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文件大小: 278687859 字节\n",
      "文件共有 70061 行\n",
      "解析错误: 0 行\n",
      "成功读取: 70061 条有效记录\n",
      "最终收集: 70061 条记录\n",
      "均匀采样了 10/70061 条记录\n",
      "开始并行处理，最大线程数: 10\n",
      "处理文章 4/10\n",
      "处理文章 8/10\n",
      "处理文章 10/10\n",
      "处理文章 5/10\n",
      "处理文章 2/10\n",
      "处理文章 9/10\n",
      "处理文章 3/10\n",
      "处理文章 1/10\n",
      "处理文章 6/10\n",
      "处理文章 7/10\n",
      "处理完成，成功处理 10/10 条记录\n",
      "结果已保存至 /root/autodl-tmp/data/sft/qwen3_sft_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os          # 用于操作系统相关功能，如文件路径操作\n",
    "import json        # 用于处理JSON格式数据的读取和写入\n",
    "import time        # 用于时间相关操作，如添加延迟\n",
    "import random      # 用于生成随机数，避免请求过于集中\n",
    "import openai      # OpenAI的Python客户端库，用于调用AI模型API\n",
    "from tqdm import tqdm  # 用于显示进度条（虽然在这个脚本中没有直接使用）\n",
    "import concurrent.futures  # 用于并行处理，提高处理效率\n",
    "import threading   # 用于多线程编程，实现并发处理\n",
    "\n",
    "# ==================== 配置参数部分 ====================\n",
    "# 这些参数控制整个程序的行为，可以根据需要修改\n",
    "\n",
    "API_KEY = \"sk2-6uvk3hui0270296skpsi3-b5\"  # 通义千问API的访问密钥，用于身份验证\n",
    "INPUT_FILE = \"/root/autodl-tmp/data/filtered_financial_news_5k.jsonl\"  # 输入文件路径，包含原始金融新闻数据\n",
    "OUTPUT_FILE = \"/root/autodl-tmp/data/sft/qwen3_sft_dataset.jsonl\"  # 输出文件路径，保存生成的训练数据\n",
    "SAMPLE_COUNT = 10  # 从输入文件中采样的记录数量，控制处理的数据量\n",
    "MAX_WORKERS = 10  # 并行处理的最大线程数，影响处理速度和资源消耗\n",
    "REQUEST_INTERVAL = 1  # 请求间隔时间（秒），防止API调用过于频繁被限制\n",
    "\n",
    "# ==================== 线程安全锁 ====================\n",
    "# 在多线程环境中，这些锁确保多个线程不会同时访问共享资源，避免数据冲突\n",
    "print_lock = threading.Lock()    # 控制打印输出的锁，确保打印信息不会混乱\n",
    "output_lock = threading.Lock()   # 控制文件写入的锁，确保多个线程不会同时写入文件\n",
    "\n",
    "# ==================== 初始化AI客户端 ====================\n",
    "# 创建OpenAI客户端实例，配置为使用通义千问的兼容接口\n",
    "client = openai.OpenAI(\n",
    "    api_key=API_KEY,  # 使用上面定义的API密钥\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 通义千问的API端点地址\n",
    ")\n",
    "\n",
    "def truncate_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    文本截断函数：限制文本长度以满足API的输入限制\n",
    "    \n",
    "    参数说明：\n",
    "    - text: 需要截断的原始文本\n",
    "    - max_length: 最大允许长度，默认5000字符\n",
    "    \n",
    "    返回值：\n",
    "    - 如果文本长度小于等于最大长度，返回原文本\n",
    "    - 如果超过最大长度，返回截断后的文本\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text  # 文本长度合适，直接返回\n",
    "    return text[:max_length]  # 文本过长，截取前max_length个字符\n",
    "\n",
    "\n",
    "def generate_question(item):\n",
    "    \"\"\"\n",
    "    第一阶段：基于金融新闻文章生成专业问题\n",
    "    \n",
    "    这个函数的作用是：\n",
    "    1. 从输入的新闻文章中提取关键信息\n",
    "    2. 使用AI模型生成一个包含详细背景信息的专业金融问题\n",
    "    3. 确保生成的问题包含足够的信息，让第三方仅凭问题就能回答\n",
    "    \n",
    "    参数说明：\n",
    "    - item: 包含新闻文章数据的字典，必须包含'Article'字段\n",
    "    \n",
    "    返回值：\n",
    "    - 成功：返回包含生成问题的字典 {\"full_question\": \"问题内容\"}\n",
    "    - 失败：返回None\n",
    "    \"\"\"\n",
    "    # 从输入数据中获取文章内容，如果没有'Article'字段则返回空字符串\n",
    "    article = item.get('Article', '')\n",
    "    \n",
    "    # 截断文章内容，确保不超过API的输入限制\n",
    "    article_truncated = truncate_text(article)\n",
    "    \n",
    "    # 构建发送给AI模型的提示词（prompt）\n",
    "    # 这个提示词详细说明了我们希望AI模型完成的任务\n",
    "    prompt = f\"\"\"\n",
    "请基于以下英文金融文章，创建一个详细的中文金融问题。\n",
    "\n",
    "英文文章:\n",
    "{article_truncated}\n",
    "\n",
    "请完成以下任务:\n",
    "1. 从文章中提取关键信息、数据、事实和核心观点\n",
    "2. 创建一个针对这些关键信息的专业金融领域问题\n",
    "3. 问题必须包含足够详细的背景信息和事实，确保仅凭问题本身就能够推导出合理的回答\n",
    "\n",
    "输出格式必须是有效的JSON，结构如下:\n",
    "{{\n",
    "  \"full_question\": \"这里是包含详细背景信息的专业金融问题\"\n",
    "}}\n",
    "\n",
    "要求:\n",
    "- 问题必须具体且深入，能够引导出金融专业领域的分析\n",
    "- 必须包含足够丰富的事实信息，使第三方仅通过阅读问题就能回答\n",
    "- 禁止出现\"本文\"、\"文章\"、\"整体基调\"、\"情绪\"等字样\n",
    "- 禁止对文章本身进行评价或总结\n",
    "- 直接以陈述事实的方式提供背景信息\n",
    "- 问题应以客观的方式呈现数据和事实，避免主观评价\n",
    "- 问题要以自然、符合实际提问习惯的方式表达\n",
    "- 问题内容要特别详细，包含文章中所有能够支持回答问题的关键信息\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用AI模型生成问题\n",
    "        # 使用流式输出（stream=True）可以实时获取生成内容\n",
    "        response_content = \"\"  # 用于存储完整的响应内容\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"qwen3-32b\",  # 指定使用的AI模型\n",
    "            messages=[\n",
    "                # 系统消息：定义AI助手的角色和任务\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融数据分析助手，精通英文金融文章翻译和问题构建。你的任务是创建包含充分背景信息的专业金融问题。\"},\n",
    "                # 用户消息：包含具体的任务指令\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=True,  # 启用流式输出，可以实时接收生成的内容\n",
    "            temperature=0.7,  # 控制生成内容的随机性，0.7表示适中的创造性\n",
    "            extra_body={\"enable_thinking\": True}  # 启用思考模式，让模型进行更深入的思考\n",
    "        )\n",
    "        \n",
    "        # 收集流式响应的所有内容片段\n",
    "        for chunk in response:\n",
    "            # 检查每个响应片段是否包含内容\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                response_content += chunk.choices[0].delta.content\n",
    "        \n",
    "        # 从响应中提取JSON格式的数据\n",
    "        json_start = response_content.find('{')  # 找到第一个左大括号的位置\n",
    "        json_end = response_content.rfind('}') + 1  # 找到最后一个右大括号的位置\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            # 提取JSON字符串\n",
    "            json_str = response_content[json_start:json_end]\n",
    "            data = json.loads(json_str)  # 解析JSON字符串为Python字典\n",
    "            \n",
    "            # 检查返回的数据是否包含必要的字段\n",
    "            if \"full_question\" in data:\n",
    "                return {\"full_question\": data[\"full_question\"]}\n",
    "            else:\n",
    "                print(\"警告: 返回的JSON缺少必要字段\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"无法在响应中找到有效的JSON\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 捕获并处理任何可能出现的错误\n",
    "        print(f\"生成问题时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_answer(question_data):\n",
    "    \"\"\"\n",
    "    第二阶段：基于生成的问题创建带有思考过程的专业答案\n",
    "    \n",
    "    这个函数的作用是：\n",
    "    1. 接收第一阶段生成的问题\n",
    "    2. 使用AI模型生成包含思考过程的专业回答\n",
    "    3. 确保回答格式正确，包含<think>标签内的思考过程和最终答案\n",
    "    \n",
    "    参数说明：\n",
    "    - question_data: 包含问题内容的字典，必须包含'full_question'字段\n",
    "    \n",
    "    返回值：\n",
    "    - 成功：返回包含答案的字典 {\"answer\": \"完整答案内容\"}\n",
    "    - 失败：返回None\n",
    "    \"\"\"\n",
    "    # 从输入数据中获取问题内容\n",
    "    full_question = question_data.get(\"full_question\", \"\")\n",
    "    \n",
    "    # 检查问题内容是否为空\n",
    "    if not full_question:\n",
    "        print(\"错误: 问题内容为空\")\n",
    "        return None\n",
    "    \n",
    "    # 构建用于生成答案的提示词\n",
    "    prompt = f\"\"\"\n",
    "请针对以下金融问题提供专业、全面的分析和回答。\n",
    "\n",
    "问题:\n",
    "{full_question}\n",
    "\n",
    "请仅基于问题中提供的信息进行回答，不要引入外部知识。你的回答必须严格按照以下两部分格式:\n",
    "\n",
    "第一部分是你的思考过程，必须用<think>和</think>标签包围:\n",
    "<think>\n",
    "这里是你的详细思考过程，包括对问题的分析、关键信息提取、推理步骤等\n",
    "</think>\n",
    "\n",
    "第二部分是你的最终专业回答（不要重复使用<think>标签）:\n",
    "这里是你的最终专业回答，不需要任何标签包围\n",
    "\n",
    "回答要求:\n",
    "- 思考过程必须详细展示你的分析思路和推理过程\n",
    "- 分析必须专业、全面且深入\n",
    "- 使用专业的金融术语和表达方式\n",
    "- 提供有价值的见解和结论\n",
    "- 清晰解释原因和影响\n",
    "- 仅基于问题中提供的信息进行回答，不要编造事实\n",
    "- 回答必须使用中文\n",
    "- 严格遵守格式：先是<think>标签内的思考过程，然后是不带标签的最终回答\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # 调用AI模型生成答案\n",
    "        response_content = \"\"  # 存储完整响应内容\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"qwen3-32b\",  # 使用相同的AI模型保持一致性\n",
    "            messages=[\n",
    "                # 系统消息：强调回答格式要求\n",
    "                {\"role\": \"system\", \"content\": \"你是一个专业的金融分析师。你的回答必须先提供思考过程（用<think>标签包围），然后提供最终答案（不带标签）。\"},\n",
    "                # 用户消息：包含问题和详细要求\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            stream=True,  # 启用流式输出\n",
    "            temperature=0.7,  # 保持适中的创造性\n",
    "            extra_body={\"enable_thinking\": True}  # 启用思考模式\n",
    "        )\n",
    "        \n",
    "        # 收集所有响应内容\n",
    "        for chunk in response:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                response_content += chunk.choices[0].delta.content\n",
    "        \n",
    "        # 修复可能的格式问题，确保思考标签格式正确\n",
    "        cleaned_response = fix_thinking_format(response_content)\n",
    "        \n",
    "        return {\"answer\": cleaned_response}\n",
    "            \n",
    "    except Exception as e:\n",
    "        # 处理生成答案过程中的错误\n",
    "        print(f\"生成回答时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def fix_thinking_format(text):\n",
    "    \"\"\"\n",
    "    修复思考过程的格式问题\n",
    "    \n",
    "    这个函数的作用是：\n",
    "    1. 确保回答中只有一对<think>标签\n",
    "    2. 确保最终答案在标签外部\n",
    "    3. 处理各种可能的格式错误\n",
    "    \n",
    "    参数说明：\n",
    "    - text: 需要修复格式的原始文本\n",
    "    \n",
    "    返回值：\n",
    "    - 格式正确的文本，包含思考过程和最终答案\n",
    "    \"\"\"\n",
    "    # 检查文本中是否包含思考标签\n",
    "    if \"<think>\" not in text or \"</think>\" not in text:\n",
    "        # 如果没有标签，尝试自动添加标签\n",
    "        parts = text.split(\"\\n\\n\", 1)  # 按双换行符分割文本\n",
    "        if len(parts) > 1:\n",
    "            # 假设第一部分是思考过程，第二部分是最终答案\n",
    "            return f\"<think>\\n{parts[0]}\\n</think>\\n\\n{parts[1]}\"\n",
    "        else:\n",
    "            # 如果无法分割，按文本长度的一半分割\n",
    "            half_point = len(text) // 2\n",
    "            thinking = text[:half_point]\n",
    "            answer = text[half_point:]\n",
    "            return f\"<think>\\n{thinking}\\n</think>\\n\\n{answer}\"\n",
    "    \n",
    "    # 如果有多个思考标签，只保留第一对\n",
    "    first_think_start = text.find(\"<think>\")  # 找到第一个开始标签\n",
    "    first_think_end = text.find(\"</think>\", first_think_start) + 8  # 找到对应的结束标签（8是\"</think>\"的长度）\n",
    "    \n",
    "    if first_think_start != -1 and first_think_end != -1:\n",
    "        # 提取思考部分\n",
    "        thinking_part = text[first_think_start:first_think_end]\n",
    "        \n",
    "        # 查找最终答案部分（在思考标签之后的内容）\n",
    "        remaining_text = text[first_think_end:].strip()\n",
    "        \n",
    "        # 清理剩余文本中可能存在的其他思考标签\n",
    "        remaining_text = remaining_text.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "        \n",
    "        # 如果剩余文本为空，可能答案被错误地包含在思考标签内\n",
    "        if not remaining_text:\n",
    "            # 提取思考标签内的内容\n",
    "            think_content = text[first_think_start + 7:first_think_end - 8].strip()\n",
    "            \n",
    "            # 尝试基于关键词分割思考和答案\n",
    "            if \"最终的专业回答\" in think_content:\n",
    "                parts = think_content.split(\"最终的专业回答\", 1)\n",
    "                thinking = parts[0].strip()\n",
    "                answer = \"最终的专业回答\" + parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                return f\"<think>\\n{thinking}\\n</think>\\n\\n{answer}\"\n",
    "            else:\n",
    "                # 如果无法找到明确分隔，按段落分割\n",
    "                parts = think_content.split(\"\\n\\n\")\n",
    "                if len(parts) > 1:\n",
    "                    # 假设前半部分是思考，后半部分是答案\n",
    "                    half_index = len(parts) // 2\n",
    "                    thinking = \"\\n\\n\".join(parts[:half_index])\n",
    "                    answer = \"\\n\\n\".join(parts[half_index:])\n",
    "                    return f\"<think>\\n{thinking}\\n</think>\\n\\n{answer}\"\n",
    "        \n",
    "        # 正常情况：思考标签后有内容\n",
    "        return f\"{thinking_part}\\n\\n{remaining_text}\"\n",
    "    \n",
    "    return text  # 如果无法修复，返回原始文本 \n",
    "\n",
    "def create_sft_data(item, index, total):\n",
    "    \"\"\"\n",
    "    创建监督微调（SFT）格式的训练数据\n",
    "    \n",
    "    这个函数是整个数据处理流程的核心，它：\n",
    "    1. 调用generate_question函数生成问题\n",
    "    2. 调用generate_answer函数生成答案\n",
    "    3. 将问题和答案组合成标准的SFT训练格式\n",
    "    \n",
    "    参数说明：\n",
    "    - item: 包含原始新闻文章的数据字典\n",
    "    - index: 当前处理的文章索引（用于显示进度）\n",
    "    - total: 总文章数量（用于显示进度）\n",
    "    \n",
    "    返回值：\n",
    "    - 成功：返回SFT格式的数据字典\n",
    "    - 失败：返回None\n",
    "    \"\"\"\n",
    "    # 使用线程锁确保打印信息不会混乱\n",
    "    with print_lock:\n",
    "        print(f\"处理文章 {index+1}/{total}\")\n",
    "    \n",
    "    # 第一阶段：生成问题\n",
    "    question_data = generate_question(item)\n",
    "    if not question_data:\n",
    "        # 如果问题生成失败，记录错误并返回None\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 问题生成失败\")\n",
    "        return None\n",
    "    \n",
    "    # 添加随机延迟，避免API请求过于集中\n",
    "    # 这有助于防止触发API的频率限制\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 第二阶段：生成答案\n",
    "    answer_data = generate_answer(question_data)\n",
    "    if not answer_data:\n",
    "        # 如果答案生成失败，记录错误并返回None\n",
    "        with print_lock:\n",
    "            print(f\"文章 {index+1}/{total}: 答案生成失败\")\n",
    "        return None\n",
    "    \n",
    "    # 将问题和答案合并为标准的SFT训练格式\n",
    "    # SFT格式包含三个字段：instruction（指令）、input（输入）、output（输出）\n",
    "    sft_data = {\n",
    "        \"instruction\": \"以下是一个关于金融领域的问题，请提供详细的分析和见解。\",  # 固定的指令模板\n",
    "        \"input\": question_data[\"full_question\"],  # 生成的问题作为输入\n",
    "        \"output\": answer_data[\"answer\"]  # 生成的答案作为期望输出\n",
    "    }\n",
    "    \n",
    "    return sft_data\n",
    "\n",
    "def process_article(args):\n",
    "    \"\"\"\n",
    "    处理单篇文章的包装函数，专门用于并行处理\n",
    "    \n",
    "    这个函数是为了配合Python的concurrent.futures模块而设计的：\n",
    "    1. 接收打包的参数\n",
    "    2. 调用create_sft_data处理文章\n",
    "    3. 将结果写入输出文件\n",
    "    4. 返回处理是否成功的布尔值\n",
    "    \n",
    "    参数说明：\n",
    "    - args: 包含(item, index, total, output_file)的元组\n",
    "    \n",
    "    返回值：\n",
    "    - True: 处理成功\n",
    "    - False: 处理失败\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    item, index, total, output_file = args\n",
    "    \n",
    "    # 添加随机延迟，避免所有线程同时发起请求\n",
    "    time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "    \n",
    "    # 处理文章，生成SFT数据\n",
    "    result = create_sft_data(item, index, total)\n",
    "    if result:\n",
    "        # 如果处理成功，将结果写入文件\n",
    "        # 使用线程锁确保多个线程不会同时写入文件\n",
    "        with output_lock:\n",
    "            with open(output_file, 'a', encoding='utf-8') as out_f:\n",
    "                # 将结果转换为JSON格式并写入文件\n",
    "                # ensure_ascii=False确保中文字符正确显示\n",
    "                out_f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def sample_evenly(data_list, sample_count):\n",
    "    \"\"\"\n",
    "    均匀采样函数：从数据列表中均匀选择指定数量的样本\n",
    "    \n",
    "    这个函数的作用是：\n",
    "    1. 如果数据量小于等于需要的样本数，返回全部数据\n",
    "    2. 如果数据量大于需要的样本数，按固定间隔均匀选择样本\n",
    "    3. 确保选择的样本能够代表整个数据集\n",
    "    \n",
    "    参数说明：\n",
    "    - data_list: 原始数据列表\n",
    "    - sample_count: 需要采样的数量\n",
    "    \n",
    "    返回值：\n",
    "    - 采样后的数据列表\n",
    "    \"\"\"\n",
    "    total_count = len(data_list)\n",
    "    \n",
    "    # 如果数据总量小于等于需要采样的数量，返回全部数据\n",
    "    if total_count <= sample_count:\n",
    "        print(f\"数据总量({total_count})小于等于需要采样的数量({sample_count})，返回全部数据\")\n",
    "        return data_list\n",
    "    \n",
    "    # 计算采样间隔\n",
    "    # 例如：如果有1000条数据，需要100个样本，间隔就是10\n",
    "    step = total_count / sample_count\n",
    "    indices = [int(i * step) for i in range(sample_count)]\n",
    "    \n",
    "    # 确保最后一个样本是数据集的最后一个元素\n",
    "    # 这样可以保证采样覆盖整个数据集的范围\n",
    "    if indices[-1] != total_count - 1:\n",
    "        indices[-1] = total_count - 1\n",
    "        \n",
    "    # 根据计算出的索引选择样本\n",
    "    return [data_list[i] for i in indices]\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数：协调整个数据处理流程\n",
    "    \n",
    "    这个函数负责：\n",
    "    1. 检查输入文件和输出目录\n",
    "    2. 读取和解析输入数据\n",
    "    3. 进行数据采样\n",
    "    4. 启动并行处理\n",
    "    5. 统计和报告处理结果\n",
    "    \"\"\"\n",
    "    # ==================== 文件和目录检查 ====================\n",
    "    \n",
    "    # 确保输出目录存在，如果不存在则创建\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"错误: 输入文件 '{INPUT_FILE}' 不存在!\")\n",
    "        return\n",
    "    \n",
    "    # 检查输入文件大小，确保文件不为空\n",
    "    file_size = os.path.getsize(INPUT_FILE)\n",
    "    print(f\"输入文件大小: {file_size} 字节\")\n",
    "    if file_size == 0:\n",
    "        print(\"错误: 输入文件为空!\")\n",
    "        return\n",
    "    \n",
    "    # ==================== 数据读取和解析 ====================\n",
    "    \n",
    "    # 初始化统计变量\n",
    "    items = []          # 存储有效的数据记录\n",
    "    line_count = 0      # 总行数计数\n",
    "    error_count = 0     # 解析错误计数\n",
    "    valid_count = 0     # 有效记录计数\n",
    "    \n",
    "    try:\n",
    "        # 逐行读取JSONL文件（每行一个JSON对象）\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line_count += 1\n",
    "                line = line.strip()  # 去除行首尾的空白字符\n",
    "                if not line:  # 跳过空行\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # 尝试解析JSON\n",
    "                    item = json.loads(line)\n",
    "                    \n",
    "                    # 检查数据是否包含必要的字段\n",
    "                    # 支持两种字段名格式：'Article'/'Summary' 或 'article'/'summary'\n",
    "                    if 'Article' in item:\n",
    "                        if 'Summary' in item:\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'Summary'字段\")\n",
    "                    elif 'article' in item:\n",
    "                        if 'summary' in item:\n",
    "                            # 标准化字段名，统一使用大写开头\n",
    "                            item['Article'] = item['article']\n",
    "                            item['Summary'] = item['summary']\n",
    "                            items.append(item)\n",
    "                            valid_count += 1\n",
    "                        else:\n",
    "                            print(f\"警告: 第 {line_num} 行没有'summary'字段\")\n",
    "                    else:\n",
    "                        # 如果没有找到预期的字段，显示可用字段以便调试\n",
    "                        print(f\"警告: 第 {line_num} 行没有'Article'或'article'字段\")\n",
    "                        print(f\"可用字段: {list(item.keys())}\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    # 处理JSON解析错误\n",
    "                    error_count += 1\n",
    "                    print(f\"错误: 第 {line_num} 行JSON解析失败: {e}\")\n",
    "                    print(f\"问题行内容: {line[:100]}...\")  # 显示前100个字符用于调试\n",
    "                    \n",
    "    except Exception as e:\n",
    "        # 处理文件读取过程中的其他错误\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "    \n",
    "    # ==================== 数据统计报告 ====================\n",
    "    \n",
    "    print(f\"文件共有 {line_count} 行\")\n",
    "    print(f\"解析错误: {error_count} 行\")\n",
    "    print(f\"成功读取: {valid_count} 条有效记录\")\n",
    "    print(f\"最终收集: {len(items)} 条记录\")\n",
    "    \n",
    "    # 如果没有读取到任何有效数据，显示调试信息\n",
    "    if len(items) == 0:\n",
    "        print(\"\\n尝试显示文件前5行内容进行调试:\")\n",
    "        try:\n",
    "            with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= 5:  # 只显示前5行\n",
    "                        break\n",
    "                    print(f\"第 {i+1} 行: {line.strip()[:200]}...\")  # 显示前200个字符\n",
    "                    try:\n",
    "                        # 尝试解析JSON并显示字段名\n",
    "                        data = json.loads(line.strip())\n",
    "                        print(f\"JSON键: {list(data.keys())}\")\n",
    "                    except:\n",
    "                        pass  # 忽略解析错误\n",
    "        except Exception as e:\n",
    "            print(f\"显示文件内容时发生错误: {e}\")\n",
    "        return\n",
    "    \n",
    "    # ==================== 数据采样 ====================\n",
    "    \n",
    "    # 从所有有效数据中均匀采样指定数量的记录\n",
    "    sampled_items = sample_evenly(items, SAMPLE_COUNT)\n",
    "    print(f\"均匀采样了 {len(sampled_items)}/{len(items)} 条记录\")\n",
    "    \n",
    "    # ==================== 输出文件准备 ====================\n",
    "    \n",
    "    # 创建或清空输出文件，准备写入新数据\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        pass  # 只是创建/清空文件，不写入任何内容\n",
    "    \n",
    "    # ==================== 并行处理 ====================\n",
    "    \n",
    "    print(f\"开始并行处理，最大线程数: {MAX_WORKERS}\")\n",
    "    \n",
    "    # 为每个要处理的文章准备参数\n",
    "    # 每个参数包含：文章数据、索引、总数、输出文件路径\n",
    "    args_list = [(item, i, len(sampled_items), OUTPUT_FILE) for i, item in enumerate(sampled_items)]\n",
    "    \n",
    "    # 使用线程池进行并行处理\n",
    "    success_count = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 将所有任务提交给线程池，并等待完成\n",
    "        results = list(executor.map(process_article, args_list))\n",
    "        # 统计成功处理的数量\n",
    "        success_count = sum(1 for r in results if r)\n",
    "    \n",
    "    # ==================== 处理结果报告 ====================\n",
    "    \n",
    "    print(f\"处理完成，成功处理 {success_count}/{len(sampled_items)} 条记录\")\n",
    "    print(f\"结果已保存至 {OUTPUT_FILE}\")\n",
    "\n",
    "# ==================== 程序入口 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # 当脚本被直接运行时，执行主函数\n",
    "    # 这个条件确保当脚本被其他模块导入时不会自动执行main函数\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ba217",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250526202542599.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717de9e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250521211036380.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb77f66",
   "metadata": {},
   "source": [
    "# 3.方案选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce782e",
   "metadata": {},
   "source": [
    "## 一、框架核心特性对比表\n",
    "\n",
    "| 框架 | 开发机构 | 核心定位 | 支持模型数量 | 训练方式 | 分布式支持 | 数据安全 | 收费模式 |\n",
    "|------|----------|----------|--------------|----------|------------|----------|----------|\n",
    "| **LLaMA-Factory** | 开源社区 | 通用微调平台 | 450+文本+150+多模态 | 预训练/SFT/DPO/PPO/KTO/ORPO | ✅ 原生支持 | ✅ 完全本地 | 完全免费 |\n",
    "| **Unsloth** | Unsloth AI | 速度优化 | 主流LLM | LoRA/QLoRA | ❌ 免费版不支持 | ⚠️ Pro版需联网 | 免费版+Pro版 |\n",
    "| **EasyR1** | hiyouga | 强化学习专用 | Llama/Qwen/DeepSeek系列 | GRPO/Reinforce++/ReMax/RLOO | ✅ 多节点支持 | ✅ 完全本地 | 完全免费 |\n",
    "| **Axolotl** | OpenAccess-AI | 灵活配置 | 主流开源模型 | 全参/LoRA/QLoRA | ✅ 支持 | ✅ 完全本地 | 完全免费 |\n",
    "| **Swift** | 阿里魔塔 | 全链路解决方案 | 450+文本+150+多模态 | 全方位训练方式 | ✅ 支持 | ✅ 完全本地 | 完全免费 |\n",
    "| **XTuner** | 上海AI实验室 | 高效轻量 | InternLM/Llama/ChatGLM等 | 预训练/SFT/Agent微调 | ✅ 多节点支持 | ✅ 完全本地 | 完全免费 |\n",
    "| **PEFT+DeepSpeed** | Meta+微软 | 底层组件 | 理论上所有 | 自定义实现 | ✅ 完全可控 | ✅ 完全本地 | 完全免费 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563f192",
   "metadata": {},
   "source": [
    "\n",
    "## 二、技术能力深度对比\n",
    "\n",
    "### 2.1 训练效率与性能\n",
    "\n",
    "| 框架 | 训练速度 | 内存效率 | GPU利用率 | 特殊优化 |\n",
    "|------|----------|----------|-----------|----------|\n",
    "| **LLaMA-Factory** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | FlashAttention-2, Unsloth集成, NEFTune |\n",
    "| **Unsloth** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | Triton重写, 手动反向传播, 零精度损失 |\n",
    "| **EasyR1** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | HybridEngine, vLLM SPMD, 无填充训练 |\n",
    "| **Axolotl** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | xformers, Flash Attention, multipacking |\n",
    "| **Swift** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | Liger-Kernel, UnSloth集成 |\n",
    "| **XTuner** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 自动算子分发, 8GB微调7B |\n",
    "| **PEFT+DeepSpeed** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ZeRO优化, 参数卸载, 自定义优化 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3aff06",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 功能完整性对比\n",
    "\n",
    "| 功能类别 | LLaMA-Factory | Unsloth | EasyR1 | Axolotl | Swift | XTuner | PEFT+DeepSpeed |\n",
    "|----------|---------------|---------|--------|---------|-------|--------|----------------|\n",
    "| **预训练** | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ✅ |\n",
    "| **SFT** | ✅ | ✅ | ⚠️ 基础 | ✅ | ✅ | ✅ | ✅ |\n",
    "| **RLHF/DPO** | ✅ | ❌ | ✅ 专业 | ✅ | ✅ | ❌ | ✅ |\n",
    "| **多模态** | ✅ | ❌ | ✅ 视觉 | ❌ | ✅ | ✅ | ✅ |\n",
    "| **量化训练** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| **推理部署** | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |\n",
    "| **Web界面** | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8d1d1",
   "metadata": {},
   "source": [
    "\n",
    "## 三、企业级关键考量因素\n",
    "\n",
    "### 3.1 数据安全与合规性\n",
    "\n",
    "| 框架 | 离线部署 | 数据本地化 | 外部依赖 | 企业合规 | 审计能力 |\n",
    "|------|----------|------------|----------|----------|----------|\n",
    "| **LLaMA-Factory** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 高 | ✅ 完整日志 |\n",
    "| **Unsloth免费版** | ✅ 支持 | ✅ 本地 | ❌ 无 | ✅ 高 | ⚠️ 有限 |\n",
    "| **Unsloth Pro版** | ⚠️ 需联网验证 | ⚠️ 部分限制 | ⚠️ 许可证验证 | ❌ 低 | ❌ 受限 |\n",
    "| **EasyR1** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 高 | ✅ 完整日志 |\n",
    "| **Axolotl** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 高 | ✅ 完整日志 |\n",
    "| **Swift** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 高 | ✅ 完整日志 |\n",
    "| **XTuner** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 高 | ✅ 完整日志 |\n",
    "| **PEFT+DeepSpeed** | ✅ 完全支持 | ✅ 100%本地 | ❌ 无强制依赖 | ✅ 最高 | ✅ 完全可控 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16042b87",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 成本结构分析\n",
    "\n",
    "| 框架 | 许可费用 | 开发成本 | 维护成本 | 培训成本 | 扩展成本 | 总体TCO |\n",
    "|------|----------|----------|----------|----------|----------|---------|\n",
    "| **LLaMA-Factory** | 免费 | 低 | 中 | 低 | 低 | ⭐⭐⭐⭐⭐ |\n",
    "| **Unsloth免费版** | 免费 | 低 | 低 | 低 | 高(需升级) | ⭐⭐⭐ |\n",
    "| **Unsloth Pro版** | 订阅制 | 低 | 中 | 低 | 中 | ⭐⭐ |\n",
    "| **EasyR1** | 免费 | 中 | 中 | 中 | 低 | ⭐⭐⭐⭐ |\n",
    "| **Axolotl** | 免费 | 高 | 高 | 高 | 低 | ⭐⭐⭐ |\n",
    "| **Swift** | 免费 | 中 | 中 | 中 | 中 | ⭐⭐⭐⭐ |\n",
    "| **XTuner** | 免费 | 中 | 中 | 中 | 低 | ⭐⭐⭐⭐ |\n",
    "| **PEFT+DeepSpeed** | 免费 | 高 | 高 | 高 | 低 | ⭐⭐ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fc606",
   "metadata": {},
   "source": [
    "\n",
    "## 四、PEFT+DeepSpeed vs 现成框架深度对比\n",
    "\n",
    "### 4.1 PEFT+DeepSpeed组合方案详细分析\n",
    "\n",
    "\n",
    "**与现成框架对比：**\n",
    "\n",
    "| 对比维度 | PEFT+DeepSpeed | 现成框架(LLaMA-Factory等) |\n",
    "|----------|----------------|---------------------------|\n",
    "| **技术控制度** | ✅ 100%可控，每个参数都可调整 | ⚠️ 受框架封装限制 |\n",
    "| **性能优化** | ✅ 可针对特定硬件深度优化 | ⚠️ 通用优化，可能不是最优 |\n",
    "| **问题定位** | ❌ 需要深入理解底层原理 | ✅ 框架提供错误处理 |\n",
    "| **开发效率** | ❌ 需要大量代码开发 | ✅ 配置文件即可使用 |\n",
    "| **维护复杂度** | ❌ 需要跟踪多个库版本兼容性 | ✅ 框架统一管理依赖 |\n",
    "| **扩展性** | ✅ 可以添加任何自定义功能 | ⚠️ 受框架功能限制 |\n",
    "| **团队要求** | ❌ 需要深度ML工程师 | ✅ 普通工程师即可 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fda6d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 内存使用效率对比\n",
    "\n",
    "| 模型规模 | PEFT+DeepSpeed ZeRO-3 | LLaMA-Factory | Unsloth | EasyR1 |\n",
    "|----------|------------------------|---------------|---------|--------|\n",
    "| **7B模型** | 16GB (优化后) | 20GB | 10GB | 18GB |\n",
    "| **13B模型** | 28GB (优化后) | 35GB | 18GB | 32GB |\n",
    "| **30B模型** | 65GB (优化后) | 80GB | N/A | 75GB |\n",
    "| **70B模型** | 150GB (优化后) | 180GB | N/A | 160GB |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b067b3e",
   "metadata": {},
   "source": [
    "\n",
    "## 五、具体应用场景与推荐方案\n",
    "\n",
    "### 5.1 金融行业（高安全要求）\n",
    "\n",
    "**场景特点：**\n",
    "- 数据绝对不能外泄\n",
    "- 需要完整的审计日志\n",
    "- 对稳定性要求极高\n",
    "- 有专业技术团队\n",
    "\n",
    "**推荐方案：**\n",
    "1. **短期（3-6个月）**：LLaMA-Factory\n",
    "   - 快速上线，风险可控\n",
    "   - 完全本地部署\n",
    "   - 功能完整，满足基本需求\n",
    "\n",
    "2. **长期（6个月+）**：PEFT+DeepSpeed自研\n",
    "   - 最大化技术控制权\n",
    "   - 可以实现最严格的安全要求\n",
    "   - 建立内部技术壁垒\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9ae78",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 医疗行业（合规+性能）\n",
    "\n",
    "**场景特点：**\n",
    "- 严格的数据合规要求\n",
    "- 需要多模态能力（医学影像）\n",
    "- 对准确性要求极高\n",
    "- 需要可解释性\n",
    "\n",
    "**推荐方案：**\n",
    "1. **主方案**：LLaMA-Factory + EasyR1组合\n",
    "   - LLaMA-Factory处理多模态SFT\n",
    "   - EasyR1进行RLHF优化\n",
    "   - 完全本地部署，满足合规要求\n",
    "\n",
    "2. **备选方案**：Swift全链路\n",
    "   - 一站式解决方案\n",
    "   - 内置评测能力\n",
    "   - 阿里技术背景，稳定性好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a2052",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3 科技公司（技术驱动）\n",
    "\n",
    "**场景特点：**\n",
    "- 技术实力强\n",
    "- 需要前沿算法\n",
    "- 追求极致性能\n",
    "- 有长期技术规划\n",
    "\n",
    "**推荐方案：**\n",
    "1. **研发阶段**：EasyR1 + LLaMA-Factory\n",
    "   - EasyR1探索RLHF前沿算法\n",
    "   - LLaMA-Factory处理常规训练\n",
    "   - 积累技术经验\n",
    "\n",
    "2. **生产阶段**：PEFT+DeepSpeed自研\n",
    "   - 完全自主可控\n",
    "   - 可以实现最优性能\n",
    "   - 建立技术护城河\n",
    "\n",
    "### 5.4 中小企业（快速上线）\n",
    "\n",
    "**场景特点：**\n",
    "- 技术团队有限\n",
    "- 需要快速验证\n",
    "- 成本控制严格\n",
    "- 功能要求基础\n",
    "\n",
    "**推荐方案：**\n",
    "1. **唯一推荐**：LLaMA-Factory\n",
    "   - 零代码Web界面\n",
    "   - 功能完整\n",
    "   - 社区支持好\n",
    "   - 学习资源丰富\n",
    "   - **避免Unsloth Pro版的收费陷阱**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68722a0",
   "metadata": {},
   "source": [
    "## 六、决策矩阵与最终建议\n",
    "\n",
    "### 6.1 决策权重矩阵\n",
    "\n",
    "| 企业类型 | 数据安全(40%) | 技术控制(25%) | 成本效益(20%) | 上手难度(15%) | 推荐方案 |\n",
    "|----------|---------------|---------------|---------------|---------------|----------|\n",
    "| **大型金融** | 极高 | 高 | 中 | 低 | PEFT+DeepSpeed |\n",
    "| **大型科技** | 高 | 极高 | 中 | 低 | EasyR1+自研 |\n",
    "| **中型制造** | 高 | 中 | 极高 | 高 | XTuner |\n",
    "| **小型企业** | 中 | 低 | 极高 | 极高 | LLaMA-Factory |\n",
    "| **医疗机构** | 极高 | 中 | 中 | 中 | LLaMA-Factory+EasyR1 |\n",
    "\n",
    "### 6.2 分阶段实施策略\n",
    "\n",
    "**第一阶段（验证期 1-3个月）：**\n",
    "- 所有企业都建议从LLaMA-Factory开始\n",
    "- 快速验证技术可行性和业务价值\n",
    "- 积累团队经验和数据\n",
    "\n",
    "**第二阶段（发展期 3-12个月）：**\n",
    "- 根据业务需求选择专业化方案\n",
    "- 大型企业开始考虑自研\n",
    "- 中小企业深化现有方案\n",
    "\n",
    "**第三阶段（成熟期 12个月+）：**\n",
    "- 建立长期技术架构\n",
    "- 大型企业实现完全自主可控\n",
    "- 中小企业优化成本效益\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90bb13",
   "metadata": {},
   "source": [
    "\n",
    "## 七、总结与行动建议\n",
    "\n",
    "**企业级微调框架选择的核心原则：**\n",
    "\n",
    "1. **数据安全第一**：优先选择完全本地部署的方案\n",
    "2. **成本可控**：避免有隐藏收费的商业方案\n",
    "3. **技术可持续**：选择有长期维护能力的开源方案\n",
    "4. **分阶段实施**：从简单方案开始，逐步深化\n",
    "\n",
    "**立即行动建议：**\n",
    "\n",
    "1. **立即开始**：使用LLaMA-Factory进行概念验证\n",
    "2. **并行准备**：评估团队技术能力，制定长期规划\n",
    "3. **持续学习**：关注EasyR1等新兴框架的发展\n",
    "4. **建立标准**：制定企业内部的AI开发规范"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfb7cb",
   "metadata": {},
   "source": [
    "# 4.deepspeed详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dc43a",
   "metadata": {},
   "source": [
    "### 什么是 DeepSpeed？ \n",
    "1.1 简单理解   \n",
    "想象一下，你要搬一座山（训练大模型），但你只有一个人（单GPU）。DeepSpeed就像是给你提供了：   \n",
    "🚛 更大的卡车（内存优化）   \n",
    "👥 更多的工人（多GPU协作）   \n",
    "📋 更好的协调方案（智能调度）   \n",
    "**DeepSpeed是微软开发的深度学习优化库，专门解决大模型训练中的内存和计算问题。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87b7f7",
   "metadata": {},
   "source": [
    "### 1.2 核心解决的问题\n",
    "```json\n",
    "传统训练问题：\n",
    "┌─────────────────┐\n",
    "│ 模型太大 → OOM  │  \n",
    "│ 训练太慢 → 等待 │\n",
    "│ 多卡难用 → 浪费 │\n",
    "└─────────────────┘\n",
    "         ↓\n",
    "DeepSpeed解决方案：\n",
    "┌─────────────────┐\n",
    "│ 内存优化 → 能跑 │\n",
    "│ 并行加速 → 更快 │  \n",
    "│ 分布式 → 扩展   │\n",
    "└─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c1772",
   "metadata": {},
   "source": [
    "### 1.3 什么是内存？什么是显存？\n",
    "\n",
    "```json\n",
    "计算机的存储层次：\n",
    "┌─────────────────┐  最快，最贵，容量最小\n",
    "│   GPU 显存      │  ← 这里存放GPU直接使用的数据\n",
    "│   (VRAM)        │     例如：24GB, 40GB, 80GB\n",
    "├─────────────────┤\n",
    "│   CPU 内存      │  ← 这里存放CPU和系统使用的数据  \n",
    "│   (RAM)         │     例如：64GB, 128GB, 256GB\n",
    "├─────────────────┤\n",
    "│   硬盘存储      │  ← 这里存放长期数据\n",
    "│   (SSD/HDD)     │     例如：1TB, 2TB, 4TB\n",
    "└─────────────────┘  最慢，最便宜，容量最大\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7720e9",
   "metadata": {},
   "source": [
    "```python\n",
    "# 传统训练时，每个GPU的内存布局\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    GPU 显存 (例如40GB)                    │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 模型参数 (Parameters)                                    │\n",
    "│ - 权重矩阵: W1, W2, W3... (FP16: 2字节/参数)            │\n",
    "│ - 偏置向量: b1, b2, b3... (FP16: 2字节/参数)            │\n",
    "│ 例如7B模型 ≈ 14GB                                       │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 梯度 (Gradients)                                        │\n",
    "│ - 每个参数对应一个梯度值                                  │\n",
    "│ - 与参数相同大小: 7B × 2字节 ≈ 14GB                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 优化器状态 (Optimizer States)                           │\n",
    "│ - Adam优化器需要存储:                                    │\n",
    "│   * 一阶动量 (momentum): 7B × 4字节 ≈ 28GB             │\n",
    "│   * 二阶动量 (variance): 7B × 4字节 ≈ 28GB             │\n",
    "│ - 总计: 56GB                                            │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 激活值 (Activations)                                    │\n",
    "│ - 前向传播中间结果                                       │\n",
    "│ - 反向传播需要保存: 5-15GB (取决于序列长度)              │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ 临时缓冲区                                               │\n",
    "│ - 通信缓冲区、临时计算空间: 2-5GB                        │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "总计: 14 + 14 + 56 + 10 + 3 = 97GB > 40GB GPU显存 ❌\n",
    "```\n",
    "**问题核心**：单个GPU根本装不下完整的训练状态！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d3e6b3",
   "metadata": {},
   "source": [
    "### 1.4 DeepSpeed ZeRO的内存重新分配策略\n",
    "\n",
    "DeepSpeed的核心思想是：**不是每个GPU都需要存储完整的训练状态**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b901a4",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "ZeRO-1: 只分割优化器状态\n",
    "├── 模型参数：每个GPU都有完整副本\n",
    "├── 梯度：每个GPU都有完整副本  \n",
    "└── 优化器状态：分割到不同GPU ✂️\n",
    "\n",
    "ZeRO-2: 分割优化器状态 + 梯度\n",
    "├── 模型参数：每个GPU都有完整副本\n",
    "├── 梯度：分割到不同GPU ✂️\n",
    "└── 优化器状态：分割到不同GPU ✂️\n",
    "\n",
    "ZeRO-3: 分割所有内容\n",
    "├── 模型参数：分割到不同GPU ✂️\n",
    "├── 梯度：分割到不同GPU ✂️\n",
    "└── 优化器状态：分割到不同GPU ✂️\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33785d7a",
   "metadata": {},
   "source": [
    "#### ZeRO-1: 优化器状态分片 (Optimizer State Partitioning)\n",
    "\n",
    "```python\n",
    "# 4个GPU的情况下，ZeRO-1的内存重新分配\n",
    "传统方式 (每个GPU重复存储):\n",
    "GPU 0: [完整模型14GB] + [完整梯度14GB] + [完整优化器56GB] = 84GB ❌\n",
    "GPU 1: [完整模型14GB] + [完整梯度14GB] + [完整优化器56GB] = 84GB ❌  \n",
    "GPU 2: [完整模型14GB] + [完整梯度14GB] + [完整优化器56GB] = 84GB ❌\n",
    "GPU 3: [完整模型14GB] + [完整梯度14GB] + [完整优化器56GB] = 84GB ❌\n",
    "\n",
    "ZeRO-1 (优化器状态分片):\n",
    "GPU 0: [完整模型14GB] + [完整梯度14GB] + [优化器1/4: 14GB] = 42GB ✅\n",
    "GPU 1: [完整模型14GB] + [完整梯度14GB] + [优化器2/4: 14GB] = 42GB ✅\n",
    "GPU 2: [完整模型14GB] + [完整梯度14GB] + [优化器3/4: 14GB] = 42GB ✅  \n",
    "GPU 3: [完整模型14GB] + [完整梯度14GB] + [优化器4/4: 14GB] = 42GB ✅\n",
    "```\n",
    "**关键机制**：\n",
    "1. **分片存储**：每个GPU只存储1/N的优化器状态\n",
    "2. **参数更新时的通信**：当需要更新参数时，GPU之间交换优化器状态\n",
    "3. **All-Gather操作**：收集所有GPU的优化器状态片段\n",
    "\n",
    "#### 通信模式\n",
    "前向传播: 无额外通信 (所有GPU都有完整模型)   \n",
    "反向传播: 无额外通信 (所有GPU都有完整梯度)   \n",
    "参数更新: All-Gather优化器状态 → 更新参数 → 广播新参数   \n",
    "\n",
    "#### 优势\n",
    "✅ 通信开销最小    \n",
    "✅ 训练速度最快    \n",
    "✅ 实现简单，稳定性最好     \n",
    "\n",
    "#### 劣势  \n",
    "❌ 内存节省有限 (只节省优化器内存)    \n",
    "❌ 仍需要大量GPU内存存储完整模型和梯度   \n",
    "\n",
    "#### 适用场景\n",
    "🎯 GPU内存充足 (40GB+)   \n",
    "🎯 追求最高训练速度   \n",
    "🎯 模型不是特别大 (7B-13B)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d510ff",
   "metadata": {},
   "source": [
    "#### ZeRO-2: 梯度分片 (Gradient Partitioning)\n",
    "\n",
    "```python\n",
    "# ZeRO-2进一步分片梯度\n",
    "GPU 0: [完整模型14GB] + [梯度1/4: 3.5GB] + [优化器1/4: 14GB] = 31.5GB ✅\n",
    "GPU 1: [完整模型14GB] + [梯度2/4: 3.5GB] + [优化器2/4: 14GB] = 31.5GB ✅\n",
    "GPU 2: [完整模型14GB] + [梯度3/4: 3.5GB] + [优化器3/4: 14GB] = 31.5GB ✅\n",
    "GPU 3: [完整模型14GB] + [梯度4/4: 3.5GB] + [优化器4/4: 14GB] = 31.5GB ✅\n",
    "```\n",
    "\n",
    "**通信机制**：\n",
    "1. **Reduce-Scatter**：反向传播后，将梯度分散到对应的GPU     \n",
    "2. **All-Gather**：前向传播前，收集完整梯度用于参数更新   \n",
    "\n",
    "\n",
    "#### 通信模式\n",
    "前向传播: 无额外通信    \n",
    "反向传播: Reduce-Scatter梯度 (将梯度分散到对应GPU)   \n",
    "参数更新: All-Gather优化器状态 → 更新参数 → 广播新参数    \n",
    "\n",
    "#### 优势\n",
    "✅ 内存节省显著 (节省75%的梯度内存)    \n",
    "✅ 通信开销适中    \n",
    "✅ 性能和内存的良好平衡    \n",
    "\n",
    "#### 劣势\n",
    "❌ 反向传播时需要额外通信   \n",
    "❌ 仍需要完整模型参数   \n",
    "\n",
    "#### 适用场景  \n",
    "🎯 GPU内存中等 (24-40GB)   \n",
    "🎯 平衡性能和内存使用    \n",
    "🎯 中等大小模型 (13B-30B)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd72bad",
   "metadata": {},
   "source": [
    "\n",
    "#### ZeRO-3: 参数分片 (Parameter Partitioning)\n",
    "\n",
    "```python\n",
    "# ZeRO-3最激进的分片策略\n",
    "GPU 0: [模型1/4: 3.5GB] + [梯度1/4: 3.5GB] + [优化器1/4: 14GB] = 21GB ✅\n",
    "GPU 1: [模型2/4: 3.5GB] + [梯度2/4: 3.5GB] + [优化器2/4: 14GB] = 21GB ✅\n",
    "GPU 2: [模型3/4: 3.5GB] + [梯度3/4: 3.5GB] + [优化器3/4: 14GB] = 21GB ✅\n",
    "GPU 3: [模型4/4: 3.5GB] + [梯度4/4: 3.5GB] + [优化器4/4: 14GB] = 21GB ✅\n",
    "```\n",
    "#### 通信模式 (最复杂)\n",
    "前向传播: 每层都需要All-Gather参数 → 计算 → 释放参数    \n",
    "反向传播: 每层都需要All-Gather参数 → 计算梯度 → Reduce-Scatter梯度    \n",
    "参数更新: All-Gather优化器状态 → 更新参数 → 分片存储      \n",
    "\n",
    "```python\n",
    "# ZeRO-3前向传播的详细通信过程\n",
    "时间轴: ────────────────────────────────────────→\n",
    "\n",
    "GPU 0: │参数1/4│ All-Gather │完整参数│ 计算 │释放│ All-Gather │...\n",
    "GPU 1: │参数2/4│     ↓      │完整参数│ 计算 │释放│     ↓      │...  \n",
    "GPU 2: │参数3/4│ 收集参数    │完整参数│ 计算 │释放│ 收集参数    │...\n",
    "GPU 3: │参数4/4│     ↑      │完整参数│ 计算 │释放│     ↑      │...\n",
    "\n",
    "步骤详解:\n",
    "1. All-Gather: 每个GPU贡献自己的参数片段，组装完整参数\n",
    "2. 计算: 所有GPU使用完整参数进行前向计算  \n",
    "3. 释放: 计算完成后立即释放参数，节省内存\n",
    "4. 重复: 下一层重复此过程\n",
    "```\n",
    "\n",
    "#### 优势\n",
    "✅ 内存节省最大 (可训练超大模型)   \n",
    "✅ 支持无限大的模型 (理论上)     \n",
    "✅ 可以在小内存GPU上训练大模型     \n",
    "\n",
    "#### 劣势\n",
    "❌ 通信开销最大 (每层都需要通信)   \n",
    "❌ 训练速度最慢    \n",
    "❌ 实现复杂，可能不稳定   \n",
    "\n",
    "#### 适用场景\n",
    "🎯 GPU内存有限 (16-24GB)   \n",
    "🎯 超大模型 (30B+)    \n",
    "🎯 愿意牺牲速度换取内存    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85c7cc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 内存节省效果对比\n",
    "\n",
    "| ZeRO阶段 | 模型参数 | 梯度 | 优化器状态 | 总内存(7B模型,4GPU) | 节省比例 |\n",
    "|----------|----------|------|------------|---------------------|----------|\n",
    "| **传统** | 14GB | 14GB | 56GB | 84GB | 0% |\n",
    "| **ZeRO-1** | 14GB | 14GB | 14GB | 42GB | 50% |\n",
    "| **ZeRO-2** | 14GB | 3.5GB | 14GB | 31.5GB | 62.5% |\n",
    "| **ZeRO-3** | 3.5GB | 3.5GB | 14GB | 21GB | 75% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d893cf",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 什么是Offload？\n",
    "\n",
    "```python\n",
    "# Offload = 卸载 = 把一些东西放到别的地方存储\n",
    "\n",
    "不用Offload：所有东西都放在GPU显存里\n",
    "┌─────────────────┐\n",
    "│   GPU 显存      │ ← 模型+梯度+优化器都在这里\n",
    "│   (40GB)        │\n",
    "└─────────────────┘\n",
    "\n",
    "用CPU Offload：把一些东西放到CPU内存里\n",
    "┌─────────────────┐    ┌─────────────────┐\n",
    "│   GPU 显存      │    │   CPU 内存      │\n",
    "│   (40GB)        │    │   (128GB)       │\n",
    "│ 正在计算的数据   │    │ 暂时不用的数据   │\n",
    "└─────────────────┘    └─────────────────┘\n",
    "\n",
    "NVMe Offload:\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   GPU 显存      │    │   CPU 内存      │    │   NVMe 存储     │\n",
    "│ 当前计算数据     │    │ 缓冲数据        │    │ 冷数据          │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818cd0b8",
   "metadata": {},
   "source": [
    "### 2.2 Offload 可以卸载什么？\n",
    "\n",
    "```python\n",
    "# Offload 可以选择卸载不同的组件：\n",
    "\n",
    "1. 优化器状态卸载 (optimizer offload)\n",
    "   - 把Adam的momentum和variance卸载到CPU/磁盘\n",
    "   - 最常用，效果最好\n",
    "\n",
    "2. 模型参数卸载 (parameter offload)  \n",
    "   - 把模型权重卸载到CPU/磁盘\n",
    "   - 只在ZeRO-3中可用\n",
    "   - 效果最激进\n",
    "\n",
    "3. 梯度卸载 (gradient offload)\n",
    "   - 把梯度卸载到CPU/磁盘\n",
    "   - 较少使用，收益有限\n",
    "```\n",
    "\n",
    "### 2.3 Offload 目标位置\n",
    "\n",
    "```python\n",
    "# 可以卸载到三个地方：\n",
    "\n",
    "1. CPU内存 (device: \"cpu\")\n",
    "   - 速度：中等（比GPU慢10-20倍）\n",
    "   - 容量：大（通常64GB-512GB）\n",
    "   - 成本：中等\n",
    "\n",
    "2. NVMe SSD (device: \"nvme\") \n",
    "   - 速度：慢（比GPU慢100-200倍）\n",
    "   - 容量：很大（1TB-8TB）\n",
    "   - 成本：低\n",
    "\n",
    "3. 普通硬盘 (device: \"disk\")\n",
    "   - 速度：很慢（比GPU慢1000倍+）\n",
    "   - 容量：最大（10TB+）\n",
    "   - 成本：最低\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21226130",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 不同Offload方式的对比\n",
    "\n",
    "```python\n",
    "# 1. 无Offload（纯GPU）\n",
    "优点：速度最快\n",
    "缺点：显存占用最多\n",
    "适合：显存充足的情况\n",
    "\n",
    "# 2. CPU Offload\n",
    "优点：节省显存，速度还可以\n",
    "缺点：比纯GPU慢一些\n",
    "适合：显存不够但CPU内存充足\n",
    "\n",
    "# 3. NVMe Offload（硬盘）\n",
    "优点：节省最多显存\n",
    "缺点：速度最慢\n",
    "适合：显存和CPU内存都不够的极端情况\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4f0cd",
   "metadata": {},
   "source": [
    "**具体的数据流动过程**：\n",
    "\n",
    "```python\n",
    "# 1. 前向传播阶段\n",
    "def forward_pass_with_offloading():\n",
    "    for layer in model.layers:\n",
    "        # 步骤1: 从CPU加载当前层参数到GPU\n",
    "        layer_params = cpu_to_gpu_transfer(layer.cpu_params)\n",
    "        \n",
    "        # 步骤2: 在GPU上执行计算\n",
    "        layer_output = layer.forward(input_data, layer_params)\n",
    "        \n",
    "        # 步骤3: 如果不是最后几层，将参数卸载回CPU\n",
    "        if not layer.is_recent_layer():\n",
    "            gpu_to_cpu_transfer(layer_params)\n",
    "        \n",
    "        input_data = layer_output\n",
    "\n",
    "# 2. 反向传播阶段  \n",
    "def backward_pass_with_offloading():\n",
    "    for layer in reversed(model.layers):\n",
    "        # 步骤1: 重新加载参数到GPU\n",
    "        layer_params = cpu_to_gpu_transfer(layer.cpu_params)\n",
    "        \n",
    "        # 步骤2: 计算梯度\n",
    "        gradients = layer.backward(grad_output, layer_params)\n",
    "        \n",
    "        # 步骤3: 将梯度发送到对应的GPU (ZeRO-2)\n",
    "        send_gradients_to_owner_gpu(gradients)\n",
    "        \n",
    "        # 步骤4: 参数卸载回CPU\n",
    "        gpu_to_cpu_transfer(layer_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ceb35b",
   "metadata": {},
   "source": [
    "\n",
    "## 3 精确配置控制\n",
    "\n",
    "### 3.1 优化器状态卸载配置\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,  // 或者 1, 3\n",
    "    \n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",              // 卸载到CPU内存\n",
    "      \"pin_memory\": true,           // 使用页锁定内存（加速传输）\n",
    "      \"buffer_count\": 4,            // 缓冲区数量（并行传输）\n",
    "      \"buffer_size\": 1e8,           // 每个缓冲区大小\n",
    "      \"max_in_cpu\": 1e9,           // CPU中最大数据量\n",
    "      \"fast_init\": false            // 快速初始化（可能不稳定）\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3.2 模型参数卸载配置（仅ZeRO-3）\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,  // 必须是ZeRO-3\n",
    "    \n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\",              // 卸载到CPU\n",
    "      \"pin_memory\": true,           \n",
    "      \"buffer_count\": 5,            // 参数传输缓冲区\n",
    "      \"buffer_size\": 1e8,\n",
    "      \"max_in_cpu\": 1e9            // CPU中最大参数量\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3.3 NVMe 卸载配置\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"nvme\",\n",
    "      \"nvme_path\": \"/tmp/deepspeed_nvme\",  // NVMe挂载路径\n",
    "      \"buffer_count\": 4,\n",
    "      \"buffer_size\": 1e8,\n",
    "      \"max_in_cpu\": 1e9,                  // CPU作为缓存\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \n",
    "    \"offload_param\": {\n",
    "      \"device\": \"nvme\", \n",
    "      \"nvme_path\": \"/tmp/deepspeed_nvme\",\n",
    "      \"buffer_count\": 5,\n",
    "      \"buffer_size\": 1e8,\n",
    "      \"max_in_cpu\": 1e9,\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49ebd9",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 ZeRO 参数层级确认\n",
    "```python\n",
    "\n",
    "ZeRO-1 可用参数：\n",
    "├── stage: 1\n",
    "├── overlap_comm\n",
    "├── contiguous_gradients  \n",
    "├── allgather_bucket_size\n",
    "├── reduce_scatter_bucket_size\n",
    "└── offload_optimizer (可选)\n",
    "\n",
    "ZeRO-2 新增参数：\n",
    "├── ZeRO-1 的所有参数\n",
    "├── reduce_bucket_size\n",
    "└── allgather_partitions\n",
    "\n",
    "ZeRO-3 独有参数：\n",
    "├── ZeRO-1/2 的所有参数\n",
    "├── stage3_max_live_parameters     # ❌ ZeRO-1/2 无法使用\n",
    "├── stage3_max_reuse_distance      # ❌ ZeRO-1/2 无法使用\n",
    "├── stage3_prefetch_bucket_size    # ❌ ZeRO-1/2 无法使用\n",
    "├── stage3_param_persistence_threshold  # ❌ ZeRO-1/2 无法使用\n",
    "├── stage3_gather_16bit_weights_on_model_save  # ❌ ZeRO-1/2 无法使用\n",
    "└── offload_param                  # ❌ ZeRO-1/2 无法使用\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44633dd2",
   "metadata": {},
   "source": [
    "\n",
    "### 3.5 Offload 配置确认\n",
    "```python\n",
    "offload_optimizer:  # 优化器状态卸载\n",
    "├── 适用于：ZeRO-1/2/3 都可以使用\n",
    "└── 卸载内容：Adam的momentum和variance\n",
    "\n",
    "offload_param:      # 模型参数卸载  \n",
    "├── 适用于：仅 ZeRO-3 可以使用 ❌ ZeRO-1/2 无法使用\n",
    "└── 卸载内容：模型权重参数\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a784a93",
   "metadata": {},
   "source": [
    "\n",
    "## 4 配置参数详细解释\n",
    "\n",
    "### 4.1 基础训练参数\n",
    "\n",
    "#### `train_micro_batch_size_per_gpu`\n",
    "```json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 2\n",
    "}\n",
    "```\n",
    "- **作用**：设置每个GPU上的微批次大小\n",
    "- **影响**：\n",
    "  - 硬件：GPU显存使用量（线性关系）\n",
    "  - 程序：影响梯度累积和内存占用\n",
    "  - 性能：更大的batch size通常训练更稳定，但占用更多显存\n",
    "- **冲突处理**：与`TrainingArguments`中`per_device_train_batch_size`不要冲突\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7ed76",
   "metadata": {},
   "source": [
    "\n",
    "#### `train_batch_size`\n",
    "```json\n",
    "{\n",
    "  \"train_batch_size\": \"auto\"  // 或具体数值如 32\n",
    "}\n",
    "```\n",
    "- **作用**：全局批次大小（所有GPU的总和）\n",
    "- **计算公式**：`train_batch_size = train_micro_batch_size_per_gpu × gradient_accumulation_steps × num_gpus`\n",
    "- **影响**：\n",
    "  - 硬件：无直接影响（由micro batch size决定）\n",
    "  - 程序：影响学习率调度和收敛行为\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dbe2b",
   "metadata": {},
   "source": [
    "\n",
    "#### `gradient_accumulation_steps`\n",
    "```json\n",
    "{\n",
    "  \"gradient_accumulation_steps\": 8\n",
    "}\n",
    "```\n",
    "- **作用**：梯度累积步数，模拟更大的batch size\n",
    "- **影响**：\n",
    "  - 硬件：不增加显存使用\n",
    "  - 程序：影响梯度更新频率和训练稳定性\n",
    "  - 性能：增加计算时间，但提高训练稳定性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02214e8e",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 ZeRO 优化参数\n",
    "\n",
    "#### `stage`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3  // 1, 2, 或 3\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：选择ZeRO优化级别\n",
    "- **影响**：\n",
    "  - 硬件：GPU显存使用（stage越高节省越多）\n",
    "  - 程序：通信开销（stage越高通信越多）\n",
    "  - 性能：内存效率 vs 训练速度的权衡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d2133",
   "metadata": {},
   "source": [
    "\n",
    "#### `overlap_comm`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"overlap_comm\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：通信与计算重叠，减少等待时间\n",
    "- **影响**：\n",
    "  - 硬件：需要支持异步通信的GPU和网络\n",
    "  - 程序：减少通信等待时间\n",
    "  - 性能：提高训练速度（5-15%）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8d947",
   "metadata": {},
   "source": [
    "\n",
    "#### `contiguous_gradients`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"contiguous_gradients\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：将梯度存储在连续内存中\n",
    "- **影响**：\n",
    "  - 硬件：减少内存碎片\n",
    "  - 程序：提高内存访问效率\n",
    "  - 性能：减少内存分配开销\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148de5f",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 ZeRO-3 专有参数\n",
    "\n",
    "#### `stage3_max_live_parameters`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"stage3_max_live_parameters\": 1e9\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：控制同时在GPU上的最大参数数量\n",
    "- **影响**：\n",
    "  - 硬件：直接控制GPU显存使用\n",
    "  - 程序：影响参数预取和释放策略\n",
    "  - 调优建议：显存紧张时减少此值\n",
    "\n",
    "#### `stage3_max_reuse_distance`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage3_max_reuse_distance\": 1e9\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：控制参数重用的最大距离\n",
    "- **影响**：\n",
    "  - 硬件：影响参数在GPU上的保留时间\n",
    "  - 程序：影响参数传输频率\n",
    "  - 调优建议：减少此值可节省显存但增加通信\n",
    "\n",
    "#### `stage3_prefetch_bucket_size`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\"  // 或具体数值\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：控制参数预取的桶大小\n",
    "- **影响**：\n",
    "  - 硬件：影响CPU-GPU传输效率\n",
    "  - 程序：影响参数预取策略\n",
    "  - 性能：更大的桶减少传输次数但占用更多内存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0982217",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 Offload 参数详解\n",
    "\n",
    "#### `offload_optimizer`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",           // 卸载目标：cpu/nvme\n",
    "      \"pin_memory\": true,        // 使用页锁定内存\n",
    "      \"buffer_count\": 4,         // 传输缓冲区数量\n",
    "      \"buffer_size\": 1e8,        // 每个缓冲区大小\n",
    "      \"max_in_cpu\": 1e9,         // CPU中最大数据量\n",
    "      \"fast_init\": false         // 快速初始化\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**各参数详解：**\n",
    "\n",
    "##### `device`\n",
    "- **作用**：指定卸载目标位置\n",
    "- **选项**：`\"cpu\"` | `\"nvme\"`\n",
    "- **影响**：\n",
    "  - `\"cpu\"`：中等速度，大容量\n",
    "  - `\"nvme\"`：较慢速度，超大容量\n",
    "\n",
    "##### `pin_memory`\n",
    "- **作用**：使用页锁定内存加速CPU-GPU传输\n",
    "- **影响**：\n",
    "  - 硬件：占用更多系统内存，但传输速度提高2-3倍\n",
    "  - 程序：减少数据传输时间\n",
    "  - 建议：系统内存充足时启用\n",
    "\n",
    "##### `buffer_count`\n",
    "- **作用**：并行传输的缓冲区数量\n",
    "- **影响**：\n",
    "  - 硬件：更多缓冲区占用更多内存\n",
    "  - 程序：提高传输并行度\n",
    "  - 调优：4-8个通常是最佳选择\n",
    "\n",
    "##### `buffer_size`\n",
    "- **作用**：每个缓冲区的大小（字节）\n",
    "- **影响**：\n",
    "  - 硬件：直接影响内存占用\n",
    "  - 程序：影响传输粒度\n",
    "  - 调优：1e8 (100MB) 通常是好的起点\n",
    "\n",
    "##### `max_in_cpu`\n",
    "- **作用**：CPU中允许的最大数据量\n",
    "- **影响**：\n",
    "  - 硬件：限制CPU内存使用\n",
    "  - 程序：超出限制时数据会进一步卸载到磁盘\n",
    "  - 调优：根据系统内存大小设置\n",
    "\n",
    "#### `offload_param` (仅ZeRO-3)\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,  // 必须是ZeRO-3\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true,\n",
    "      \"buffer_count\": 5,         // 通常比optimizer多1个\n",
    "      \"buffer_size\": 1e8,\n",
    "      \"max_in_cpu\": 1e9\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：卸载模型参数到CPU/NVMe\n",
    "- **限制**：❌ 只能在ZeRO-3中使用\n",
    "- **影响**：最激进的内存节省，但速度最慢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90234797",
   "metadata": {},
   "source": [
    "\n",
    "### 4.5 通信优化参数\n",
    "\n",
    "#### `allgather_bucket_size`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"allgather_bucket_size\": 5e8\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：AllGather操作的桶大小\n",
    "- **影响**：\n",
    "  - 硬件：影响网络带宽利用率\n",
    "  - 程序：影响参数收集效率\n",
    "  - 调优：网络带宽高时可以增大\n",
    "\n",
    "#### `reduce_bucket_size`\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"reduce_bucket_size\": 5e8\n",
    "  }\n",
    "}\n",
    "```\n",
    "- **作用**：Reduce操作的桶大小\n",
    "- **影响**：\n",
    "  - 硬件：影响梯度同步效率\n",
    "  - 程序：影响梯度聚合性能\n",
    "  - 调优：与allgather_bucket_size保持一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d56bce",
   "metadata": {},
   "source": [
    "\n",
    "### 4.6 混合精度参数\n",
    "\n",
    "#### `fp16`\n",
    "```json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"initial_scale_power\": 12,    // 初始损失缩放\n",
    "    \"loss_scale_window\": 1000,    // 损失缩放窗口\n",
    "    \"hysteresis\": 2,              // 缩放调整滞后\n",
    "    \"min_loss_scale\": 1           // 最小损失缩放\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**各参数详解：**\n",
    "\n",
    "##### `enabled`\n",
    "- **作用**：启用FP16混合精度训练\n",
    "- **影响**：\n",
    "  - 硬件：减少50%显存使用，提高训练速度\n",
    "  - 程序：可能出现数值不稳定\n",
    "  - 建议：现代GPU都应该启用\n",
    "\n",
    "##### `initial_scale_power`\n",
    "- **作用**：初始损失缩放因子 = 2^initial_scale_power\n",
    "- **影响**：\n",
    "  - 程序：防止梯度下溢\n",
    "  - 调优：12-16通常是好的选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411af3a",
   "metadata": {},
   "source": [
    "\n",
    "### 4.7 激活检查点参数\n",
    "\n",
    "#### `activation_checkpointing`\n",
    "```json\n",
    "{\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,    // 分割激活值\n",
    "    \"cpu_checkpointing\": true,        // CPU检查点\n",
    "    \"contiguous_memory_optimization\": false,  // 连续内存优化\n",
    "    \"number_checkpoints\": 1,          // 检查点数量\n",
    "    \"synchronize_checkpoint_boundary\": false,  // 同步检查点边界\n",
    "    \"profile\": false                  // 性能分析\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**各参数详解：**\n",
    "\n",
    "##### `partition_activations`\n",
    "- **作用**：在多GPU间分割激活值\n",
    "- **影响**：\n",
    "  - 硬件：减少每个GPU的激活值内存\n",
    "  - 程序：增加通信开销\n",
    "  - 建议：显存紧张时启用\n",
    "\n",
    "##### `cpu_checkpointing`\n",
    "- **作用**：将激活值检查点存储在CPU\n",
    "- **影响**：\n",
    "  - 硬件：大幅减少GPU显存，增加CPU内存使用\n",
    "  - 程序：增加CPU-GPU传输开销\n",
    "  - 权衡：内存 vs 速度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953db49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5 参数调优指南\n",
    "\n",
    "### 5.1 显存不足时的调优顺序\n",
    "```python\n",
    "# 按效果递增的顺序：\n",
    "1. 减少 train_micro_batch_size_per_gpu: 4→2→1\n",
    "2. 启用 activation_checkpointing\n",
    "3. 从 ZeRO-1 升级到 ZeRO-2\n",
    "4. 启用 offload_optimizer\n",
    "5. 升级到 ZeRO-3\n",
    "6. 启用 offload_param\n",
    "7. 减少 stage3_max_live_parameters\n",
    "8. 使用 cpu_checkpointing\n",
    "```\n",
    "\n",
    "### 5.2 速度优化的调优顺序\n",
    "```python\n",
    "# 按效果递增的顺序：\n",
    "1. 启用 overlap_comm\n",
    "2. 启用 contiguous_gradients  \n",
    "3. 增加 buffer_count: 4→6→8\n",
    "4. 优化 bucket_size 大小\n",
    "5. 禁用不必要的 offload\n",
    "6. 降低 ZeRO stage: 3→2→1\n",
    "7. 禁用 activation_checkpointing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd8481",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "DeepSpeed ZeRO = 让多个GPU分工合作，避免重复存储相同数据\n",
    "Offload = 把暂时不用的数据放到CPU内存或硬盘里\n",
    "\n",
    "选择原则：\n",
    "- 显存够 → 选ZeRO-1，速度快\n",
    "- 显存紧张 → 选ZeRO-2/3 + Offload，省显存\n",
    "- 显存很少 → 选ZeRO-3 + NVMe Offload，能跑就行\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda01eaa",
   "metadata": {},
   "source": [
    "# 5.模型微调 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeacd6",
   "metadata": {},
   "source": [
    "## 5.1 企业级AI训练系统核心注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0c521",
   "metadata": {},
   "source": [
    "## 注意事项1：分布式训练集成\n",
    "\n",
    "### 企业级程序优先需要满足分布式的原因：\n",
    "\n",
    "**成本效益考量**：\n",
    "- 单卡训练大模型可能需要80GB显存的A100，成本约$15,000/月\n",
    "- 4卡分布式可使用24GB的RTX 4090，总成本约$8,000/月，节省近50%成本\n",
    "- 企业为了充分利用现有GPU集群资源，避免硬件闲置造成的资源浪费\n",
    "\n",
    "**效率提升需求**：\n",
    "- 单卡训练70B参数模型需要15-20天\n",
    "- 8卡分布式训练可缩短至2-3天，大幅提升业务响应速度\n",
    "\n",
    "**可能遇到的企业场景**：\n",
    "```python\n",
    "# 当前代码支持的分布式配置\n",
    "deepspeed=\"ds_config.json\"  # DeepSpeed分布式框架\n",
    "per_device_train_batch_size=3  # 每卡批大小\n",
    "gradient_accumulation_steps=12  # 梯度累积\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **电商平台双11备战**：需要在1周内完成推荐算法升级，单卡无法满足时间要求\n",
    "- **金融机构风控模型更新**：监管要求30天内完成模型迭代，必须使用多卡加速\n",
    "- **医疗AI公司产品发布**：临床试验数据到位后，需要快速完成模型训练以赶上产品发布窗口\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98d60c",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项2：容错与恢复机制\n",
    "\n",
    "### 企业级程序必须具备容错能力的原因：\n",
    "\n",
    "**业务连续性保障**：\n",
    "- 企业训练任务通常持续数天至数周，任何中断都意味着巨大的时间和成本损失\n",
    "- 云服务器可能因维护、故障等原因重启，需要恢复能力\n",
    "- 多人协作环境下，误操作或网络波动频发\n",
    "\n",
    "**资源成本控制**：\n",
    "- GPU租用按小时计费，训练中断重来会造成直接经济损失\n",
    "- 大型训练任务重启可能需要重新排队等待GPU资源\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 自动检测并恢复训练\n",
    "last_checkpoint_dir = \"/root/autodl-tmp/finetune_output/checkpoint-30\"\n",
    "if os.path.exists(last_checkpoint_dir):\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint_dir)\n",
    "else:\n",
    "    trainer.train()\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **云服务商维护窗口**：阿里云、腾讯云定期维护导致实例重启，系统自动从最新checkpoint恢复\n",
    "- **网络波动中断**：跨地域训练时网络不稳定，系统能够自动重连并继续训练\n",
    "- **硬件故障替换**：某块GPU故障需要更换，系统可以在新硬件上恢复训练进度\n",
    "- **人为操作失误**：运维人员误关闭训练进程，系统能够快速恢复到中断前状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00834b96",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项3：智能早停机制\n",
    "\n",
    "### 企业级程序需要智能早停的原因：\n",
    "\n",
    "**成本控制需求**：\n",
    "- 过度训练不仅浪费GPU资源，还可能导致模型过拟合，影响业务效果\n",
    "- 企业需要在模型性能和训练成本之间找到最优平衡点\n",
    "- 自动化决策减少人工监控成本\n",
    "\n",
    "**业务效率优化**：\n",
    "- 及时停止无效训练，释放资源给其他项目\n",
    "- 避免模型性能退化，确保业务指标稳定\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 智能早停配置\n",
    "EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,      # 连续5次评估无改善\n",
    "    early_stopping_threshold=0.002  # 改善阈值0.002\n",
    ")\n",
    "load_best_model_at_end=True  # 自动加载最佳模型\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "\n",
    "**场景1 - 客服机器人优化项目**：\n",
    "- 训练进行到第15轮时，客户满意度指标已达到95%\n",
    "- 继续训练到第25轮，满意度反而下降到92%\n",
    "- 早停机制在第20轮自动停止，保存95%满意度的最佳模型\n",
    "- **节省成本**：避免10轮无效训练，节省约30%的GPU费用\n",
    "\n",
    "**场景2 - 金融风控模型迭代**：\n",
    "- 欺诈检测准确率在第8轮达到98.5%峰值\n",
    "- 后续训练准确率波动在98.2%-98.4%之间\n",
    "- 早停机制识别无显著改善，自动停止并保存最佳模型\n",
    "- **业务价值**：避免过拟合导致的误报率上升，保护客户体验\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1998c",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项4：内存优化与资源管理\n",
    "\n",
    "### 企业级程序需要精细化资源管理的原因：\n",
    "\n",
    "**硬件成本最大化利用**：\n",
    "- 企业GPU资源有限，需要在有限硬件上训练更大的模型\n",
    "- 内存优化技术可以将原本需要80GB显存的任务压缩到24GB显存完成\n",
    "\n",
    "**可能遇到的企业场景**：\n",
    "```python\n",
    "# 内存优化策略\n",
    "gradient_checkpointing_enable()  # 梯度检查点\n",
    "torch_dtype=torch.float16       # 半精度训练\n",
    "zero_optimization: stage=2      # ZeRO优化器状态分片\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **创业公司资源受限**：只有4张RTX 4090，通过内存优化成功训练130亿参数模型\n",
    "- **科研院所预算紧张**：使用现有设备完成原本需要更高端硬件的研究项目\n",
    "- **边缘计算部署**：在工厂现场的边缘服务器上完成模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f23ce3",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项5：混合数据集处理能力\n",
    "\n",
    "### 企业级程序必须支持多源异构数据的原因：\n",
    "\n",
    "**业务数据复杂性**：\n",
    "- 企业实际数据来源多样：客服对话、产品文档、用户反馈、历史记录等\n",
    "- 数据格式不统一：单轮问答、多轮对话、结构化表格、非结构化文本\n",
    "- 需要统一处理管道整合所有数据源，避免数据孤岛\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 自动检测和处理不同数据格式\n",
    "dataset_type=\"auto\"  # 自动识别数据类型\n",
    "def _process_single_turn(self, item):  # 处理单轮对话\n",
    "def _process_multi_turn(self, item):   # 处理多轮对话\n",
    "TRAIN_DATA_PATHS = [多个数据源路径]    # 支持多文件输入\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **银行智能客服升级**：整合电话录音转录(多轮)、在线客服记录(多轮)、FAQ文档(单轮)、产品说明书(单轮)\n",
    "- **电商平台商品推荐**：融合用户评论(单轮)、客服咨询(多轮)、商品描述(单轮)、购买对话(多轮)\n",
    "- **医疗AI诊断系统**：结合病历记录(多轮)、医学文献(单轮)、医患对话(多轮)、诊断指南(单轮)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b417e",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项6：参数高效微调(PEFT)架构\n",
    "\n",
    "### 企业级程序需要PEFT的原因：\n",
    "\n",
    "**快速业务适应**：\n",
    "- 基础大模型训练成本高昂，企业无法为每个业务场景重新训练\n",
    "- 需要在预训练模型基础上快速适应特定业务需求\n",
    "- 降低部署和维护成本，支持多业务线并行开发\n",
    "\n",
    "**模型管理便利性**：\n",
    "- 一个基础模型+多个LoRA适配器，比维护多个完整模型更经济\n",
    "- 便于版本控制和模型更新\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# LoRA高效微调配置\n",
    "lora_config = LoraConfig(\n",
    "    r=24,                    # 低秩分解维度\n",
    "    lora_alpha=48,          # 缩放因子\n",
    "    target_modules=[...],   # 目标模块\n",
    "    lora_dropout=0.1        # 防过拟合\n",
    ")\n",
    "# 只训练0.1%的参数，节省99.9%的训练成本\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **多品牌集团公司**：基础客服模型+各品牌专属LoRA适配器，快速部署不同品牌的客服系统\n",
    "- **SaaS服务提供商**：为不同行业客户提供定制化AI服务，每个客户一个LoRA适配器\n",
    "- **跨国企业本地化**：基础模型+各国语言文化LoRA适配器，快速适应不同市场需求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ac441",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项7：全面的模型版本管理\n",
    "\n",
    "### 企业级程序需要完整版本控制的原因：\n",
    "\n",
    "**业务风险控制**：\n",
    "- 新模型上线可能出现性能回退，需要快速回滚到稳定版本\n",
    "- 监管合规要求，需要完整的模型开发和部署记录\n",
    "- A/B测试需要同时维护多个模型版本\n",
    "\n",
    "**团队协作需求**：\n",
    "- 多个团队并行开发，需要清晰的版本分支管理\n",
    "- 模型迭代历史追溯，便于问题定位和经验总结\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 多版本模型自动保存\n",
    "final_lora_model/          # 最终训练版本\n",
    "best_complete_model/       # 最佳性能版本\n",
    "checkpoint-{step}/         # 训练过程检查点\n",
    "best_model_info.txt       # 最佳模型元信息\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **金融风控系统更新**：新模型误报率上升，立即回滚到上一稳定版本，避免业务损失\n",
    "- **内容审核平台升级**：监管政策变化后，需要追溯模型决策依据，提供完整版本记录\n",
    "- **智能制造质检系统**：产线调整后模型精度下降，快速定位到最适合当前产线的历史版本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8616b7d",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项8：实时监控与可观测性\n",
    "\n",
    "### 企业级程序需要全方位监控的原因：\n",
    "\n",
    "**运营风险预警**：\n",
    "- 训练过程异常需要及时发现和处理\n",
    "- 资源使用情况监控，避免成本超支\n",
    "- 模型性能趋势分析，预测业务影响\n",
    "\n",
    "**团队协作透明化**：\n",
    "- 项目进度实时可见，便于资源调配和决策\n",
    "- 问题排查有据可依，提高故障处理效率\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 多维度监控配置\n",
    "report_to=[\"tensorboard\"]           # 可视化监控\n",
    "logging_steps=10                    # 高频日志记录\n",
    "eval_steps=300                      # 定期性能评估\n",
    "AggressiveMemoryOptimizationCallback # 资源使用监控\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **7×24小时运营中心**：夜班运维通过监控大屏实时了解所有训练任务状态\n",
    "- **项目管理决策**：CTO通过训练进度决定是否需要增加GPU资源或调整项目优先级\n",
    "- **成本控制审计**：财务部门通过资源使用报告进行成本分析和预算规划\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e68b3",
   "metadata": {},
   "source": [
    "\n",
    "## 注意事项9：企业级安全与合规\n",
    "\n",
    "### 企业级程序需要安全合规机制的原因：\n",
    "\n",
    "**数据安全保护**：\n",
    "- 训练数据可能包含敏感信息，需要安全处理\n",
    "- 模型权重和检查点需要加密存储\n",
    "- 访问控制和审计日志记录\n",
    "\n",
    "**可能遇到的企业场景及当前技术解决方案**：\n",
    "```python\n",
    "# 安全相关配置\n",
    "trust_remote_code=True              # 代码安全验证\n",
    "remove_unused_columns=False         # 数据完整性保护\n",
    "save_total_limit=10                 # 存储空间管理\n",
    "```\n",
    "\n",
    "**实际应用场景**：\n",
    "- **医疗数据训练**：患者隐私保护，训练过程符合HIPAA合规要求\n",
    "- **金融机构模型开发**：满足央行监管要求，完整的模型开发审计链\n",
    "- **政府项目交付**：符合国家信息安全等级保护要求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec33e56",
   "metadata": {},
   "source": [
    "## 总结：企业级AI训练系统的9大核心注意事项\n",
    "\n",
    "1. **分布式训练集成** - 成本效益与效率提升\n",
    "2. **容错与恢复机制** - 业务连续性保障  \n",
    "3. **智能早停机制** - 成本控制与效率优化\n",
    "4. **内存优化与资源管理** - 硬件资源最大化利用\n",
    "5. **混合数据集处理能力** - 业务数据复杂性应对\n",
    "6. **参数高效微调架构** - 快速业务适应与成本控制\n",
    "7. **全面的模型版本管理** - 风险控制与团队协作\n",
    "8. **实时监控与可观测性** - 运营风险预警与透明化管理\n",
    "9. **企业级安全与合规** - 数据安全与监管要求\n",
    "\n",
    "这些注意事项构成了完整的企业级AI训练系统架构，确保在**安全、可靠、高效、经济**的前提下满足各种复杂的企业业务场景需求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed232fb5",
   "metadata": {},
   "source": [
    "## 5.2 必备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9b75c",
   "metadata": {},
   "source": [
    "## 1. 监控部分：TensorBoard实现\n",
    "\n",
    "### TensorBoard是什么？\n",
    "TensorBoard是Google开发的可视化工具，专门用于机器学习实验的监控和分析。它能够实时展示训练过程中的各种指标，帮助开发者理解模型行为。\n",
    "\n",
    "### 在代码中的实现：\n",
    "```python\n",
    "# 启用TensorBoard监控\n",
    "report_to=[\"tensorboard\"],              # 指定使用TensorBoard作为监控后端\n",
    "logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),  # 设置日志存储路径\n",
    "logging_steps=10,                       # 每10步记录一次日志\n",
    "```\n",
    "\n",
    "### 使用场景与价值：\n",
    "**企业场景1 - 模型性能实时监控**：\n",
    "- **问题**：训练工程师需要实时了解loss变化趋势，判断是否需要调整超参数\n",
    "- **解决方案**：通过TensorBoard实时查看train_loss和eval_loss曲线\n",
    "- **业务价值**：及时发现训练异常，避免浪费GPU资源\n",
    "\n",
    "**企业场景2 - 团队协作与汇报**：\n",
    "- **问题**：项目经理需要向上级汇报训练进度，技术团队需要共享实验结果\n",
    "- **解决方案**：团队成员通过浏览器访问TensorBoard界面，查看统一的训练报告\n",
    "- **业务价值**：提高沟通效率，支持数据驱动决策\n",
    "\n",
    "### 具体使用方法：\n",
    "```bash\n",
    "# 启动TensorBoard服务\n",
    "tensorboard --logdir=/root/autodl-tmp/finetune_output/logs --port=6006\n",
    "\n",
    "# 浏览器访问\n",
    "http://localhost:6006\n",
    "```\n",
    "\n",
    "**可监控的关键指标**：\n",
    "- `train_loss`: 训练损失变化\n",
    "- `eval_loss`: 验证损失变化  \n",
    "- `learning_rate`: 学习率调度\n",
    "- `grad_norm`: 梯度范数（检测梯度爆炸）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1a090",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 早停部分：回调机制实现\n",
    "\n",
    "### 回调机制是什么？\n",
    "回调机制是一种设计模式，允许在训练过程的特定时刻执行自定义逻辑。HuggingFace Trainer通过回调系统提供了灵活的训练控制能力。\n",
    "\n",
    "### 在代码中的实现：\n",
    "```python\n",
    "# 早停回调配置\n",
    "EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,      # 容忍5次连续无改善\n",
    "    early_stopping_threshold=0.002  # 改善阈值0.002\n",
    ")\n",
    "\n",
    "# 训练器中注册回调\n",
    "callbacks=[\n",
    "    EarlyStoppingCallback(...),\n",
    "    AggressiveMemoryOptimizationCallback(),\n",
    "    best_and_last_callback\n",
    "]\n",
    "```\n",
    "\n",
    "### 工作原理：\n",
    "```python\n",
    "# 早停逻辑伪代码\n",
    "class EarlyStoppingCallback:\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        current_metric = metrics[args.metric_for_best_model]  # eval_loss\n",
    "        \n",
    "        if self.best_metric is None or current_metric < self.best_metric - threshold:\n",
    "            self.best_metric = current_metric\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "        if self.patience_counter >= self.patience:\n",
    "            control.should_training_stop = True  # 触发训练停止\n",
    "```\n",
    "\n",
    "### 企业应用场景：\n",
    "**场景1 - 成本控制**：\n",
    "- **问题**：GPU租用费用高昂，过度训练浪费资源\n",
    "- **解决方案**：模型性能不再提升时自动停止训练\n",
    "- **实际效果**：某电商推荐系统项目节省40%训练成本\n",
    "\n",
    "**场景2 - 防止过拟合**：\n",
    "- **问题**：模型在验证集上性能开始下降\n",
    "- **解决方案**：early_stopping_threshold=0.002确保只有显著改善才继续训练\n",
    "- **业务价值**：保证模型泛化能力，避免生产环境性能下降\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e06b29",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 容错恢复：Checkpoint机制\n",
    "\n",
    "### Checkpoint机制是什么？\n",
    "Checkpoint是训练过程中定期保存的模型状态快照，包含模型权重、优化器状态、训练步数等信息。\n",
    "\n",
    "### 在代码中的实现：\n",
    "```python\n",
    "# 自动检测和恢复\n",
    "last_checkpoint_dir = \"/root/autodl-tmp/finetune_output/checkpoint-30\"\n",
    "if os.path.exists(last_checkpoint_dir):\n",
    "    print(f\"从checkpoint {last_checkpoint_dir} 恢复训练...\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint_dir, ignore_keys_for_eval=[\"*\"])\n",
    "else:\n",
    "    print(\"从头开始训练...\")\n",
    "    trainer.train()\n",
    "\n",
    "# Checkpoint保存策略\n",
    "save_strategy=\"steps\",\n",
    "save_steps=300,                     # 每300步保存一次\n",
    "save_total_limit=10,               # 最多保存10个checkpoint\n",
    "```\n",
    "\n",
    "### 恢复机制原理：\n",
    "```python\n",
    "# Checkpoint包含的信息\n",
    "checkpoint/\n",
    "├── pytorch_model.bin              # 模型权重\n",
    "├── optimizer.pt                   # 优化器状态\n",
    "├── scheduler.pt                   # 学习率调度器状态\n",
    "├── trainer_state.json            # 训练状态(步数、epoch等)\n",
    "└── training_args.bin              # 训练参数\n",
    "```\n",
    "\n",
    "### 企业应用场景：\n",
    "**场景1 - 云服务器维护**：\n",
    "- **问题**：阿里云通知凌晨2点维护重启服务器\n",
    "- **解决方案**：训练自动从最新checkpoint恢复，无需人工干预\n",
    "- **业务价值**：保证7×24小时连续训练能力\n",
    "\n",
    "**场景2 - 硬件故障处理**：\n",
    "- **问题**：某张GPU故障需要更换\n",
    "- **解决方案**：在新硬件上自动恢复到故障前状态\n",
    "- **实际效果**：某金融公司风控模型训练，硬件故障仅损失2小时进度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea784c2",
   "metadata": {},
   "source": [
    "## 4. 模型管理：多版本自动保存\n",
    "\n",
    "### 智能模型保存策略\n",
    "```python\n",
    "class SaveBestAndLastModelCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # 检查是否为新的最佳模型\n",
    "        if metrics[metric_to_check] < self.best_metric:\n",
    "            self.best_metric = metrics[metric_to_check]\n",
    "            self.best_model_checkpoint = f\"checkpoint-{state.global_step}\"\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # 保存最终模型和最佳模型\n",
    "        self._save_final_model()\n",
    "        self._save_best_complete_model()\n",
    "```\n",
    "\n",
    "### 版本管理结构\n",
    "```\n",
    "finetune_output/\n",
    "├── checkpoint-300/              # 训练检查点\n",
    "├── checkpoint-600/\n",
    "├── best_lora_model/            # 最佳LoRA模型\n",
    "├── best_complete_model/        # 最佳完整模型\n",
    "├── final_lora_model/           # 最终LoRA模型\n",
    "├── final_complete_model/       # 最终完整模型\n",
    "└── best_model_info.txt         # 最佳模型元信息\n",
    "```\n",
    "\n",
    "### 企业应用场景：\n",
    "**场景 - 电商推荐系统A/B测试**：\n",
    "- **需求**：同时测试3个模型版本的效果\n",
    "- **解决方案**：自动保存最佳模型、最终模型、特定检查点\n",
    "- **部署策略**：\n",
    "  - 生产环境：使用best_complete_model\n",
    "  - 测试环境：使用final_complete_model\n",
    "  - 回滚备份：使用历史checkpoint\n",
    "- **业务价值**：支持快速版本切换，降低上线风险\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5faa442",
   "metadata": {},
   "source": [
    "当前在autodl上租赁服务器，硬件信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b048d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528163755464.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b810485",
   "metadata": {},
   "source": [
    "```python\n",
    "# 创建一个新环境\n",
    "conda create -n torch25 python=3.11 -y\n",
    "conda activate torch25\n",
    "\n",
    "# 安装旧版本的PyTorch (2.5.1)\n",
    "pip install torch==2.5.1 torchvision==0.16.1\n",
    "\n",
    "# 安装其他依赖\n",
    "pip install transformers deepspeed peft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff1742",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250520192710061.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639ac5f",
   "metadata": {},
   "source": [
    "## 5.3 微调主代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e65bb",
   "metadata": {},
   "source": [
    "```python\n",
    "# 基础库导入\n",
    "import os                   # 操作系统接口，用于文件路径操作（创建文件夹、检查文件是否存在等）\n",
    "import json                 # JSON数据处理库，用于读取和解析训练数据文件\n",
    "import torch                # PyTorch深度学习框架核心库，提供神经网络训练的基础功能\n",
    "from torch.utils.data import Dataset  # PyTorch数据集基类，用于创建自定义数据加载器\n",
    "import glob                 # 文件路径匹配工具，可以批量查找符合条件的文件\n",
    "\n",
    "# DeepSpeed相关导入（解决PyTorch 2.6加载兼容性问题）\n",
    "# DeepSpeed是微软开发的深度学习优化库，可以大幅减少显存占用\n",
    "import deepspeed.runtime.fp16.loss_scaler    # DeepSpeed的半精度浮点数损失缩放器\n",
    "import deepspeed.runtime.zero.config         # DeepSpeed的ZeRO内存优化配置\n",
    "from torch.serialization import add_safe_globals  # PyTorch安全序列化功能，防止加载恶意代码\n",
    "\n",
    "# Transformers库核心组件\n",
    "# Transformers是Hugging Face开发的预训练模型库，包含GPT、BERT等各种AI模型\n",
    "from transformers import (\n",
    "    AutoTokenizer,          # 自动分词器：将文本转换为模型能理解的数字序列（token）\n",
    "    AutoModelForCausalLM,   # 自动因果语言模型：专门用于文本生成任务的模型类型\n",
    "    Trainer,                # 训练器：封装了完整训练流程的高级接口，简化训练代码\n",
    "    TrainingArguments,      # 训练参数配置类：包含学习率、批大小、训练轮数等所有训练设置\n",
    "    DataCollatorForLanguageModeling,  # 语言模型数据整理器：将多个文本样本组合成训练批次\n",
    "    EarlyStoppingCallback,  # 早停回调：防止过拟合，当模型性能不再提升时自动停止训练\n",
    "    TrainerCallback         # 自定义回调基类：用于在训练过程中执行自定义操作\n",
    ")\n",
    "\n",
    "# PEFT库组件（用于高效参数微调）\n",
    "# PEFT (Parameter-Efficient Fine-Tuning) 是一种只训练少量参数的微调技术\n",
    "# 相比传统全参数微调，PEFT可以用更少的显存和时间达到相似效果\n",
    "from peft import (\n",
    "    get_peft_model,         # 获取PEFT模型：将普通模型转换为参数高效微调模型\n",
    "    LoraConfig,             # LoRA配置：LoRA是最流行的PEFT方法，通过低秩矩阵分解减少参数\n",
    "    TaskType,               # 任务类型枚举：指定模型要执行的任务（文本生成、分类等）\n",
    "    PeftModel               # PEFT模型基类：用于加载和操作参数高效微调模型\n",
    ")\n",
    "\n",
    "import deepspeed            # DeepSpeed分布式训练框架：提供内存优化和多GPU训练功能\n",
    "\n",
    "# 定义模型路径和数据路径\n",
    "# 这些路径告诉程序在哪里找到预训练模型、训练数据，以及把结果保存到哪里\n",
    "MODEL_PATH = \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B\"  # 基础预训练模型的存储路径\n",
    "# 更新为SFT数据路径\n",
    "# 训练数据路径列表：可以同时使用多个数据文件，程序会自动合并\n",
    "TRAIN_DATA_PATHS = [\"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_1w.jsonl\",\"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_3k.jsonl\"]\n",
    "VAL_DATA_PATH = \"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_1_1k.jsonl\"  # 验证数据集：用于评估模型训练效果\n",
    "OUTPUT_DIR = \"/root/autodl-tmp/finetune_output\"              # 输出目录：训练过程中的模型、日志等文件保存位置\n",
    "\n",
    "# 创建输出目录(如果不存在)\n",
    "# exist_ok=True 表示如果目录已经存在也不会报错\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 加载分词器\n",
    "# 分词器的作用是将人类可读的文本转换为模型可以处理的数字序列\n",
    "# trust_remote_code=True 允许加载包含自定义代码的模型（某些模型需要特殊的分词逻辑）\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "# 处理没有pad_token的分词器\n",
    "# pad_token用于将不同长度的文本填充到相同长度，这样可以批量处理\n",
    "# 如果分词器没有专门的填充标记，就使用文本结束标记代替\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 使用eos_token作为填充标记\n",
    "\n",
    "# 自定义数据集类，用于处理SFT格式数据\n",
    "# SFT (Supervised Fine-Tuning) 是监督微调的意思，使用问答对数据训练模型\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_paths, tokenizer, max_length=1024):\n",
    "        \"\"\"\n",
    "        初始化SFT数据集\n",
    "        \n",
    "        这个函数的作用：\n",
    "        1. 读取JSONL格式的训练数据文件（每行一个JSON对象）\n",
    "        2. 将每条数据转换为模型训练所需的格式\n",
    "        3. 处理文本分词和长度限制\n",
    "        \n",
    "        参数解释：\n",
    "        data_paths: 数据文件路径，可以是单个文件路径（字符串）或多个文件路径（列表）\n",
    "        tokenizer: 分词器对象，用于将文本转换为token ID\n",
    "        max_length: 文本序列的最大长度，超过这个长度的文本会被截断（1024是常用值）\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer      # 保存分词器，后续处理数据时使用\n",
    "        self.max_length = max_length    # 保存最大长度限制\n",
    "        self.examples = []              # 用于存储所有处理后的训练样本\n",
    "\n",
    "        # 确保data_paths是列表格式，方便统一处理\n",
    "        # 如果传入的是单个字符串路径，转换为包含一个元素的列表\n",
    "        if isinstance(data_paths, str):\n",
    "            data_paths = [data_paths]\n",
    "\n",
    "        # 初始化计数器，用于统计数据加载情况\n",
    "        total_lines = 0      # 文件总行数\n",
    "        processed_lines = 0  # 成功处理的行数\n",
    "\n",
    "        # 逐个处理每个数据文件\n",
    "        for data_path in data_paths:\n",
    "            try:\n",
    "                # 以UTF-8编码打开文件，确保中文字符正确显示\n",
    "                with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                    # 逐行读取文件内容（JSONL格式：每行一个JSON对象）\n",
    "                    for line in f:\n",
    "                        total_lines += 1\n",
    "                        try:\n",
    "                            # 解析JSON格式的数据\n",
    "                            item = json.loads(line)\n",
    "                            # 处理SFT格式数据\n",
    "                            # SFT数据通常包含三个字段：instruction（指令）、input（输入）、output（期望输出）\n",
    "                            instruction = item.get(\"instruction\", \"\")  # 获取任务指令，如果没有则为空字符串\n",
    "                            input_text = item.get(\"input\", \"\")         # 获取输入内容\n",
    "                            output = item.get(\"output\", \"\")            # 获取期望的输出内容\n",
    "\n",
    "                            # 构建提示模板\n",
    "                            # 这个模板告诉模型如何理解和回应用户的问题\n",
    "                            if input_text:\n",
    "                                # 如果有输入内容，将指令和输入组合，用换行符分隔\n",
    "                                prompt = f\"{instruction}\\n\\n{input_text}\\n\\n\"\n",
    "                            else:\n",
    "                                # 如果没有输入内容，只使用指令\n",
    "                                prompt = f\"{instruction}\\n\\n\"\n",
    "\n",
    "                            # 构建完整的训练文本：提示 + 期望输出 + 结束标记\n",
    "                            # 结束标记告诉模型这里是文本的结尾\n",
    "                            full_text = prompt + output + tokenizer.eos_token\n",
    "\n",
    "                            # 将处理后的数据添加到样本列表\n",
    "                            self.examples.append({\n",
    "                                \"prompt\": prompt,        # 提示部分（模型不需要学习生成这部分）\n",
    "                                \"full_text\": full_text   # 完整文本（用于训练）\n",
    "                            })\n",
    "                            processed_lines += 1\n",
    "                        except Exception as e:\n",
    "                            # 如果某行数据解析失败，打印错误信息但继续处理其他行\n",
    "                            print(f\"处理第{total_lines}行时出错: {e}\")\n",
    "            except Exception as e:\n",
    "                # 如果文件打开失败，打印错误信息\n",
    "                print(f\"打开文件{data_path}时出错: {e}\")\n",
    "\n",
    "        # 打印数据加载统计信息，让用户了解数据处理情况\n",
    "        print(f\"数据集加载 - 总行数: {total_lines}, 成功处理: {processed_lines}, 最终样本数: {len(self.examples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中样本的总数，PyTorch需要这个信息来组织训练\"\"\"\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取指定索引的训练样本\n",
    "        \n",
    "        这个方法会被PyTorch的DataLoader自动调用，用于获取批次数据\n",
    "        每次训练时，PyTorch会调用这个方法来获取一个样本\n",
    "        \n",
    "        参数：\n",
    "        idx: 样本索引（第几个样本）\n",
    "        \n",
    "        返回：\n",
    "        包含input_ids、attention_mask、labels的字典，这是模型训练需要的格式\n",
    "        \"\"\"\n",
    "        # 获取指定索引的样本\n",
    "        example = self.examples[idx]\n",
    "        full_text = example[\"full_text\"]  # 完整的训练文本\n",
    "        prompt = example[\"prompt\"]        # 提示部分\n",
    "\n",
    "        # 使用分词器将文本转换为token ID\n",
    "        # token ID是模型能理解的数字序列，每个数字代表一个词或字符\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,                    # 要编码的文本\n",
    "            truncation=True,              # 启用截断：如果文本太长会被截断到max_length\n",
    "            max_length=self.max_length,   # 最大长度限制\n",
    "            padding=\"max_length\",         # 填充到最大长度：短文本会用pad_token填充\n",
    "            return_tensors=\"pt\"           # 返回PyTorch张量格式\n",
    "        )\n",
    "\n",
    "        # 提取编码结果\n",
    "        input_ids = encodings[\"input_ids\"][0]        # token ID序列：文本转换成的数字序列\n",
    "        attention_mask = encodings[\"attention_mask\"][0]  # 注意力掩码：告诉模型哪些是真实内容，哪些是填充\n",
    "\n",
    "        # 创建训练标签，用于计算损失\n",
    "        # 在因果语言模型中，标签就是输入序列（模型要学会预测下一个词）\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 计算提示部分的长度\n",
    "        # 我们不希望模型学习如何生成提示，只学习如何生成回答\n",
    "        prompt_tokens = self.tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        prompt_len = len(prompt_tokens)\n",
    "\n",
    "        # 将提示部分的标签设为-100，这样在计算损失时会被忽略\n",
    "        # -100是PyTorch中的特殊值，表示在计算损失时忽略这些位置\n",
    "        # 这样模型只会学习生成回答部分，不会学习生成提示\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        # 返回训练所需的所有数据\n",
    "        return {\n",
    "            \"input_ids\": input_ids,           # 输入的token ID序列\n",
    "            \"attention_mask\": attention_mask, # 注意力掩码（区分真实内容和填充）\n",
    "            \"labels\": labels                  # 训练标签（告诉模型应该输出什么）\n",
    "        }\n",
    "\n",
    "# 加载训练和验证数据集\n",
    "# 创建数据集实例，这些对象会自动处理数据加载和预处理\n",
    "train_dataset = SFTDataset(TRAIN_DATA_PATHS, tokenizer)  # 使用列表形式的数据路径创建训练集\n",
    "eval_dataset = SFTDataset(VAL_DATA_PATH, tokenizer)      # 创建验证集，用于评估模型性能\n",
    "\n",
    "\n",
    "# 定义LoRA配置\n",
    "# LoRA (Low-Rank Adaptation) 是一种参数高效的微调方法\n",
    "# 它不修改原始模型权重，而是添加小的\"适配器\"层，大幅减少需要训练的参数\n",
    "lora_config = LoraConfig(\n",
    "    # target_modules: 指定要应用LoRA的模块名称\n",
    "    # 这些是Transformer模型中的关键组件：注意力机制和前馈网络\n",
    "    # q_proj, k_proj, v_proj, o_proj 是注意力机制的四个投影层\n",
    "    # gate_proj, down_proj, up_proj 是前馈网络的三个层\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型（用于文本生成，模型只能看到前面的内容）\n",
    "    r=32,                   # LoRA的秩（rank）：控制适配器的大小\n",
    "                           # 数值越大，适配器越大，表达能力越强，但参数也越多\n",
    "                           # 32是一个在性能和效率之间平衡的常用值\n",
    "    lora_alpha=64,          # LoRA的缩放因子：控制LoRA权重对最终结果的影响程度\n",
    "                           # 通常设置为r的2倍，这里64 = 32 * 2\n",
    "    lora_dropout=0.15,      # LoRA层的dropout率：随机丢弃15%的连接，防止过拟合\n",
    "    bias=\"none\",            # 偏置参数处理方式：\"none\"表示不训练偏置参数\n",
    "                           # 这进一步减少了需要训练的参数数量\n",
    ")\n",
    "\n",
    "# 修改训练参数配置\n",
    "# TrainingArguments包含了训练过程中的所有重要设置，类似于训练的\"配方\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # 输出目录：模型检查点、日志等文件的保存位置\n",
    "    per_device_train_batch_size=3,          # 每个GPU的训练批大小：一次处理3个样本\n",
    "                                            # 较小的值减少显存占用，但可能影响训练稳定性\n",
    "    per_device_eval_batch_size=3,           # 每个GPU的评估批大小：评估时一次处理3个样本\n",
    "    eval_accumulation_steps=3,              # 评估时的梯度累积步数：累积3个批次再计算指标\n",
    "    gradient_accumulation_steps=16,          # 训练时的梯度累积步数：累积16个小批次模拟大批次\n",
    "                                            # 实际批大小 = per_device_train_batch_size × gradient_accumulation_steps = 3×16=48\n",
    "    num_train_epochs=70,                    # 训练轮数：整个数据集被训练70次\n",
    "                                            # 这是一个相对较多的设置，适合小数据集的充分训练\n",
    "    #max_steps=3000,                         # 最大训练步数（被注释掉）\n",
    "                                            # 如果设置了max_steps，会覆盖num_train_epochs\n",
    "    learning_rate=5e-6,                     # 初始学习率：控制参数更新的步长\n",
    "                                            # 5e-6 (0.000005) 是一个较小的学习率，适合微调预训练模型\n",
    "    warmup_ratio=0.1,                        # 预热阶段占比：前10%的训练步数用于学习率预热\n",
    "                                            # 预热期间学习率从0逐渐增加到设定值，有助于训练稳定\n",
    "    fp16=True,                              # 启用FP16混合精度训练：使用16位浮点数代替32位\n",
    "                                            # 可以减少一半显存占用并加速训练，但可能略微影响精度\n",
    "    eval_strategy=\"steps\",                  # 评估策略：按训练步数进行评估（而不是按轮数）\n",
    "    eval_steps=300,                         # 每300步评估一次模型性能\n",
    "    do_eval=True,                           # 启用评估：在训练过程中定期评估模型性能\n",
    "    save_strategy=\"steps\",                  # 保存策略：按训练步数保存检查点\n",
    "    save_steps=300,                         # 每300步保存一次模型检查点\n",
    "    logging_steps=10,                       # 每10步记录一次训练日志（损失、学习率等）\n",
    "    save_total_limit=10,                    # 最多保存10个检查点\n",
    "                                            # 超过限制时会删除最旧的检查点以节省磁盘空间\n",
    "    remove_unused_columns=False,            # 保留数据中的所有列，不自动删除未使用的列\n",
    "    load_best_model_at_end=True,            # 训练结束时自动加载性能最佳的模型\n",
    "    metric_for_best_model=\"eval_loss\",      # 使用验证损失作为最佳模型的评判标准\n",
    "    greater_is_better=False,                # 损失越小越好（False表示指标越小越好）\n",
    "    deepspeed=\"ds_config.json\",             # DeepSpeed配置文件路径\n",
    "                                            # DeepSpeed提供内存优化和分布式训练功能\n",
    "    report_to=[\"tensorboard\"],              # 使用TensorBoard记录训练过程，可以可视化训练曲线\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),  # TensorBoard日志保存路径\n",
    "    max_grad_norm=0.5,                      # 梯度裁剪阈值：防止梯度爆炸\n",
    "                                            # 当梯度范数超过0.5时会被缩放到0.5\n",
    "    weight_decay=0.03,                      # 权重衰减：L2正则化系数，防止过拟合\n",
    "                                            # 0.03表示对权重施加适度的衰减惩罚\n",
    "    lr_scheduler_type=\"cosine\",             # 学习率调度器类型：余弦退火\n",
    "                                            # 学习率会按余弦函数逐渐降低，有助于模型收敛\n",
    ")\n",
    "\n",
    "# 创建数据整理器，用于批处理和填充\n",
    "# DataCollator负责将多个训练样本组合成批次，并处理长度不一致的问题\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,    # 使用前面定义的分词器\n",
    "    mlm=False               # mlm=False表示使用因果语言模型模式（用于文本生成）\n",
    "                           # 因果语言模型只能看到当前位置之前的内容，适合生成任务\n",
    "                           # mlm=True是掩码语言模型模式（如BERT），用于理解任务\n",
    ")\n",
    "\n",
    "# 修改后的模型加载和准备流程\n",
    "def create_and_prepare_model():\n",
    "    \"\"\"\n",
    "    创建并准备用于训练的模型\n",
    "    \n",
    "    这个函数执行以下步骤：\n",
    "    1. 从指定路径加载预训练的基础模型\n",
    "    2. 配置模型以减少显存占用（启用梯度检查点）\n",
    "    3. 应用LoRA配置，将普通模型转换为参数高效微调模型\n",
    "    \n",
    "    返回：\n",
    "    配置好的PEFT模型，可以直接用于训练\n",
    "    \"\"\"\n",
    "    # 加载模型到CPU，之后DeepSpeed会处理GPU分配\n",
    "    # 不直接加载到GPU是为了让DeepSpeed更好地管理显存分配\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,                 # 预训练模型的路径\n",
    "        torch_dtype=torch.float16,  # 使用FP16数据类型减少显存占用\n",
    "                                   # FP16使用16位浮点数，相比FP32可以节省一半显存\n",
    "        device_map=None,            # 不使用Hugging Face的自动设备映射\n",
    "                                   # 让DeepSpeed来处理设备分配，避免冲突\n",
    "        low_cpu_mem_usage=True,     # 启用低CPU内存使用模式\n",
    "                                   # 在加载大模型时减少CPU内存占用\n",
    "        trust_remote_code=True      # 允许执行模型中的自定义代码\n",
    "                                   # 某些模型（如DeepSeek）可能包含自定义的模型架构代码\n",
    "    )\n",
    "\n",
    "    # 启用梯度检查点以节省内存（牺牲少量计算性能换取更多内存）\n",
    "    # 梯度检查点是一种内存优化技术：不保存所有中间激活值，而是在反向传播时重新计算\n",
    "    # 这样可以大幅减少显存占用，但会增加一些计算时间\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 应用LoRA配置，将模型转换为PEFT模型\n",
    "    # 这会在原始模型的基础上添加LoRA适配器层，只有这些层的参数会被训练\n",
    "    # 原始模型的参数保持冻结，大幅减少训练参数数量\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "model = create_and_prepare_model()\n",
    "# 打印可训练参数占比，确认LoRA设置生效\n",
    "# 这会显示总参数数量、可训练参数数量和占比，帮助确认LoRA是否正确应用\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 内存优化回调，在训练关键点主动释放内存\n",
    "# 这个回调类在训练过程中的关键时刻清理显存，防止显存不足导致训练中断\n",
    "class AggressiveMemoryOptimizationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    激进的内存优化回调\n",
    "    \n",
    "    在以下时机进行显存清理：\n",
    "    1. 评估开始前：清理训练阶段积累的显存碎片\n",
    "    2. 评估结束后：清理评估阶段的显存占用\n",
    "    3. 每次日志输出后：定期清理显存碎片\n",
    "    \"\"\"\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"在评估开始前执行内存优化\"\"\"\n",
    "        import gc  # Python垃圾回收模块\n",
    "        # 手动触发垃圾收集，释放Python中不再使用的对象\n",
    "        gc.collect()\n",
    "        # 清空PyTorch的CUDA缓存，释放GPU显存中的未使用内存\n",
    "        torch.cuda.empty_cache()\n",
    "        # 临时将模型设置为eval模式，这是评估的标准做法\n",
    "        model.eval()\n",
    "        # 禁用梯度计算，在评估时不需要计算梯度，可以节省显存\n",
    "        model.config.use_cache = False\n",
    "        return control\n",
    "\n",
    "    def on_evaluate_end(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"在评估结束后执行内存清理\"\"\"\n",
    "        import gc\n",
    "        # 再次清理内存，确保评估阶段的显存占用被释放\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # 恢复模型设置，准备继续训练\n",
    "        model.train()  # 切换回训练模式\n",
    "        model.config.use_cache = True  # 重新启用缓存\n",
    "        return control\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"每次日志输出后清理内存\"\"\"\n",
    "        # 每次日志输出后清理显存，防止内存碎片积累\n",
    "        # 这是一个轻量级的清理，不会显著影响训练速度\n",
    "        torch.cuda.empty_cache()\n",
    "        return control\n",
    "\n",
    "# 追踪最佳模型并在训练结束时保存最终模型和最佳模型信息\n",
    "class SaveBestAndLastModelCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    最佳模型跟踪和保存回调\n",
    "    \n",
    "    功能：\n",
    "    1. 在训练过程中跟踪性能最佳的模型\n",
    "    2. 在训练结束时保存最终轮次的模型\n",
    "    3. 自动合并LoRA权重到基础模型，生成可直接使用的完整模型\n",
    "    4. 保存模型信息到文件，方便后续查找和使用\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化回调，设置跟踪变量\"\"\"\n",
    "        self.best_metric = None           # 记录最佳指标值（如最低的验证损失）\n",
    "        self.best_model_checkpoint = None # 记录最佳模型的检查点路径\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"在每次评估后检查是否出现了新的最佳模型\"\"\"\n",
    "        # 在每次评估后检查是否是新的最佳模型\n",
    "        # 只在主进程（rank 0）执行，避免多进程环境下的重复操作\n",
    "        if state.is_world_process_zero and metrics is not None:  # 仅在主进程执行\n",
    "            # 获取用于判断最佳模型的指标名称（在training_args中设置）\n",
    "            metric_to_check = args.metric_for_best_model\n",
    "            if metric_to_check in metrics:\n",
    "                current_metric = metrics[metric_to_check]  # 当前评估的指标值\n",
    "                # 检查是否为更好的模型\n",
    "                # 判断逻辑：如果是第一次评估，或者当前指标比历史最佳更好\n",
    "                if self.best_metric is None or (\n",
    "                    args.greater_is_better and current_metric > self.best_metric  # 指标越大越好的情况\n",
    "                ) or (\n",
    "                    not args.greater_is_better and current_metric < self.best_metric  # 指标越小越好的情况（如损失）\n",
    "                ):\n",
    "                    # 更新最佳模型记录\n",
    "                    self.best_metric = current_metric\n",
    "                    self.best_model_checkpoint = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "                    print(f\"\\n发现新的最佳模型! {metric_to_check}: {self.best_metric}, 保存在 {self.best_model_checkpoint}\\n\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"训练结束时的处理逻辑\"\"\"\n",
    "        # 训练结束时保存最终轮次的模型\n",
    "        # 只在主进程执行，避免多进程重复保存\n",
    "        if state.is_world_process_zero:  # 仅在主进程执行\n",
    "            # 1. 保存最终轮次的LoRA模型\n",
    "            # LoRA模型只包含训练的适配器权重，文件较小\n",
    "            final_lora_path = os.path.join(args.output_dir, \"final_lora_model\")\n",
    "            os.makedirs(final_lora_path, exist_ok=True)\n",
    "            # 保存当前状态（最后一轮）的模型\n",
    "            if \"model\" in kwargs:\n",
    "                kwargs[\"model\"].save_pretrained(final_lora_path)\n",
    "                print(f\"最终轮次的LoRA模型已保存至: {final_lora_path}\")\n",
    "\n",
    "            # 2. 合并最终轮次的完整模型\n",
    "            # 将LoRA权重合并到基础模型中，生成可以直接使用的完整模型\n",
    "            print(\"合并并保存最终轮次的完整模型...\")\n",
    "            # 重新加载干净的基础模型\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_PATH,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # 加载LoRA权重并合并\n",
    "            final_lora_model = PeftModel.from_pretrained(base_model, final_lora_path)\n",
    "            final_merged_model = final_lora_model.merge_and_unload()  # 合并权重并卸载LoRA层\n",
    "\n",
    "            # 保存合并后的完整模型\n",
    "            final_complete_path = os.path.join(args.output_dir, \"final_complete_model\")\n",
    "            os.makedirs(final_complete_path, exist_ok=True)\n",
    "            final_merged_model.save_pretrained(final_complete_path)\n",
    "\n",
    "            # 保存分词器，确保模型可以直接使用\n",
    "            # 完整模型需要配套的分词器才能正常工作\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "            tokenizer.save_pretrained(final_complete_path)\n",
    "            print(f\"最终轮次的完整模型已保存至: {final_complete_path}\")\n",
    "\n",
    "            # 3. 保存最佳模型信息到文件，方便后续查看和使用\n",
    "            # 创建一个文本文件记录最佳模型的详细信息\n",
    "            with open(os.path.join(args.output_dir, \"best_model_info.txt\"), \"w\") as f:\n",
    "                f.write(f\"最佳模型checkpoint路径: {self.best_model_checkpoint}\\n\")\n",
    "                f.write(f\"最佳{args.metric_for_best_model}: {self.best_metric}\")\n",
    "            print(f\"最佳模型信息已保存至: {os.path.join(args.output_dir, 'best_model_info.txt')}\")\n",
    "\n",
    "# 创建自定义回调实例\n",
    "# 实例化我们定义的回调类，准备在训练过程中使用\n",
    "best_and_last_callback = SaveBestAndLastModelCallback()\n",
    "\n",
    "# 创建Trainer实例\n",
    "# Trainer是Hugging Face提供的高级训练接口，封装了完整的训练流程\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 要训练的模型（已应用LoRA的PEFT模型）\n",
    "    args=training_args,             # 训练参数配置（学习率、批大小等）\n",
    "    train_dataset=train_dataset,    # 训练数据集\n",
    "    eval_dataset=eval_dataset,      # 验证数据集（用于评估模型性能）\n",
    "    data_collator=data_collator,    # 数据整理器（处理批次组装和填充）\n",
    "    callbacks=[\n",
    "        # 早停回调：防止过拟合的重要机制\n",
    "        # 如果连续3次评估验证损失没有改善超过0.005，就自动停止训练\n",
    "        # early_stopping_patience=3: 容忍3次没有改善\n",
    "        # early_stopping_threshold=0.005: 改善的最小阈值\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.005),\n",
    "        # 内存优化回调：在关键时刻清理显存，防止显存不足\n",
    "        AggressiveMemoryOptimizationCallback(),\n",
    "        # 最佳模型跟踪回调：自动保存最佳模型和最终模型\n",
    "        best_and_last_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 开始训练 - 优先尝试从最新checkpoint恢复\n",
    "# 支持断点续训功能，如果训练中断可以从上次保存的地方继续\n",
    "last_checkpoint_dir = \"/root/autodl-tmp/finetune_output/checkpoint-3000\"  # 检查点路径\n",
    "if os.path.exists(last_checkpoint_dir):\n",
    "    # 如果找到检查点文件，从断点继续训练\n",
    "    # 这样可以节省时间，不需要从头开始训练\n",
    "    # 从上次中断的地方继续\n",
    "    print(f\"从checkpoint {last_checkpoint_dir} 恢复训练...\")\n",
    "    # resume_from_checkpoint: 指定要恢复的检查点路径\n",
    "    # ignore_keys_for_eval: 在评估时忽略某些键，避免版本兼容性问题\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint_dir, ignore_keys_for_eval=[\"*\"])\n",
    "else:\n",
    "    # 如果没有找到检查点，从头开始训练\n",
    "    # 这是全新训练的情况\n",
    "    # 如果没有检查点，从头开始训练\n",
    "    print(\"从头开始训练...\")\n",
    "    trainer.train()\n",
    "\n",
    "# 保存最佳LoRA模型 - 利用load_best_model_at_end=True特性\n",
    "# 由于设置了load_best_model_at_end=True，此时trainer.model已经是训练过程中性能最佳的模型\n",
    "best_lora_model_path = os.path.join(OUTPUT_DIR, \"best_lora_model\")\n",
    "os.makedirs(best_lora_model_path, exist_ok=True)\n",
    "# 保存最佳LoRA权重，这个文件比较小，只包含训练的适配器参数\n",
    "trainer.model.save_pretrained(best_lora_model_path)\n",
    "print(f\"最佳LoRA模型已保存至: {best_lora_model_path}\")\n",
    "\n",
    "# 保存完整的最佳模型（合并LoRA权重到基础模型）\n",
    "# 完整模型可以直接使用，不需要额外的LoRA配置\n",
    "print(\"开始合并LoRA权重与基础模型，并保存最佳完整模型...\")\n",
    "# 获取基础模型\n",
    "# 重新加载一个干净的基础模型，用于合并LoRA权重\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 加载最佳LoRA模型与基础模型合并\n",
    "# 将保存的LoRA权重加载到基础模型上\n",
    "best_lora_model = PeftModel.from_pretrained(base_model, best_lora_model_path)\n",
    "# 合并LoRA权重到基础模型\n",
    "# merge_and_unload()会将LoRA的权重合并到原始模型参数中，并移除LoRA层\n",
    "best_merged_model = best_lora_model.merge_and_unload()\n",
    "\n",
    "# 保存完整最佳模型\n",
    "# 这个模型包含了所有参数，可以像普通的预训练模型一样直接使用\n",
    "best_complete_model_path = os.path.join(OUTPUT_DIR, \"best_complete_model\")\n",
    "os.makedirs(best_complete_model_path, exist_ok=True)\n",
    "best_merged_model.save_pretrained(best_complete_model_path)\n",
    "# 同时保存分词器，确保模型可以完整使用\n",
    "tokenizer.save_pretrained(best_complete_model_path)\n",
    "\n",
    "print(f\"最佳完整模型已保存至: {best_complete_model_path}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4b23e",
   "metadata": {},
   "source": [
    "deepspeed对应ds_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4d4a4",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"initial_scale_power\": 12\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"overlap_comm\": true,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": \"auto\",\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false,\n",
    "  \"zero_allow_untested_optimizer\": true,\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"cpu_checkpointing\": true,\n",
    "    \"profile\": false\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ee964",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  // ==================== 混合精度训练配置 ====================\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,                    // 启用FP16混合精度训练\n",
    "                                       // 作用：减少50%显存使用，提高训练速度1.5-2倍\n",
    "                                       // 影响：可能出现数值不稳定，需要损失缩放\n",
    "                                       // 硬件要求：现代GPU（V100/A100/RTX系列）\n",
    "    \n",
    "    \"initial_scale_power\": 12          // 初始损失缩放因子 = 2^12 = 4096\n",
    "                                       // 作用：防止FP16训练时梯度下溢（变成0）\n",
    "                                       // 调优：12-16通常效果好，太大可能溢出，太小可能下溢\n",
    "                                       // 自动调整：训练过程中会动态调整此值\n",
    "  },\n",
    "\n",
    "  // ==================== ZeRO优化配置 ====================\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,                        // ZeRO优化级别：2级\n",
    "                                       // ZeRO-1：只分割优化器状态，节省~50%显存\n",
    "                                       // ZeRO-2：分割优化器状态+梯度，节省~65%显存  \n",
    "                                       // ZeRO-3：分割优化器状态+梯度+参数，节省~75%显存\n",
    "                                       // 选择2级：平衡内存节省和通信开销\n",
    "    \n",
    "    \"contiguous_gradients\": true,      // 梯度连续存储优化\n",
    "                                       // 作用：将梯度存储在连续内存块中\n",
    "                                       // 好处：减少内存碎片，提高内存访问效率\n",
    "                                       // 影响：轻微增加内存整理开销，但总体提升性能\n",
    "                                       // 建议：通常保持启用\n",
    "    \n",
    "    \"overlap_comm\": true,              // 通信与计算重叠\n",
    "                                       // 作用：在计算前向传播时同时进行梯度通信\n",
    "                                       // 好处：隐藏通信延迟，提高训练速度5-15%\n",
    "                                       // 要求：GPU和网络支持异步操作\n",
    "                                       // 注意：可能增加GPU内存使用\n",
    "    \n",
    "    \"offload_optimizer\": {             // 优化器状态卸载配置\n",
    "      \"device\": \"cpu\",                 // 卸载目标：CPU内存\n",
    "                                       // 选项：cpu（CPU内存）| nvme（NVMe SSD）\n",
    "                                       // cpu：中等速度，大容量，适合大多数场景\n",
    "                                       // nvme：较慢速度，超大容量，极端内存不足时使用\n",
    "      \n",
    "      \"pin_memory\": true               // 使用页锁定内存\n",
    "                                       // 作用：将CPU内存锁定，加速CPU-GPU数据传输\n",
    "                                       // 好处：传输速度提高2-3倍\n",
    "                                       // 代价：占用更多系统内存，减少可用内存\n",
    "                                       // 建议：系统内存充足时启用（>32GB）\n",
    "    }\n",
    "  },\n",
    "\n",
    "  // ==================== 自动配置参数 ====================\n",
    "  \"gradient_accumulation_steps\": \"auto\", // 梯度累积步数：自动配置\n",
    "                                         // 作用：模拟更大的batch size而不增加显存\n",
    "                                         // auto：DeepSpeed根据train_batch_size自动计算\n",
    "                                         // 公式：gradient_accumulation_steps = train_batch_size / (train_micro_batch_size_per_gpu × num_gpus)\n",
    "                                         // 手动设置：可以设置具体数值如8、16等\n",
    "  \n",
    "  \"gradient_clipping\": \"auto\",          // 梯度裁剪：自动配置\n",
    "                                        // 作用：防止梯度爆炸，稳定训练\n",
    "                                        // auto：使用TrainingArguments中的max_grad_norm值\n",
    "                                        // 手动设置：可以设置具体数值如1.0、0.5等\n",
    "                                        // 建议：大模型训练通常设置为1.0\n",
    "  \n",
    "  \"steps_per_print\": \"auto\",            // 日志打印频率：自动配置\n",
    "                                        // 作用：控制训练日志输出频率\n",
    "                                        // auto：使用TrainingArguments中的logging_steps值\n",
    "                                        // 手动设置：可以设置具体数值如10、50等\n",
    "                                        // 影响：仅影响日志输出，不影响训练性能\n",
    "  \n",
    "  \"train_batch_size\": \"auto\",           // 全局批次大小：自动配置\n",
    "                                        // 作用：所有GPU的总批次大小\n",
    "                                        // auto：从TrainingArguments自动计算\n",
    "                                        // 计算：per_device_train_batch_size × gradient_accumulation_steps × num_gpus\n",
    "                                        // 影响：影响学习率调度和收敛行为\n",
    "  \n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\", // 每GPU微批次大小：自动配置\n",
    "                                            // 作用：每个GPU实际处理的批次大小\n",
    "                                            // auto：使用TrainingArguments中的per_device_train_batch_size值\n",
    "                                            // 影响：直接影响GPU显存使用量\n",
    "                                            // 调优：显存不足时减少此值\n",
    "\n",
    "  // ==================== 调试和监控配置 ====================\n",
    "  \"wall_clock_breakdown\": false,       // 性能分析：禁用\n",
    "                                        // 作用：详细记录各个训练阶段的时间消耗\n",
    "                                        // true：输出详细的时间分析，用于性能调优\n",
    "                                        // false：不输出详细分析，减少日志量\n",
    "                                        // 建议：调试时启用，生产环境禁用\n",
    "  \n",
    "  \"zero_allow_untested_optimizer\": true, // 允许未测试的优化器：启用\n",
    "                                         // 作用：允许使用DeepSpeed未充分测试的优化器\n",
    "                                         // true：可以使用AdamW、Lion等新优化器\n",
    "                                         // false：只能使用DeepSpeed官方测试的优化器\n",
    "                                         // 风险：可能出现兼容性问题\n",
    "                                         // 建议：使用常见优化器时可以启用\n",
    "\n",
    "  // ==================== 激活检查点配置 ====================\n",
    "  \"activation_checkpointing\": {         // 激活检查点：内存优化技术\n",
    "    \"partition_activations\": true,      // 激活值分割：启用\n",
    "                                        // 作用：将激活值在多个GPU间分割存储\n",
    "                                        // 好处：减少每个GPU的激活值内存占用\n",
    "                                        // 代价：增加GPU间通信开销\n",
    "                                        // 适用：显存紧张的多GPU训练\n",
    "    \n",
    "    \"cpu_checkpointing\": true,          // CPU检查点：启用\n",
    "                                        // 作用：将激活值检查点存储在CPU内存中\n",
    "                                        // 好处：大幅减少GPU显存使用（50-80%）\n",
    "                                        // 代价：增加CPU-GPU传输开销，训练速度降低20-40%\n",
    "                                        // 权衡：内存 vs 速度，显存严重不足时启用\n",
    "    \n",
    "    \"profile\": false                    // 性能分析：禁用\n",
    "                                        // 作用：分析激活检查点的性能影响\n",
    "                                        // true：输出详细的检查点性能数据\n",
    "                                        // false：不输出分析数据\n",
    "                                        // 建议：调试时启用，正常训练时禁用\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e28bde",
   "metadata": {},
   "source": [
    "执行脚本 run_distributed.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8a1ea",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# 设置环境变量\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "# 添加PyTorch内存优化\n",
    "export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# 启动分布式训练\n",
    "# 使用deepspeed的launcher\n",
    "deepspeed lora_deepspeed.py \\\n",
    "    --deepspeed ds_config.json \\\n",
    "    --num_gpus=4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69e3ae",
   "metadata": {},
   "source": [
    "执行命令  nohup bash run_distributed.sh > 2025_05_26_16_53_multi.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e76e1",
   "metadata": {},
   "source": [
    "查看日志 tail -f 2025_05_26_16_53_multi.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ad45c",
   "metadata": {},
   "source": [
    "使用两张卡的监控信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07228d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250522210633380.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81242800",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250522210704837.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c5666",
   "metadata": {},
   "source": [
    "使用4张卡的监控信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390180a7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250522214420315.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3197046",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250522214452036.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8633ad1",
   "metadata": {},
   "source": [
    "tensorboard --logdir=./autodl-tmp/finetune_output/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade91ec",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528013516544.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebe231",
   "metadata": {},
   "source": [
    "本地连接tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbff21",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528013617649.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e651426",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523120022511.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd0333",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523175446718.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95564a",
   "metadata": {},
   "source": [
    "链接成功后打开本地地址 http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50455ec3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523175721196.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612cf28",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523175855850.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309b8df",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523175932528.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8360a28",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250523175958091.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaefb22",
   "metadata": {},
   "source": [
    "best_mode自动保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b299a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528014204184.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b06815",
   "metadata": {},
   "source": [
    "训练完成后模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5aa4e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528165403679.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c9466",
   "metadata": {},
   "source": [
    "实际checkpoint状况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4e395",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528002119376.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd5aa5",
   "metadata": {},
   "source": [
    "### HuggingFace Trainer原生文件：\n",
    "```python\n",
    "trainer_state.json          # 训练状态：当前步数、epoch、最佳指标等\n",
    "training_args.bin          # 训练参数：学习率、批大小等配置\n",
    "```\n",
    "\n",
    "### PEFT/LoRA相关文件：\n",
    "```python\n",
    "adapter_config.json        # LoRA适配器配置\n",
    "adapter_model.safetensors  # LoRA权重文件（安全张量格式）\n",
    "```\n",
    "\n",
    "### 分词器相关文件：\n",
    "```python\n",
    "special_tokens_map.json    # 特殊token映射\n",
    "tokenizer_config.json      # 分词器配置\n",
    "tokenizer.json            # 分词器主文件\n",
    "```\n",
    "\n",
    "### DeepSpeed相关文件：\n",
    "```python\n",
    "global_step3584           # DeepSpeed全局步数记录\n",
    "rng_state_0.pth          # 随机数生成器状态（GPU 0）\n",
    "rng_state_1.pth          # 随机数生成器状态（GPU 1）\n",
    "rng_state_2.pth          # 随机数生成器状态（GPU 2）\n",
    "rng_state_3.pth          # 随机数生成器状态（GPU 3）\n",
    "scheduler.pt             # 学习率调度器状态\n",
    "zero_to_fp32.py         # DeepSpeed权重转换脚本\n",
    "```\n",
    "\n",
    "### 其他文件：\n",
    "```python\n",
    "README.md               # 说明文档\n",
    "latest                  # 指向最新checkpoint的符号链接\n",
    "```\n",
    "\n",
    "### 2. 为什么Checkpoint不能直接推理？\n",
    "\n",
    "### 核心问题分析：\n",
    "```python\n",
    "# Checkpoint保存的是什么？\n",
    "checkpoint/\n",
    "├── adapter_model.safetensors    # 只有LoRA的增量权重！\n",
    "├── adapter_config.json          # LoRA配置\n",
    "└── 分词器文件                    # 分词器信息\n",
    "# 缺少：基础模型的完整权重！\n",
    "```\n",
    "\n",
    "**关键理解**：\n",
    "- Checkpoint保存的是**LoRA适配器权重**，不是完整模型权重\n",
    "- LoRA是在基础模型上的**增量修改**，需要基础模型才能工作\n",
    "- 就像\"补丁\"需要\"原始软件\"才能运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba31e07",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 如何正确加载Checkpoint进行推理？\n",
    "\n",
    "### 方法1：加载LoRA + 基础模型\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. 加载基础模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B/\",  # 原始基础模型路径\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 2. 加载分词器（可以从checkpoint或基础模型加载）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/root/autodl-tmp/finetune_output/checkpoint-3600/\",  # 从checkpoint加载\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. 加载LoRA适配器\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"/root/autodl-tmp/finetune_output/checkpoint-3600/\"  # checkpoint路径\n",
    ")\n",
    "\n",
    "# 4. 进行推理\n",
    "inputs = tokenizer(\"你好，请介绍一下自己\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128fa47",
   "metadata": {},
   "source": [
    "\n",
    "### 方法2：合并后推理（推荐生产环境）\n",
    "```python\n",
    "# 1. 加载并合并模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "merged_model = lora_model.merge_and_unload()  # 合并LoRA权重到基础模型\n",
    "\n",
    "# 2. 保存合并后的完整模型\n",
    "merged_model.save_pretrained(\"/path/to/merged_model/\")\n",
    "tokenizer.save_pretrained(\"/path/to/merged_model/\")\n",
    "\n",
    "# 3. 直接加载完整模型推理（更快）\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/path/to/merged_model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/path/to/merged_model/\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096bf51",
   "metadata": {},
   "source": [
    "## 5.4 进一步调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6a12d",
   "metadata": {},
   "source": [
    "```python \n",
    "# 开始训练 - 优先尝试从最新checkpoint恢复\n",
    "last_checkpoint_dir = \"/root/autodl-tmp/finetune_output/checkpoint-3000\"  # 检查点路径\n",
    "\n",
    "# 定义LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    lora_dropout=0.1,      # LoRA层的dropout率\n",
    "    bias=\"none\",            # 不微调偏置参数\n",
    ")\n",
    "\n",
    "# 修改训练参数配置\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=12,          # 训练时的梯度累积步数\n",
    "    num_train_epochs=70,                    # 训练轮数\n",
    "    #max_steps=3000,                         # 最大训练步数\n",
    "    learning_rate=5e-6,                     # 初始学习率\n",
    "    warmup_ratio=0.05,                       # 预热阶段占比\n",
    "    eval_steps=300,                         # 每300步评估一次\n",
    "    save_steps=300,                         # 每300步保存一次\n",
    "    max_grad_norm=0.5,                      # 梯度裁剪阈值\n",
    "    weight_decay=0.01,                      # 添加适度的权重衰减\n",
    "    lr_scheduler_type=\"cosine\",             # 添加余弦退火调度\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8221796",
   "metadata": {},
   "source": [
    "注意 如果想eval_steps=300 与save_steps 设置生效需要同样更改选择checkpoint节点中的trainer_state.json文件中对应内容，否则无法生效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1165f9",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528142516777.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f217d28",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528143134511.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787036c6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528143217355.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab451e",
   "metadata": {},
   "source": [
    "## 5.5 第三阶段调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ffe34",
   "metadata": {},
   "source": [
    "混合数据源微调 从上一阶段保存后的best_mode进行继续训练，主要可以更改lora相关内容，以及增添多轮对话微调合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a649434",
   "metadata": {},
   "source": [
    "```python\n",
    "# 基础库导入\n",
    "import os                   # 操作系统接口，用于文件路径操作（创建文件夹、检查文件是否存在等）\n",
    "import json                 # JSON数据处理库，用于读取和解析训练数据文件\n",
    "import torch                # PyTorch深度学习框架核心库，提供神经网络训练的基础功能\n",
    "from torch.utils.data import Dataset  # PyTorch数据集基类，用于创建自定义数据加载器\n",
    "import glob                 # 文件路径匹配工具，可以批量查找符合条件的文件\n",
    "\n",
    "# DeepSpeed相关导入（解决PyTorch 2.6加载兼容性问题）\n",
    "# DeepSpeed是微软开发的深度学习优化库，可以大幅减少显存占用\n",
    "import deepspeed.runtime.fp16.loss_scaler    # DeepSpeed的半精度浮点数损失缩放器\n",
    "import deepspeed.runtime.zero.config         # DeepSpeed的ZeRO内存优化配置\n",
    "from torch.serialization import add_safe_globals  # PyTorch安全序列化功能，防止加载恶意代码\n",
    "\n",
    "# Transformers库核心组件\n",
    "# Transformers是Hugging Face开发的预训练模型库，包含GPT、BERT等各种AI模型\n",
    "from transformers import (\n",
    "    AutoTokenizer,          # 自动分词器：将文本转换为模型能理解的数字序列（token）\n",
    "    AutoModelForCausalLM,   # 自动因果语言模型：专门用于文本生成任务的模型类型\n",
    "    Trainer,                # 训练器：封装了完整训练流程的高级接口，简化训练代码\n",
    "    TrainingArguments,      # 训练参数配置类：包含学习率、批大小、训练轮数等所有训练设置\n",
    "    DataCollatorForLanguageModeling,  # 语言模型数据整理器：将多个文本样本组合成训练批次\n",
    "    EarlyStoppingCallback,  # 早停回调：防止过拟合，当模型性能不再提升时自动停止训练\n",
    "    TrainerCallback         # 自定义回调基类：用于在训练过程中执行自定义操作\n",
    ")\n",
    "\n",
    "# PEFT库组件（用于高效参数微调）\n",
    "# PEFT (Parameter-Efficient Fine-Tuning) 是一种只训练少量参数的微调技术\n",
    "# 相比传统全参数微调，PEFT可以用更少的显存和时间达到相似效果\n",
    "from peft import (\n",
    "    get_peft_model,         # 获取PEFT模型：将普通模型转换为参数高效微调模型\n",
    "    LoraConfig,             # LoRA配置：LoRA是最流行的PEFT方法，通过低秩矩阵分解减少参数\n",
    "    TaskType,               # 任务类型枚举：指定模型要执行的任务（文本生成、分类等）\n",
    "    PeftModel               # PEFT模型基类：用于加载和操作参数高效微调模型\n",
    ")\n",
    "\n",
    "import deepspeed            # DeepSpeed分布式训练框架：提供内存优化和多GPU训练功能\n",
    "\n",
    "# 定义模型路径和数据路径\n",
    "# 这些路径告诉程序在哪里找到预训练模型、训练数据，以及把结果保存到哪里\n",
    "MODEL_PATH = \"/root/autodl-tmp/best_complete_model_05255100/\"  # 基础预训练模型的存储路径（这里使用的是之前训练好的模型）\n",
    "# 更新为SFT数据路径\n",
    "# 训练数据路径列表：包含多轮对话数据和单轮SFT数据，程序会自动识别和处理不同格式\n",
    "TRAIN_DATA_PATHS = [\"/root/autodl-tmp/data/sft/deepspeek_multi_turn_dataset.jsonl\",\"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_3k.jsonl\",\"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_5k.jsonl\"]\n",
    "VAL_DATA_PATH = \"/root/autodl-tmp/data/sft/deepspeek_sft_dataset_1_1k.jsonl\"  # 验证数据集：用于评估模型训练效果\n",
    "OUTPUT_DIR = \"/root/autodl-tmp/finetune_output\"              # 输出目录：训练过程中的模型、日志等文件保存位置\n",
    "\n",
    "# 创建输出目录(如果不存在)\n",
    "# exist_ok=True 表示如果目录已经存在也不会报错\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 加载分词器\n",
    "# 分词器的作用是将人类可读的文本转换为模型可以处理的数字序列\n",
    "# trust_remote_code=True 允许加载包含自定义代码的模型（某些模型需要特殊的分词逻辑）\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "# 处理没有pad_token的分词器\n",
    "# pad_token用于将不同长度的文本填充到相同长度，这样可以批量处理\n",
    "# 如果分词器没有专门的填充标记，就使用文本结束标记代替\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 使用eos_token作为填充标记\n",
    "\n",
    "# 自定义数据集类，用于处理SFT格式数据\n",
    "# 这个增强版的数据集类可以同时处理单轮对话和多轮对话数据\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_paths, tokenizer, max_length=1024, dataset_type=\"auto\"):\n",
    "        \"\"\"\n",
    "        初始化SFT数据集（支持单轮和多轮对话）\n",
    "        \n",
    "        这个函数的作用：\n",
    "        1. 读取JSONL格式的训练数据文件（每行一个JSON对象）\n",
    "        2. 自动识别单轮SFT格式和多轮对话格式\n",
    "        3. 将每条数据转换为模型训练所需的格式\n",
    "        4. 处理文本分词和长度限制\n",
    "        \n",
    "        参数解释：\n",
    "        data_paths: 数据文件路径，可以是单个文件路径（字符串）或多个文件路径（列表）\n",
    "        tokenizer: 分词器对象，用于将文本转换为token ID\n",
    "        max_length: 文本序列的最大长度，超过这个长度的文本会被截断（1024是常用值）\n",
    "        dataset_type: 数据集类型控制\n",
    "                     - \"auto\": 自动检测并处理所有类型的数据（推荐）\n",
    "                     - \"single\": 只处理单轮对话数据\n",
    "                     - \"multi\": 只处理多轮对话数据\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer      # 保存分词器，后续处理数据时使用\n",
    "        self.max_length = max_length    # 保存最大长度限制\n",
    "        self.dataset_type = dataset_type # 保存数据集类型设置\n",
    "        self.examples = []              # 用于存储所有处理后的训练样本\n",
    "\n",
    "        # 确保data_paths是列表格式，方便统一处理\n",
    "        # 如果传入的是单个字符串路径，转换为包含一个元素的列表\n",
    "        if isinstance(data_paths, str):\n",
    "            data_paths = [data_paths]\n",
    "\n",
    "        # 初始化计数器，用于统计数据加载情况\n",
    "        total_lines = 0         # 文件总行数\n",
    "        processed_lines = 0     # 成功处理的行数\n",
    "        single_turn_count = 0   # 单轮对话样本数量\n",
    "        multi_turn_count = 0    # 多轮对话样本数量\n",
    "\n",
    "        # 逐个处理每个数据文件\n",
    "        for data_path in data_paths:\n",
    "            try:\n",
    "                # 以UTF-8编码打开文件，确保中文字符正确显示\n",
    "                with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                    # 逐行读取文件内容（JSONL格式：每行一个JSON对象）\n",
    "                    for line in f:\n",
    "                        total_lines += 1\n",
    "                        try:\n",
    "                            # 解析JSON格式的数据\n",
    "                            item = json.loads(line)\n",
    "\n",
    "                            # 检测数据格式类型\n",
    "                            # 多轮对话数据包含\"messages\"和\"turns\"字段\n",
    "                            # 单轮SFT数据包含\"instruction\"、\"input\"、\"output\"字段\n",
    "                            is_multi_turn = \"messages\" in item and \"turns\" in item\n",
    "\n",
    "                            # 根据设置决定是否处理该样本\n",
    "                            # dataset_type=\"auto\": 处理所有类型\n",
    "                            # dataset_type=\"single\": 只处理单轮数据\n",
    "                            # dataset_type=\"multi\": 只处理多轮数据\n",
    "                            if dataset_type == \"auto\" or \\\n",
    "                               (dataset_type == \"single\" and not is_multi_turn) or \\\n",
    "                               (dataset_type == \"multi\" and is_multi_turn):\n",
    "\n",
    "                                if is_multi_turn:\n",
    "                                    # 处理多轮对话格式数据\n",
    "                                    processed_example = self._process_multi_turn(item)\n",
    "                                    if processed_example:\n",
    "                                        self.examples.append(processed_example)\n",
    "                                        multi_turn_count += 1\n",
    "                                        processed_lines += 1\n",
    "                                else:\n",
    "                                    # 处理单轮SFT格式数据\n",
    "                                    processed_example = self._process_single_turn(item)\n",
    "                                    if processed_example:\n",
    "                                        self.examples.append(processed_example)\n",
    "                                        single_turn_count += 1\n",
    "\n",
    "                        except Exception as e:\n",
    "                            # 如果某行数据解析失败，打印错误信息但继续处理其他行\n",
    "                            print(f\"处理第{total_lines}行时出错: {e}\")\n",
    "            except Exception as e:\n",
    "                # 如果文件打开失败，打印错误信息\n",
    "                print(f\"打开文件{data_path}时出错: {e}\")\n",
    "\n",
    "        # 打印详细的数据加载统计信息，让用户了解数据处理情况\n",
    "        print(f\"数据集加载 - 总行数: {total_lines}, 成功处理: {processed_lines}, 最终样本数: {len(self.examples)}\")\n",
    "        print(f\"单轮对话样本: {single_turn_count}, 多轮对话样本: {multi_turn_count}\")\n",
    "\n",
    "    def _process_single_turn(self, item):\n",
    "        \"\"\"\n",
    "        处理单轮SFT格式数据\n",
    "        \n",
    "        单轮SFT数据格式：\n",
    "        {\n",
    "            \"instruction\": \"任务指令\",\n",
    "            \"input\": \"输入内容（可选）\",\n",
    "            \"output\": \"期望输出\"\n",
    "        }\n",
    "        \n",
    "        参数：\n",
    "        item: 包含单轮对话数据的字典\n",
    "        \n",
    "        返回：\n",
    "        处理后的样本字典，包含prompt、full_text和data_type\n",
    "        \"\"\"\n",
    "        # 提取单轮SFT数据的三个核心字段\n",
    "        instruction = item.get(\"instruction\", \"\")  # 任务指令，告诉模型要做什么\n",
    "        input_text = item.get(\"input\", \"\")         # 输入内容，可能为空\n",
    "        output = item.get(\"output\", \"\")            # 期望的输出内容\n",
    "\n",
    "        # 构建提示模板\n",
    "        # 根据是否有输入内容，构建不同的提示格式\n",
    "        if input_text:\n",
    "            # 如果有输入内容，将指令和输入组合，用换行符分隔\n",
    "            prompt = f\"{instruction}\\n\\n{input_text}\\n\\n\"\n",
    "        else:\n",
    "            # 如果没有输入内容，只使用指令\n",
    "            prompt = f\"{instruction}\\n\\n\"\n",
    "\n",
    "        # 构建完整的训练文本：提示 + 期望输出 + 结束标记\n",
    "        # 结束标记告诉模型这里是文本的结尾\n",
    "        full_text = prompt + output + self.tokenizer.eos_token\n",
    "\n",
    "        # 返回处理后的样本\n",
    "        return {\n",
    "            \"prompt\": prompt,           # 提示部分（模型不需要学习生成这部分）\n",
    "            \"full_text\": full_text,     # 完整文本（用于训练）\n",
    "            \"data_type\": \"single\"       # 标记为单轮对话数据\n",
    "        }\n",
    "\n",
    "    def _process_multi_turn(self, item):\n",
    "        \"\"\"\n",
    "        处理多轮对话格式数据\n",
    "        \n",
    "        多轮对话数据格式：\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"系统提示\"},\n",
    "                {\"role\": \"user\", \"content\": \"用户问题1\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"助手回答1\"},\n",
    "                {\"role\": \"user\", \"content\": \"用户问题2\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"助手回答2\"}\n",
    "            ],\n",
    "            \"turns\": 对话轮数\n",
    "        }\n",
    "        \n",
    "        参数：\n",
    "        item: 包含多轮对话数据的字典\n",
    "        \n",
    "        返回：\n",
    "        处理后的样本字典，包含prompt、full_text和data_type\n",
    "        如果数据无效则返回None\n",
    "        \"\"\"\n",
    "        # 提取对话消息列表\n",
    "        messages = item.get(\"messages\", [])\n",
    "        if not messages:\n",
    "            # 如果没有消息内容，返回None表示无效数据\n",
    "            return None\n",
    "\n",
    "        # 构建完整的对话文本\n",
    "        full_text = \"\"              # 完整对话文本\n",
    "        last_user_prompt = \"\"       # 最后一个用户输入之前的所有内容（用作提示）\n",
    "\n",
    "        # 遍历所有消息，构建对话文本\n",
    "        for i, msg in enumerate(messages):\n",
    "            role = msg.get(\"role\", \"\")      # 消息角色：system/user/assistant\n",
    "            content = msg.get(\"content\", \"\") # 消息内容\n",
    "\n",
    "            # 根据角色添加不同的前缀\n",
    "            if role == \"system\":\n",
    "                # 系统消息：通常包含对话的背景或规则\n",
    "                full_text += f\"系统: {content}\\n\\n\"\n",
    "            elif role == \"user\":\n",
    "                # 用户消息：用户的问题或输入\n",
    "                user_text = f\"用户: {content}\\n\\n\"\n",
    "                full_text += user_text\n",
    "                # 记录最后一个用户输入作为提示\n",
    "                # 倒数第二个消息是最后一个用户输入（最后一个是assistant回复）\n",
    "                if i == len(messages) - 2:  # 倒数第二个消息是最后一个用户输入\n",
    "                    last_user_prompt = full_text\n",
    "            elif role == \"assistant\":\n",
    "                # 助手消息：AI的回复，每个回复后添加结束标记\n",
    "                full_text += f\"助手: {content}{self.tokenizer.eos_token}\\n\\n\"\n",
    "\n",
    "        # 如果没有找到有效的提示，使用除最后一个assistant回复外的所有内容\n",
    "        # 这种情况可能发生在对话格式不标准的时候\n",
    "        if not last_user_prompt:\n",
    "            # 找到最后一个assistant消息的位置\n",
    "            for i in range(len(messages)-1, -1, -1):\n",
    "                if messages[i].get(\"role\") == \"assistant\":\n",
    "                    # 构建提示为所有直到这个assistant消息之前的内容\n",
    "                    last_user_prompt = \"\"\n",
    "                    for j in range(i):\n",
    "                        role = messages[j].get(\"role\", \"\")\n",
    "                        content = messages[j].get(\"content\", \"\")\n",
    "                        if role == \"system\":\n",
    "                            last_user_prompt += f\"系统: {content}\\n\\n\"\n",
    "                        elif role == \"user\":\n",
    "                            last_user_prompt += f\"用户: {content}\\n\\n\"\n",
    "                        elif role == \"assistant\":\n",
    "                            last_user_prompt += f\"助手: {content}\\n\\n\"\n",
    "                    break\n",
    "\n",
    "        # 返回处理后的多轮对话样本\n",
    "        return {\n",
    "            \"prompt\": last_user_prompt,  # 提示部分（对话历史，不需要学习生成）\n",
    "            \"full_text\": full_text,      # 完整对话文本（用于训练）\n",
    "            \"data_type\": \"multi\"         # 标记为多轮对话数据\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中样本的总数，PyTorch需要这个信息来组织训练\"\"\"\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取指定索引的训练样本\n",
    "        \n",
    "        这个方法会被PyTorch的DataLoader自动调用，用于获取批次数据\n",
    "        每次训练时，PyTorch会调用这个方法来获取一个样本\n",
    "        \n",
    "        参数：\n",
    "        idx: 样本索引（第几个样本）\n",
    "        \n",
    "        返回：\n",
    "        包含input_ids、attention_mask、labels、data_type的字典，这是模型训练需要的格式\n",
    "        \"\"\"\n",
    "        # 获取指定索引的样本\n",
    "        example = self.examples[idx]\n",
    "        full_text = example[\"full_text\"]  # 完整的训练文本\n",
    "        prompt = example[\"prompt\"]        # 提示部分\n",
    "\n",
    "        # 使用分词器将文本转换为token ID\n",
    "        # token ID是模型能理解的数字序列，每个数字代表一个词或字符\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,                    # 要编码的文本\n",
    "            truncation=True,              # 启用截断：如果文本太长会被截断到max_length\n",
    "            max_length=self.max_length,   # 最大长度限制\n",
    "            padding=\"max_length\",         # 填充到最大长度：短文本会用pad_token填充\n",
    "            return_tensors=\"pt\"           # 返回PyTorch张量格式\n",
    "        )\n",
    "\n",
    "        # 提取编码结果\n",
    "        input_ids = encodings[\"input_ids\"][0]        # token ID序列：文本转换成的数字序列\n",
    "        attention_mask = encodings[\"attention_mask\"][0]  # 注意力掩码：告诉模型哪些是真实内容，哪些是填充\n",
    "\n",
    "        # 创建训练标签，用于计算损失\n",
    "        # 在因果语言模型中，标签就是输入序列（模型要学会预测下一个词）\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 计算提示部分的长度\n",
    "        # 我们不希望模型学习如何生成提示，只学习如何生成回答\n",
    "        prompt_tokens = self.tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        prompt_len = len(prompt_tokens)\n",
    "\n",
    "        # 将提示部分的标签设为-100，这样在计算损失时会被忽略\n",
    "        # -100是PyTorch中的特殊值，表示在计算损失时忽略这些位置\n",
    "        # 这样模型只会学习生成回答部分，不会学习生成提示\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        # 将data_type编码为整数，方便后续分析\n",
    "        # 1表示多轮对话，0表示单轮对话\n",
    "        data_type_value = 1 if example[\"data_type\"] == \"multi\" else 0\n",
    "\n",
    "        # 返回训练所需的所有数据\n",
    "        return {\n",
    "            \"input_ids\": input_ids,           # 输入的token ID序列\n",
    "            \"attention_mask\": attention_mask, # 注意力掩码（区分真实内容和填充）\n",
    "            \"labels\": labels,                 # 训练标签（告诉模型应该输出什么）\n",
    "            \"data_type\": torch.tensor(data_type_value)  # 数据类型标记（转换为PyTorch张量）\n",
    "        }\n",
    "\n",
    "# 加载训练和验证数据集\n",
    "# 创建数据集实例，这些对象会自动处理数据加载和预处理\n",
    "train_dataset = SFTDataset(TRAIN_DATA_PATHS, tokenizer,dataset_type=\"auto\")  # 使用自动检测模式，处理所有类型的数据\n",
    "eval_dataset = SFTDataset(VAL_DATA_PATH, tokenizer)      # 创建验证集，用于评估模型性能\n",
    "\n",
    "\n",
    "# 定义LoRA配置\n",
    "# LoRA (Low-Rank Adaptation) 是一种参数高效的微调方法\n",
    "# 它不修改原始模型权重，而是添加小的\"适配器\"层，大幅减少需要训练的参数\n",
    "lora_config = LoraConfig(\n",
    "    # target_modules: 指定要应用LoRA的模块名称\n",
    "    # 这些是Transformer模型中的关键组件：注意力机制和前馈网络\n",
    "    # q_proj, k_proj, v_proj, o_proj 是注意力机制的四个投影层\n",
    "    # gate_proj, down_proj, up_proj 是前馈网络的三个层\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型（用于文本生成，模型只能看到前面的内容）\n",
    "    r=24,                   # LoRA的秩（rank）：控制适配器的大小\n",
    "                           # 24是一个适中的值，比之前的32稍小，减少参数量但保持足够的表达能力\n",
    "    lora_alpha=48,          # LoRA的缩放因子：控制LoRA权重对最终结果的影响程度\n",
    "                           # 48 = 24 * 2，保持alpha是r的2倍的常见设置\n",
    "    lora_dropout=0.1,      # LoRA层的dropout率：随机丢弃10%的连接，防止过拟合\n",
    "                           # 0.1比之前的0.15稍低，适合更复杂的多轮对话训练\n",
    "    bias=\"none\",            # 偏置参数处理方式：\"none\"表示不训练偏置参数\n",
    "                           # 这进一步减少了需要训练的参数数量\n",
    ")\n",
    "\n",
    "# 修改训练参数配置\n",
    "# TrainingArguments包含了训练过程中的所有重要设置，类似于训练的\"配方\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # 输出目录：模型检查点、日志等文件的保存位置\n",
    "    per_device_train_batch_size=3,          # 每个GPU的训练批大小：一次处理3个样本\n",
    "                                            # 较小的值减少显存占用，适合处理长序列的多轮对话\n",
    "    per_device_eval_batch_size=3,           # 每个GPU的评估批大小：评估时一次处理3个样本\n",
    "    eval_accumulation_steps=3,              # 评估时的梯度累积步数：累积3个批次再计算指标\n",
    "    gradient_accumulation_steps=12,          # 训练时的梯度累积步数：累积12个小批次模拟大批次\n",
    "                                            # 实际批大小 = per_device_train_batch_size × gradient_accumulation_steps = 3×12=36\n",
    "                                            # 比之前的16稍小，适合多轮对话的复杂训练\n",
    "    num_train_epochs=80,                    # 训练轮数：整个数据集被训练80次\n",
    "                                            # 增加到80轮，因为多轮对话需要更多训练来学会上下文理解\n",
    "    #max_steps=3000,                         # 最大训练步数（被注释掉）\n",
    "                                            # 如果设置了max_steps，会覆盖num_train_epochs\n",
    "    learning_rate=2e-6,                     # 初始学习率：控制参数更新的步长\n",
    "                                            # 2e-6 (0.000002) 比之前更小，因为使用了预训练模型作为基础\n",
    "    warmup_ratio=0.05,                        # 预热阶段占比：前5%的训练步数用于学习率预热\n",
    "                                            # 减少到5%，因为基础模型已经比较成熟\n",
    "    fp16=True,                              # 启用FP16混合精度训练：使用16位浮点数代替32位\n",
    "                                            # 可以减少一半显存占用并加速训练，但可能略微影响精度\n",
    "    eval_strategy=\"steps\",                  # 评估策略：按训练步数进行评估（而不是按轮数）\n",
    "    eval_steps=300,                         # 每300步评估一次模型性能\n",
    "    do_eval=True,                           # 启用评估：在训练过程中定期评估模型性能\n",
    "    save_strategy=\"steps\",                  # 保存策略：按训练步数保存检查点\n",
    "    save_steps=300,                         # 每300步保存一次模型检查点\n",
    "    logging_steps=10,                       # 每10步记录一次训练日志（损失、学习率等）\n",
    "    save_total_limit=10,                    # 最多保存10个检查点\n",
    "                                            # 超过限制时会删除最旧的检查点以节省磁盘空间\n",
    "    remove_unused_columns=False,            # 保留数据中的所有列，不自动删除未使用的列\n",
    "                                            # 对于包含data_type字段的数据很重要\n",
    "    load_best_model_at_end=True,            # 训练结束时自动加载性能最佳的模型\n",
    "    metric_for_best_model=\"eval_loss\",      # 使用验证损失作为最佳模型的评判标准\n",
    "    greater_is_better=False,                # 损失越小越好（False表示指标越小越好）\n",
    "    deepspeed=\"ds_config.json\",             # DeepSpeed配置文件路径\n",
    "                                            # DeepSpeed提供内存优化和分布式训练功能\n",
    "    report_to=[\"tensorboard\"],              # 使用TensorBoard记录训练过程，可以可视化训练曲线\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),  # TensorBoard日志保存路径\n",
    "    max_grad_norm=0.5,                      # 梯度裁剪阈值：防止梯度爆炸\n",
    "                                            # 当梯度范数超过0.5时会被缩放到0.5\n",
    "    weight_decay=0.03,                      # 权重衰减：L2正则化系数，防止过拟合\n",
    "                                            # 0.03表示对权重施加适度的衰减惩罚\n",
    "    lr_scheduler_type=\"cosine\",             # 学习率调度器类型：余弦退火\n",
    "                                            # 学习率会按余弦函数逐渐降低，有助于模型收敛\n",
    ")\n",
    "\n",
    "# 创建数据整理器，用于批处理和填充\n",
    "# DataCollator负责将多个训练样本组合成批次，并处理长度不一致的问题\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,    # 使用前面定义的分词器\n",
    "    mlm=False               # mlm=False表示使用因果语言模型模式（用于文本生成）\n",
    "                           # 因果语言模型只能看到当前位置之前的内容，适合生成任务\n",
    "                           # mlm=True是掩码语言模型模式（如BERT），用于理解任务\n",
    ")\n",
    "\n",
    "# 修改后的模型加载和准备流程\n",
    "def create_and_prepare_model():\n",
    "    \"\"\"\n",
    "    创建并准备用于训练的模型\n",
    "    \n",
    "    这个函数执行以下步骤：\n",
    "    1. 从指定路径加载预训练的基础模型（这里是之前训练好的模型）\n",
    "    2. 配置模型以减少显存占用（启用梯度检查点）\n",
    "    3. 应用LoRA配置，将普通模型转换为参数高效微调模型\n",
    "    \n",
    "    返回：\n",
    "    配置好的PEFT模型，可以直接用于训练\n",
    "    \"\"\"\n",
    "    # 加载模型到CPU，之后DeepSpeed会处理GPU分配\n",
    "    # 不直接加载到GPU是为了让DeepSpeed更好地管理显存分配\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,                 # 预训练模型的路径（使用之前训练好的模型作为基础）\n",
    "        torch_dtype=torch.float16,  # 使用FP16数据类型减少显存占用\n",
    "                                   # FP16使用16位浮点数，相比FP32可以节省一半显存\n",
    "        device_map=None,            # 不使用Hugging Face的自动设备映射\n",
    "                                   # 让DeepSpeed来处理设备分配，避免冲突\n",
    "        low_cpu_mem_usage=True,     # 启用低CPU内存使用模式\n",
    "                                   # 在加载大模型时减少CPU内存占用\n",
    "        trust_remote_code=True      # 允许执行模型中的自定义代码\n",
    "                                   # 某些模型（如DeepSeek）可能包含自定义的模型架构代码\n",
    "    )\n",
    "\n",
    "    # 启用梯度检查点以节省内存（牺牲少量计算性能换取更多内存）\n",
    "    # 梯度检查点是一种内存优化技术：不保存所有中间激活值，而是在反向传播时重新计算\n",
    "    # 这样可以大幅减少显存占用，但会增加一些计算时间\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 应用LoRA配置，将模型转换为PEFT模型\n",
    "    # 这会在原始模型的基础上添加LoRA适配器层，只有这些层的参数会被训练\n",
    "    # 原始模型的参数保持冻结，大幅减少训练参数数量\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "model = create_and_prepare_model()\n",
    "# 打印可训练参数占比，确认LoRA设置生效\n",
    "# 这会显示总参数数量、可训练参数数量和占比，帮助确认LoRA是否正确应用\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 内存优化回调，在训练关键点主动释放内存\n",
    "# 这个回调类在训练过程中的关键时刻清理显存，防止显存不足导致训练中断\n",
    "class AggressiveMemoryOptimizationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    激进的内存优化回调\n",
    "    \n",
    "    在以下时机进行显存清理：\n",
    "    1. 评估开始前：清理训练阶段积累的显存碎片\n",
    "    2. 评估结束后：清理评估阶段的显存占用\n",
    "    3. 每次日志输出后：定期清理显存碎片\n",
    "    \n",
    "    对于多轮对话训练特别重要，因为长序列会占用更多显存\n",
    "    \"\"\"\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"在评估开始前执行内存优化\"\"\"\n",
    "        import gc  # Python垃圾回收模块\n",
    "        # 手动触发垃圾收集，释放Python中不再使用的对象\n",
    "        gc.collect()\n",
    "        # 清空PyTorch的CUDA缓存，释放GPU显存中的未使用内存\n",
    "        torch.cuda.empty_cache()\n",
    "        # 临时将模型设置为eval模式，这是评估的标准做法\n",
    "        model.eval()\n",
    "        # 禁用梯度计算，在评估时不需要计算梯度，可以节省显存\n",
    "        model.config.use_cache = False\n",
    "        return control\n",
    "\n",
    "    def on_evaluate_end(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"在评估结束后执行内存清理\"\"\"\n",
    "        import gc\n",
    "        # 再次清理内存，确保评估阶段的显存占用被释放\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # 恢复模型设置，准备继续训练\n",
    "        model.train()  # 切换回训练模式\n",
    "        model.config.use_cache = True  # 重新启用缓存\n",
    "        return control\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        \"\"\"每次日志输出后清理内存\"\"\"\n",
    "        # 每次日志输出后清理显存，防止内存碎片积累\n",
    "        # 这是一个轻量级的清理，不会显著影响训练速度\n",
    "        torch.cuda.empty_cache()\n",
    "        return control\n",
    "\n",
    "# 追踪最佳模型并在训练结束时保存最终模型和最佳模型信息\n",
    "class SaveBestAndLastModelCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    最佳模型跟踪和保存回调\n",
    "    \n",
    "    功能：\n",
    "    1. 在训练过程中跟踪性能最佳的模型\n",
    "    2. 在训练结束时保存最终轮次的模型\n",
    "    3. 自动合并LoRA权重到基础模型，生成可直接使用的完整模型\n",
    "    4. 保存模型信息到文件，方便后续查找和使用\n",
    "    \n",
    "    对于多轮对话模型特别重要，因为需要保存能够处理复杂对话的最佳版本\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化回调，设置跟踪变量\"\"\"\n",
    "        self.best_metric = None           # 记录最佳指标值（如最低的验证损失）\n",
    "        self.best_model_checkpoint = None # 记录最佳模型的检查点路径\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"在每次评估后检查是否出现了新的最佳模型\"\"\"\n",
    "        # 在每次评估后检查是否是新的最佳模型\n",
    "        # 只在主进程（rank 0）执行，避免多进程环境下的重复操作\n",
    "        if state.is_world_process_zero and metrics is not None:  # 仅在主进程执行\n",
    "            # 获取用于判断最佳模型的指标名称（在training_args中设置）\n",
    "            metric_to_check = args.metric_for_best_model\n",
    "            if metric_to_check in metrics:\n",
    "                current_metric = metrics[metric_to_check]  # 当前评估的指标值\n",
    "                # 检查是否为更好的模型\n",
    "                # 判断逻辑：如果是第一次评估，或者当前指标比历史最佳更好\n",
    "                if self.best_metric is None or (\n",
    "                    args.greater_is_better and current_metric > self.best_metric  # 指标越大越好的情况\n",
    "                ) or (\n",
    "                    not args.greater_is_better and current_metric < self.best_metric  # 指标越小越好的情况（如损失）\n",
    "                ):\n",
    "                    # 更新最佳模型记录\n",
    "                    self.best_metric = current_metric\n",
    "                    self.best_model_checkpoint = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "                    print(f\"\\n发现新的最佳模型! {metric_to_check}: {self.best_metric}, 保存在 {self.best_model_checkpoint}\\n\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"训练结束时的处理逻辑\"\"\"\n",
    "        # 训练结束时保存最终轮次的模型\n",
    "        # 只在主进程执行，避免多进程重复保存\n",
    "        if state.is_world_process_zero:  # 仅在主进程执行\n",
    "            # 1. 保存最终轮次的LoRA模型\n",
    "            # LoRA模型只包含训练的适配器权重，文件较小\n",
    "            final_lora_path = os.path.join(args.output_dir, \"final_lora_model\")\n",
    "            os.makedirs(final_lora_path, exist_ok=True)\n",
    "            # 保存当前状态（最后一轮）的模型\n",
    "            if \"model\" in kwargs:\n",
    "                kwargs[\"model\"].save_pretrained(final_lora_path)\n",
    "                print(f\"最终轮次的LoRA模型已保存至: {final_lora_path}\")\n",
    "\n",
    "            # 2. 合并最终轮次的完整模型\n",
    "            # 将LoRA权重合并到基础模型中，生成可以直接使用的完整模型\n",
    "            print(\"合并并保存最终轮次的完整模型...\")\n",
    "            # 重新加载干净的基础模型\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_PATH,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # 加载LoRA权重并合并\n",
    "            final_lora_model = PeftModel.from_pretrained(base_model, final_lora_path)\n",
    "            final_merged_model = final_lora_model.merge_and_unload()  # 合并权重并卸载LoRA层\n",
    "\n",
    "            # 保存合并后的完整模型\n",
    "            final_complete_path = os.path.join(args.output_dir, \"final_complete_model\")\n",
    "            os.makedirs(final_complete_path, exist_ok=True)\n",
    "            final_merged_model.save_pretrained(final_complete_path)\n",
    "\n",
    "            # 保存分词器，确保模型可以直接使用\n",
    "            # 完整模型需要配套的分词器才能正常工作\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "            tokenizer.save_pretrained(final_complete_path)\n",
    "            print(f\"最终轮次的完整模型已保存至: {final_complete_path}\")\n",
    "\n",
    "            # 3. 保存最佳模型信息到文件，方便后续查看和使用\n",
    "            # 创建一个文本文件记录最佳模型的详细信息\n",
    "            with open(os.path.join(args.output_dir, \"best_model_info.txt\"), \"w\") as f:\n",
    "                f.write(f\"最佳模型checkpoint路径: {self.best_model_checkpoint}\\n\")\n",
    "                f.write(f\"最佳{args.metric_for_best_model}: {self.best_metric}\")\n",
    "            print(f\"最佳模型信息已保存至: {os.path.join(args.output_dir, 'best_model_info.txt')}\")\n",
    "\n",
    "# 创建自定义回调实例\n",
    "# 实例化我们定义的回调类，准备在训练过程中使用\n",
    "best_and_last_callback = SaveBestAndLastModelCallback()\n",
    "\n",
    "# 创建Trainer实例\n",
    "# Trainer是Hugging Face提供的高级训练接口，封装了完整的训练流程\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 要训练的模型（已应用LoRA的PEFT模型）\n",
    "    args=training_args,             # 训练参数配置（学习率、批大小等）\n",
    "    train_dataset=train_dataset,    # 训练数据集（包含单轮和多轮对话数据）\n",
    "    eval_dataset=eval_dataset,      # 验证数据集（用于评估模型性能）\n",
    "    data_collator=data_collator,    # 数据整理器（处理批次组装和填充）\n",
    "    callbacks=[\n",
    "        # 早停回调：防止过拟合的重要机制\n",
    "        # 如果连续5次评估验证损失没有改善超过0.002，就自动停止训练\n",
    "        # early_stopping_patience=5: 容忍5次没有改善（比之前的3次更宽松）\n",
    "        # early_stopping_threshold=0.002: 改善的最小阈值（比之前的0.005更严格）\n",
    "        # 这样的设置适合多轮对话的复杂训练，给模型更多时间学习但要求更明显的改善\n",
    "        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.002),\n",
    "        # 内存优化回调：在关键时刻清理显存，防止显存不足\n",
    "        AggressiveMemoryOptimizationCallback(),\n",
    "        # 最佳模型跟踪回调：自动保存最佳模型和最终模型\n",
    "        best_and_last_callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 开始训练 - 优先尝试从最新checkpoint恢复\n",
    "# 支持断点续训功能，如果训练中断可以从上次保存的地方继续\n",
    "last_checkpoint_dir = \"/root/autodl-tmp/finetune_output/checkpoint-30\"  # 检查点路径（指定要恢复的检查点）\n",
    "if os.path.exists(last_checkpoint_dir):\n",
    "    # 如果找到检查点文件，从断点继续训练\n",
    "    # 这样可以节省时间，不需要从头开始训练\n",
    "    # 从上次中断的地方继续\n",
    "    print(f\"从checkpoint {last_checkpoint_dir} 恢复训练...\")\n",
    "    # resume_from_checkpoint: 指定要恢复的检查点路径\n",
    "    # ignore_keys_for_eval: 在评估时忽略某些键，避免版本兼容性问题\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint_dir, ignore_keys_for_eval=[\"*\"])\n",
    "else:\n",
    "    # 如果没有找到检查点，从头开始训练\n",
    "    # 这是全新训练的情况\n",
    "    # 如果没有检查点，从头开始训练\n",
    "    print(\"从头开始训练...\")\n",
    "    trainer.train()\n",
    "\n",
    "# 保存最佳LoRA模型 - 利用load_best_model_at_end=True特性\n",
    "# 由于设置了load_best_model_at_end=True，此时trainer.model已经是训练过程中性能最佳的模型\n",
    "best_lora_model_path = os.path.join(OUTPUT_DIR, \"best_lora_model\")\n",
    "os.makedirs(best_lora_model_path, exist_ok=True)\n",
    "# 保存最佳LoRA权重，这个文件比较小，只包含训练的适配器参数\n",
    "trainer.model.save_pretrained(best_lora_model_path)\n",
    "print(f\"最佳LoRA模型已保存至: {best_lora_model_path}\")\n",
    "\n",
    "# 保存完整的最佳模型（合并LoRA权重到基础模型）\n",
    "# 完整模型可以直接使用，不需要额外的LoRA配置\n",
    "print(\"开始合并LoRA权重与基础模型，并保存最佳完整模型...\")\n",
    "# 获取基础模型\n",
    "# 重新加载一个干净的基础模型，用于合并LoRA权重\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,                     # 基础模型路径（这里是之前训练好的模型）\n",
    "    torch_dtype=torch.float16,     # 使用FP16数据类型，保持与训练时一致\n",
    "    low_cpu_mem_usage=True,        # 启用低CPU内存使用模式，减少加载时的内存占用\n",
    "    trust_remote_code=True         # 允许执行自定义代码，确保模型正确加载\n",
    ")\n",
    "\n",
    "# 加载最佳LoRA模型与基础模型合并\n",
    "# 将保存的LoRA权重加载到基础模型上，形成完整的微调模型\n",
    "best_lora_model = PeftModel.from_pretrained(base_model, best_lora_model_path)\n",
    "# 合并LoRA权重到基础模型\n",
    "# merge_and_unload()会将LoRA的权重合并到原始模型参数中，并移除LoRA层\n",
    "# 这样生成的模型就是一个标准的完整模型，可以像普通预训练模型一样使用\n",
    "best_merged_model = best_lora_model.merge_and_unload()\n",
    "\n",
    "# 保存完整最佳模型\n",
    "# 创建保存完整模型的目录，这个模型包含了所有训练后的参数\n",
    "best_complete_model_path = os.path.join(OUTPUT_DIR, \"best_complete_model\")\n",
    "os.makedirs(best_complete_model_path, exist_ok=True)\n",
    "# 保存合并后的完整模型，包含所有权重参数\n",
    "best_merged_model.save_pretrained(best_complete_model_path)\n",
    "# 同时保存分词器，确保模型可以完整使用\n",
    "# 完整的模型部署需要模型权重和分词器两部分\n",
    "tokenizer.save_pretrained(best_complete_model_path)\n",
    "\n",
    "# 打印最终保存信息，告知用户完整模型的保存位置\n",
    "# 这个路径下的模型可以直接用于推理部署，无需额外的LoRA配置\n",
    "print(f\"最佳完整模型已保存至: {best_complete_model_path}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41e27a",
   "metadata": {},
   "source": [
    "第一轮数值输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839d077",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528150012084.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411fb8c",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528145943851.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441910e",
   "metadata": {},
   "source": [
    "最后一轮数值输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab3d7e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528144329599.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544ed51",
   "metadata": {},
   "source": [
    "\n",
    "### 训练阶段\n",
    "```python\n",
    "{'loss': 1.0461, 'grad_norm': 0.0551740787923336, 'learning_rate': 8.136549057710484e-07, 'epoch': 45.77}\n",
    "```\n",
    "\n",
    "**1. loss (训练损失): 1.0461**\n",
    "- **含义**：模型在训练数据上的损失值，衡量模型预测与真实标签的差距\n",
    "- **数值解读**：1.0461是一个中等偏低的损失值，说明模型已经学到了一定的知识\n",
    "- **趋势判断**：理想情况下应该随训练逐渐下降\n",
    "\n",
    "**2. grad_norm (梯度范数): 0.055**\n",
    "- **含义**：梯度的L2范数，反映参数更新的幅度\n",
    "- **数值解读**：0.055是一个较小的值，说明：\n",
    "  - 梯度更新比较温和，训练稳定\n",
    "  - 可能接近收敛状态\n",
    "  - 没有出现梯度爆炸问题\n",
    "\n",
    "**3. learning_rate (学习率): 8.136e-07**\n",
    "- **含义**：当前的学习率，控制参数更新的步长\n",
    "- **数值解读**：8.136e-07 = 0.0000008136，这是一个非常小的学习率\n",
    "- **原因分析**：\n",
    "  - 使用了余弦退火调度器，在第45.77轮时学习率已经衰减得很小\n",
    "  - 小学习率有助于模型精细调优，避免破坏已学到的知识\n",
    "\n",
    "**4. epoch: 45.77**\n",
    "- **含义**：当前训练进度，已完成45.77个完整的数据集遍历\n",
    "- **进度判断**：在总共80轮的训练中，已完成约57%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3688443",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528144313386.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bf09a",
   "metadata": {},
   "source": [
    "\n",
    "### 验证阶段\n",
    "```python\n",
    "{'eval_loss': 1.1038808822631836, 'eval_runtime': 50.9555, 'eval_samples_per_second': 23.707, 'eval_steps_per_second': 1.982, 'epoch': 45.77}\n",
    "```\n",
    "\n",
    "**1. eval_loss (验证损失): 1.1039**\n",
    "- **含义**：模型在验证集上的损失值，衡量泛化能力\n",
    "- **关键作用**：这是判断模型性能最重要的指标\n",
    "\n",
    "**2. eval_runtime: 50.96秒**\n",
    "- **含义**：完成一次验证评估所需的时间\n",
    "\n",
    "**3. eval_samples_per_second: 23.707**\n",
    "- **含义**：每秒处理的验证样本数，反映推理速度\n",
    "\n",
    "**4. eval_steps_per_second: 1.982**\n",
    "- **含义**：每秒完成的验证步数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb55d02",
   "metadata": {},
   "source": [
    "# 6.模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475973a",
   "metadata": {},
   "source": [
    "校验包安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c0714",
   "metadata": {},
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be9f14",
   "metadata": {},
   "source": [
    "### ROUGE定义\n",
    "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) 是一套用于自动评估文本摘要和机器翻译质量的指标体系，由Lin在2004年提出。它通过计算候选文本与参考文本之间的n-gram重叠来衡量生成文本的质量。\n",
    "\n",
    "### 核心原理\n",
    "ROUGE基于**召回率导向**的评估思想，主要通过以下方式计算：\n",
    "\n",
    "```\n",
    "ROUGE-N = Σ(重叠的n-gram数量) / Σ(参考文本中的n-gram总数)\n",
    "```\n",
    "\n",
    "### 主要变体\n",
    "\n",
    "#### 1. **ROUGE-N**\n",
    "- **ROUGE-1**: 基于unigram（单词）的重叠\n",
    "- **ROUGE-2**: 基于bigram（双词组）的重叠\n",
    "- **ROUGE-3**: 基于trigram（三词组）的重叠\n",
    "\n",
    "#### 2. **ROUGE-L**\n",
    "基于**最长公共子序列**(Longest Common Subsequence, LCS)的评估，不要求连续匹配。\n",
    "\n",
    "#### 3. **ROUGE-W**\n",
    "加权最长公共子序列，对连续匹配给予更高权重。\n",
    "\n",
    "#### 4. **ROUGE-S**\n",
    "基于跳跃双词组(Skip-bigram)的评估，允许词汇间有间隔。\n",
    "\n",
    "\n",
    "### ROUGE怎么工作？\n",
    "\n",
    "#### **比喻1：找相同的积木**\n",
    "```\n",
    "标准答案: \"苹果公司今天股价上涨了5%，投资者很高兴\"\n",
    "学生答案: \"苹果股价今天上涨5%，投资者感到高兴\"\n",
    "\n",
    "ROUGE就像在数两堆积木中有多少块是相同的：\n",
    "相同的\"积木\"(词汇): 苹果、股价、今天、上涨、5%、投资者、高兴\n",
    "```\n",
    "\n",
    "#### **比喻2：拼图游戏**\n",
    "- **ROUGE-1**: 看单个拼图块有多少重合 (单词匹配)\n",
    "- **ROUGE-2**: 看两个连在一起的拼图块有多少重合 (词组匹配)  \n",
    "- **ROUGE-L**: 看整体图案的相似度 (结构匹配)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf300555",
   "metadata": {},
   "source": [
    "\n",
    "### 具体例子解释\n",
    "\n",
    "#### **例子1：ROUGE-1 (单词匹配)**\n",
    "```json\n",
    "参考答案: \"今天 天气 很 好\"  (4个词)\n",
    "生成回答: \"今天 天气 不错\"    (3个词)\n",
    "\n",
    "重叠词汇: \"今天\"、\"天气\" (2个词)\n",
    "ROUGE-1 = 2/4 = 0.5 (50%)\n",
    "```\n",
    "\n",
    "#### **例子2：ROUGE-2 (词组匹配)**\n",
    "```json\n",
    "参考答案: \"今天天气\" \"天气很\" \"很好\"     (3个词组)\n",
    "生成回答: \"今天天气\" \"天气不\" \"不错\"     (3个词组)\n",
    "\n",
    "重叠词组: \"今天天气\" (1个词组)\n",
    "ROUGE-2 = 1/3 = 0.33 (33%)\n",
    "```\n",
    "\n",
    "#### **例子3：ROUGE-L (整体结构)**\n",
    "```json\n",
    "参考答案: \"苹果公司股价上涨\"\n",
    "生成回答: \"苹果股价大幅上涨\"\n",
    "\n",
    "最长公共子序列: \"苹果\" + \"股价\" + \"上涨\" = 3个词\n",
    "ROUGE-L = 3/4 = 0.75 (75%)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360a8eb",
   "metadata": {},
   "source": [
    "\n",
    "###  为什么需要不同的ROUGE？\n",
    "\n",
    "#### **ROUGE-1**: 词汇准确性\n",
    "```json\n",
    "就像检查作文用词是否正确\n",
    "- 高分: 用词准确，专业术语使用恰当\n",
    "- 低分: 用词不当，关键词缺失\n",
    "```\n",
    "\n",
    "#### **ROUGE-2**: 语法连贯性\n",
    "```json\n",
    "就像检查句子是否通顺\n",
    "- 高分: 词汇搭配自然，语法正确\n",
    "- 低分: 词汇搭配奇怪，语法错误\n",
    "```\n",
    "\n",
    "#### **ROUGE-L**: 整体逻辑性\n",
    "```json\n",
    "就像检查文章结构是否合理\n",
    "- 高分: 逻辑清晰，结构完整\n",
    "- 低分: 逻辑混乱，结构松散\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8769e",
   "metadata": {},
   "source": [
    "## 困惑度(Perplexity)详解\n",
    "\n",
    "### 正式介绍\n",
    "\n",
    "#### **定义**\n",
    "困惑度是衡量**语言模型**质量的内在指标，它测量模型对给定文本序列的预测不确定性。困惑度越低，表示模型对语言的理解越好。\n",
    "\n",
    "#### 猜词游戏\n",
    "想象你在玩一个猜词游戏：\n",
    "\n",
    "```json\n",
    "句子: \"我今天去___买东西\"\n",
    "\n",
    "如果你的大脑(模型)很聪明：\n",
    "- 可能的选择: \"超市\"、\"商店\"、\"市场\" (3个选择)\n",
    "- 困惑度 ≈ 3\n",
    "\n",
    "如果你的大脑(模型)很困惑：\n",
    "- 可能的选择: \"超市\"、\"学校\"、\"火星\"、\"昨天\"... (20个选择)\n",
    "- 困惑度 ≈ 20\n",
    "```\n",
    "\n",
    "#### **核心理解**\n",
    "- **困惑度 = 模型在预测下一个词时的\"选择困难程度\"**\n",
    "- **数值越小 = 模型越\"确定\" = 语言理解能力越强**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4976633",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 困惑度 vs ROUGE 对比\n",
    "\n",
    "### **评估维度对比**\n",
    "\n",
    "| 维度 | 困惑度(Perplexity) | ROUGE |\n",
    "|------|-------------------|-------|\n",
    "| **评估对象** | 模型的语言理解能力 | 生成文本的质量 |\n",
    "| **计算方式** | 基于概率预测 | 基于文本重叠 |\n",
    "| **数值方向** | 越小越好 | 越大越好 |\n",
    "| **评估阶段** | 可以单独评估模型 | 需要对比参考答案 |\n",
    "| **反映能力** | 内在语言能力 | 外在生成效果 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5951e64",
   "metadata": {},
   "source": [
    "\n",
    "### 具体区别\n",
    "\n",
    "#### 困惑度：内在能力测试\n",
    "```python\n",
    "# 困惑度测试\n",
    "输入: \"苹果公司的股价\"\n",
    "模型内部: 计算每个可能下一词的概率\n",
    "- \"上涨\" (概率: 0.4)\n",
    "- \"下跌\" (概率: 0.3)  \n",
    "- \"稳定\" (概率: 0.2)\n",
    "- 其他词 (概率: 0.1)\n",
    "\n",
    "困惑度 = 1/概率分布的确定性\n",
    "```\n",
    "\n",
    "#### ROUGE：外在效果测试\n",
    "```python\n",
    "# ROUGE测试\n",
    "参考答案: \"苹果公司股价今日上涨5%\"\n",
    "模型生成: \"苹果股价上涨了5个百分点\"\n",
    "\n",
    "ROUGE-1: 计算词汇重叠度\n",
    "ROUGE-2: 计算词组重叠度\n",
    "ROUGE-L: 计算结构相似度\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f92df",
   "metadata": {},
   "source": [
    "## 验证主代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa13d6",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from rouge import Rouge\n",
    "\n",
    "# 定义命令行参数解析器\n",
    "# 这个工具允许用户在运行脚本时通过命令行传入不同的参数，而不需要修改代码\n",
    "parser = argparse.ArgumentParser(description='评估微调后的模型效果')\n",
    "parser.add_argument('--model_path', type=str, default=\"/root/autodl-tmp/finetune_output/fianl_complete_model\", \n",
    "                    help='微调后的完整模型路径')  # 指定要评估的模型位置\n",
    "parser.add_argument('--test_file', type=str, default=\"/root/autodl-tmp/data/sft/deepspeek_test_sft_dataset_0_2k.jsonl\", \n",
    "                    help='测试数据集路径')  # 指定测试数据的位置\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='批次大小')  # 一次处理多少个样本\n",
    "parser.add_argument('--max_length', type=int, default=1024, help='最大序列长度')  # 输入文本的最大长度限制\n",
    "parser.add_argument('--device', type=str, default='cuda:0', help='使用的设备，可以是cuda:0或auto等')  # 指定使用GPU还是CPU\n",
    "parser.add_argument('--sample_size', type=int, default=200, help='评估样本数量，设为-1使用全部')  # 评估多少个样本\n",
    "parser.add_argument('--max_new_tokens', type=int, default=2048, \n",
    "                    help='生成的最大新token数量')  # 模型生成回答时的最大长度\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 启用TensorFloat32优化\n",
    "# 这是一种GPU计算优化技术，可以提高计算速度而不显著影响精度\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# 自定义数据集类，用于加载和预处理标准SFT格式的测试数据\n",
    "# Dataset是PyTorch提供的数据集基类，我们继承它来创建自己的数据集\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=1024):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        参数:\n",
    "            data_path: 数据文件路径 - 测试数据存放的位置\n",
    "            tokenizer: 分词器 - 将文本转换为模型能理解的数字序列\n",
    "            max_length: 最大序列长度 - 防止文本过长导致内存不足\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer  # 保存分词器，用于后续文本处理\n",
    "        self.max_length = max_length  # 保存最大长度限制\n",
    "        self.instructions = []  # 存储指令 - 告诉模型要做什么的部分\n",
    "        self.inputs = []        # 存储输入 - 具体的问题或背景信息\n",
    "        self.outputs = []       # 存储期望输出 - 标准答案，用于对比评估\n",
    "        self.prompts = []       # 存储完整提示 - 指令+输入组合成的完整问题\n",
    "        \n",
    "        total_lines = 0      # 统计文件总行数\n",
    "        processed_lines = 0  # 统计成功处理的行数\n",
    "        \n",
    "        try:\n",
    "            # 打开测试数据文件，每行是一个JSON格式的样本\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    total_lines += 1\n",
    "                    try:\n",
    "                        # 解析JSON格式的数据\n",
    "                        item = json.loads(line)\n",
    "                        \n",
    "                        # 提取标准SFT格式的字段\n",
    "                        # instruction: 指令，告诉模型要执行什么任务\n",
    "                        # input: 输入，具体的问题或数据\n",
    "                        # output: 输出，期望的答案\n",
    "                        instruction = item.get(\"instruction\", \"\")\n",
    "                        input_text = item.get(\"input\", \"\")\n",
    "                        output = item.get(\"output\", \"\")\n",
    "                        \n",
    "                        # 确保有输出 - 没有标准答案就无法评估\n",
    "                        if output:\n",
    "                            self.instructions.append(instruction)\n",
    "                            self.inputs.append(input_text)\n",
    "                            self.outputs.append(output)\n",
    "                            \n",
    "                            # 构建提示模板 - 将指令和输入组合成完整的问题\n",
    "                            # 这个格式要与训练时使用的格式保持一致\n",
    "                            if input_text:\n",
    "                                prompt = f\"{instruction}\\n\\n{input_text}\\n\"\n",
    "                            else:\n",
    "                                prompt = f\"{instruction}\\n\"\n",
    "                                \n",
    "                            self.prompts.append(prompt)\n",
    "                            processed_lines += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理第{total_lines}行时出错: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"打开文件时出错: {e}\")\n",
    "        \n",
    "        # 打印数据加载统计信息，帮助用户了解数据质量\n",
    "        print(f\"数据集{data_path} - 总行数: {total_lines}, 成功处理: {processed_lines}, 最终样本数: {len(self.prompts)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小 - PyTorch要求实现这个方法\"\"\"\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"获取指定索引的样本 - PyTorch要求实现这个方法\"\"\"\n",
    "        return {\n",
    "            \"prompt\": self.prompts[idx],        # 完整的问题\n",
    "            \"output\": self.outputs[idx],        # 标准答案\n",
    "            \"instruction\": self.instructions[idx],  # 指令部分\n",
    "            \"input\": self.inputs[idx]           # 输入部分\n",
    "        }\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, device, batch_size=16):\n",
    "    \"\"\"\n",
    "    计算模型在测试集上的困惑度\n",
    "    \n",
    "    困惑度(Perplexity)是评估语言模型的重要指标：\n",
    "    - 困惑度越低，说明模型对文本的预测越准确\n",
    "    - 可以理解为模型在预测下一个词时的\"困惑程度\"\n",
    "    - 数学上等于损失函数的指数：perplexity = exp(loss)\n",
    "    \n",
    "    参数:\n",
    "        model: 待评估的模型\n",
    "        tokenizer: 分词器\n",
    "        dataset: 测试数据集\n",
    "        device: 计算设备(GPU或CPU)\n",
    "        batch_size: 批次大小，一次处理多少个样本\n",
    "    \n",
    "    返回:\n",
    "        perplexity: 困惑度值，越低表示模型效果越好\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置为评估模式，关闭dropout等训练时的随机性\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)  # 创建数据加载器，支持批量处理\n",
    "    \n",
    "    total_loss = 0  # 累积总损失\n",
    "    total_tokens = 0  # 累积总token数，用于计算平均损失\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度，节省内存和计算时间\n",
    "        for batch in tqdm(dataloader, desc=\"计算困惑度\"):  # tqdm显示进度条\n",
    "            prompts = batch[\"prompt\"]    # 获取问题\n",
    "            outputs = batch[\"output\"]    # 获取标准答案\n",
    "            \n",
    "            # 构建输入序列 - 提示+输出\n",
    "            # 这样模型需要预测整个回答，我们可以计算预测的准确性\n",
    "            texts = [p + o for p, o in zip(prompts, outputs)]\n",
    "            \n",
    "            # 批量编码 - 将文本转换为模型能理解的数字序列\n",
    "            encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=args.max_length)\n",
    "            input_ids = encodings[\"input_ids\"].to(device)        # 输入序列\n",
    "            attention_mask = encodings[\"attention_mask\"].to(device)  # 注意力掩码，告诉模型哪些位置是有效的\n",
    "            \n",
    "            # 创建标签并找到提示结束位置\n",
    "            # 标签用于计算损失，我们只想计算回答部分的损失，不包括问题部分\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # 批量处理标签\n",
    "            for i, p in enumerate(prompts):\n",
    "                # 计算提示token数量\n",
    "                prompt_tokens = len(tokenizer.encode(p, add_special_tokens=False))\n",
    "                # 将提示部分的标签设为-100，这样计算损失时会忽略这部分\n",
    "                # 只计算模型生成回答部分的损失\n",
    "                labels[i, :prompt_tokens] = -100\n",
    "            \n",
    "            # 计算损失 - 模型预测与真实答案的差距\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # 累加损失和有效token数\n",
    "            # 只统计非忽略的token（即回答部分）\n",
    "            non_ignored = (labels != -100).sum().item()\n",
    "            total_loss += loss.item() * non_ignored\n",
    "            total_tokens += non_ignored\n",
    "            \n",
    "            # 清理GPU内存，防止内存溢出\n",
    "            del input_ids, attention_mask, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 计算困惑度 = exp(平均损失)\n",
    "    # 平均损失越小，困惑度越小，模型效果越好\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "    return perplexity\n",
    "\n",
    "def generate_answers(model, tokenizer, dataset, device, batch_size=32, max_new_tokens=2048):\n",
    "    \"\"\"\n",
    "    使用模型批量生成回答 - 优化版，防止输出不完整\n",
    "    \n",
    "    这个函数让模型根据问题生成回答，然后与标准答案对比\n",
    "    这是评估模型实际应用效果的重要方法\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置为评估模式\n",
    "    generated_answers = []  # 存储模型生成的回答\n",
    "    reference_answers = []  # 存储标准答案\n",
    "    \n",
    "    # 创建批次 - 将数据分批处理，提高效率\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_batches = len(dataloader)\n",
    "    print(f\"开始生成回答，共{total_batches}个批次...\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"生成回答\")):\n",
    "        prompts = batch[\"prompt\"]      # 获取问题\n",
    "        batch_outputs = batch[\"output\"]  # 获取标准答案\n",
    "        \n",
    "        try:\n",
    "            # 批量编码输入 - 将问题转换为模型能理解的格式\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # 增强生成参数，确保输出完整\n",
    "            # 这些参数控制模型如何生成回答\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,      # 最大生成长度，确保回答足够完整\n",
    "                    min_new_tokens=100,       # 最小生成长度，强制生成一定长度的回答\n",
    "                    do_sample=True,           # 使用采样而非贪心搜索，增加回答多样性\n",
    "                    temperature=0.8,          # 温度参数，控制生成的随机性，0.8是较好的平衡\n",
    "                    top_p=0.95,               # 核采样参数，只考虑概率最高的95%的词汇\n",
    "                    num_beams=4,              # 束搜索数量，平衡质量和速度\n",
    "                    repetition_penalty=1.2,   # 重复惩罚，避免生成重复内容\n",
    "                    length_penalty=1.0,       # 长度惩罚，1.0表示不惩罚长度\n",
    "                    no_repeat_ngram_size=3,   # 避免重复的n-gram大小\n",
    "                    pad_token_id=tokenizer.eos_token_id,  # 填充token\n",
    "                    # 注意：在生成完整测试时可以临时禁用EOS token\n",
    "                    # eos_token_id=None,      # 如果想要强制生成更多内容，可以禁用EOS\n",
    "                )\n",
    "            \n",
    "            # 解码生成的文本并提取回答\n",
    "            batch_generated = []\n",
    "            for i, output in enumerate(outputs):\n",
    "                # 将数字序列转换回文本\n",
    "                generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "                # 提取回答部分（去掉问题部分）\n",
    "                generated_answer = generated_text[len(prompts[i]):]\n",
    "                batch_generated.append(generated_answer)\n",
    "                generated_answers.append(generated_answer)\n",
    "                reference_answers.append(batch_outputs[i])\n",
    "            \n",
    "            # 检查生成内容的长度 - 用于诊断生成质量\n",
    "            if batch_idx == 0:  # 只在第一批显示详细信息\n",
    "                token_lengths = [len(tokenizer.encode(ans)) for ans in batch_generated]\n",
    "                avg_length = sum(token_lengths) / len(token_lengths) if token_lengths else 0\n",
    "                print(f\"第一批样本的平均生成长度: {avg_length:.1f} tokens\")\n",
    "                print(f\"样本长度分布: 最短 {min(token_lengths) if token_lengths else 0}，最长 {max(token_lengths) if token_lengths else 0} tokens\")\n",
    "                \n",
    "                # 打印第一个样本详情 - 帮助用户了解生成效果\n",
    "                print(f\"指令示例: {batch['instruction'][0]}\")\n",
    "                if batch['input'][0]:\n",
    "                    print(f\"输入示例: {batch['input'][0]}\")\n",
    "                print(f\"生成回答示例(前200字符): {batch_generated[0][:200]}...\")\n",
    "                print(f\"参考回答示例(前200字符): {batch_outputs[0][:200]}...\")\n",
    "                print(f\"生成回答token数: {token_lengths[0] if token_lengths else 0}\")\n",
    "            \n",
    "            # 按需清理GPU内存 - 防止内存溢出\n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                del input_ids, attention_mask, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"生成回答时出错 (批次 {batch_idx}/{total_batches}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 最终清理内存\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 生成完成后分析生成长度 - 评估生成质量\n",
    "    if generated_answers:\n",
    "        token_lengths = [len(tokenizer.encode(ans)) for ans in generated_answers]\n",
    "        avg_length = sum(token_lengths) / len(token_lengths)\n",
    "        print(f\"\\n生成完成，共生成{len(generated_answers)}个回答\")\n",
    "        print(f\"生成回答平均长度: {avg_length:.1f} tokens\")\n",
    "        print(f\"生成回答长度分布: 最短 {min(token_lengths)}，最长 {max(token_lengths)} tokens\")\n",
    "        \n",
    "        # 检查生成是否被截断 - 过短的回答可能质量不佳\n",
    "        short_responses = sum(1 for l in token_lengths if l < 100)\n",
    "        if short_responses > 0:\n",
    "            print(f\"警告: 有{short_responses}个回答长度小于100个token，占比{short_responses/len(token_lengths)*100:.1f}%\")\n",
    "    \n",
    "    return generated_answers, reference_answers\n",
    "\n",
    "def evaluate_rouge(generated_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    计算生成回答与参考答案的ROUGE分数\n",
    "    \n",
    "    ROUGE(Recall-Oriented Understudy for Gisting Evaluation)是评估文本生成质量的重要指标：\n",
    "    - ROUGE-1: 基于单词重叠的评估，衡量词汇覆盖度\n",
    "    - ROUGE-2: 基于双词组重叠的评估，衡量语法连贯性\n",
    "    - ROUGE-L: 基于最长公共子序列的评估，衡量整体结构相似性\n",
    "    \n",
    "    分数范围0-1，越高表示生成的回答与标准答案越相似\n",
    "    \n",
    "    参数:\n",
    "        generated_answers: 生成的回答列表\n",
    "        reference_answers: 参考答案列表\n",
    "    \n",
    "    返回:\n",
    "        rouge_scores: 包含ROUGE-1/2/L分数的字典\n",
    "    \"\"\"\n",
    "    rouge = Rouge()  # 初始化ROUGE评估器\n",
    "    \n",
    "    # 确保所有文本不为空，空文本会导致ROUGE计算错误\n",
    "    valid_pairs = [(g, r) for g, r in zip(generated_answers, reference_answers) \n",
    "                 if len(g.strip()) > 0 and len(r.strip()) > 0]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {\"rouge-1\": 0, \"rouge-2\": 0, \"rouge-l\": 0}\n",
    "    \n",
    "    gen_valid, ref_valid = zip(*valid_pairs)\n",
    "    \n",
    "    try:\n",
    "        # 计算ROUGE分数\n",
    "        scores = rouge.get_scores(gen_valid, ref_valid, avg=True)\n",
    "        return {\n",
    "            \"rouge-1\": scores[\"rouge-1\"][\"f\"],  # ROUGE-1 F1分数\n",
    "            \"rouge-2\": scores[\"rouge-2\"][\"f\"],  # ROUGE-2 F1分数\n",
    "            \"rouge-l\": scores[\"rouge-l\"][\"f\"]   # ROUGE-L F1分数\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"计算ROUGE得分时出错: {e}\")\n",
    "        return {\"rouge-1\": 0, \"rouge-2\": 0, \"rouge-l\": 0}\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，执行整个评估流程\n",
    "    \n",
    "    评估流程：\n",
    "    1. 加载模型和分词器\n",
    "    2. 加载测试数据\n",
    "    3. 计算困惑度（衡量模型对语言的理解能力）\n",
    "    4. 生成回答（测试模型的实际应用能力）\n",
    "    5. 计算ROUGE分数（衡量生成质量）\n",
    "    6. 保存评估结果\n",
    "    \"\"\"\n",
    "    print(f\"加载模型: {args.model_path}\")\n",
    "    # 加载分词器 - 负责将文本转换为模型能理解的数字序列\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # 确保有填充token，用于批量处理\n",
    "    # 设置默认padding侧为右侧 - 影响批量处理的对齐方式\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # 处理device_map参数，使用更高效的自动分配\n",
    "    # 自动分配可以让模型在多个GPU上分布，提高处理速度\n",
    "    if args.device == \"auto\" or \",\" in args.device:\n",
    "        device_map = \"auto\"\n",
    "        print(\"使用自动设备映射进行多GPU分配\")\n",
    "    else:\n",
    "        device_map = args.device\n",
    "    \n",
    "    # 加载模型，使用BF16格式提高效率和显存利用率\n",
    "    # BF16是一种半精度浮点格式，可以节省显存并加速计算\n",
    "    print(\"使用BF16格式加载模型以提高显存利用率\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_path,\n",
    "            torch_dtype=torch.bfloat16,  # 尝试使用bfloat16以获得更好的精度和显存利用率\n",
    "            device_map=device_map,       # 设备映射，支持多GPU\n",
    "            trust_remote_code=True,      # 允许执行自定义代码\n",
    "            max_memory={i: f\"{int(torch.cuda.get_device_properties(i).total_memory * 0.85 / 1024**3)}GiB\" \n",
    "                     for i in range(torch.cuda.device_count())}  # 最大限度利用显存，使用85%的可用显存\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"使用BF16加载失败，回退到FP16: {e}\")\n",
    "        # 如果BF16不支持，回退到FP16\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    \n",
    "    # 尝试使用torch.compile加速\n",
    "    # torch.compile是PyTorch 2.0的新特性，可以显著加速模型推理\n",
    "    if torch.__version__ >= \"2.0.0\" and torch.cuda.is_available():\n",
    "        try:\n",
    "            print(\"使用torch.compile()加速模型...\")\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            print(f\"模型编译失败，将使用原始模型: {e}\")\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    test_dataset = SFTDataset(args.test_file, tokenizer, max_length=args.max_length)\n",
    "    print(f\"加载了{len(test_dataset)}个测试样例\")\n",
    "    \n",
    "    # 设置计算设备\n",
    "    if args.device == \"auto\":\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(args.device.split(\",\")[0] if \",\" in args.device else args.device)\n",
    "    \n",
    "    # 计算困惑度 - 增加batch_size以充分利用显存\n",
    "    print(\"计算模型困惑度...\")\n",
    "    # 优化batch_size，根据可用GPU数量调整\n",
    "    # 更多GPU意味着可以处理更大的批次\n",
    "    adjusted_batch_size = args.batch_size * max(1, torch.cuda.device_count())\n",
    "    print(f\"自动调整批处理大小为: {adjusted_batch_size}\")\n",
    "    \n",
    "    # 计算困惑度 - 这是评估语言模型最基础的指标\n",
    "    perplexity = compute_perplexity(model, tokenizer, test_dataset, device, batch_size=adjusted_batch_size)\n",
    "    print(f\"模型困惑度: {perplexity:.4f}\")\n",
    "    \n",
    "    # 使用全部样本或采样\n",
    "    # 如果测试集很大，可以只评估一部分样本以节省时间\n",
    "    if args.sample_size > 0 and args.sample_size < len(test_dataset):\n",
    "        sampled_indices = np.random.choice(len(test_dataset), args.sample_size, replace=False)\n",
    "        sampled_dataset = torch.utils.data.Subset(test_dataset, sampled_indices)\n",
    "        print(f\"从{len(test_dataset)}个样例中随机采样{args.sample_size}个进行评估\")\n",
    "        dataset_for_generation = sampled_dataset\n",
    "    else:\n",
    "        dataset_for_generation = test_dataset\n",
    "        print(f\"使用全部{len(test_dataset)}个样例进行评估\")\n",
    "    \n",
    "    # 生成回答，使用调整后的batch_size\n",
    "    # 这是测试模型实际应用能力的关键步骤\n",
    "    generated_answers, reference_answers = generate_answers(model, tokenizer, dataset_for_generation, device, batch_size=adjusted_batch_size, max_new_tokens=args.max_new_tokens)\n",
    "    \n",
    "    # 计算ROUGE得分\n",
    "    # ROUGE分数衡量生成的回答与标准答案的相似程度\n",
    "    print(\"计算ROUGE得分...\")\n",
    "    rouge_scores = evaluate_rouge(generated_answers, reference_answers)\n",
    "    print(f\"ROUGE-1: {rouge_scores['rouge-1']:.4f}\")  # 词汇重叠度\n",
    "    print(f\"ROUGE-2: {rouge_scores['rouge-2']:.4f}\")  # 双词组重叠度\n",
    "    print(f\"ROUGE-L: {rouge_scores['rouge-l']:.4f}\")  # 最长公共子序列相似度\n",
    "    \n",
    "    # 创建结果目录\n",
    "    results_dir = os.path.join(os.path.dirname(args.model_path), \"evaluation_results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存评估指标\n",
    "    metrics = {\n",
    "        \"perplexity\": perplexity,    # 困惑度\n",
    "        \"rouge\": rouge_scores        # ROUGE分数\n",
    "    }\n",
    "    with open(os.path.join(results_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 在保存前添加检查\n",
    "    print(f\"生成的回答数量: {len(generated_answers)}\")\n",
    "    print(f\"参考答案数量: {len(reference_answers)}\")\n",
    "    if len(generated_answers) > 0:\n",
    "        print(f\"第一个生成的回答样例: {generated_answers[0][:100]}...\")\n",
    "\n",
    "    # 保存生成的回答与参考答案对比\n",
    "    # 这些样例可以用于人工检查模型的生成质量\n",
    "    with open(os.path.join(results_dir, \"generation_samples.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, idx in enumerate(range(len(generated_answers))):\n",
    "            if idx < len(generated_answers) and idx < len(reference_answers):\n",
    "                # 获取原始样本索引（如果使用了采样）\n",
    "                orig_idx = idx\n",
    "                if isinstance(dataset_for_generation, torch.utils.data.Subset):\n",
    "                    orig_idx = dataset_for_generation.indices[idx]\n",
    "                \n",
    "                # 构建结果样本\n",
    "                sample = {\n",
    "                    \"instruction\": test_dataset.instructions[orig_idx] if orig_idx < len(test_dataset.instructions) else \"\",\n",
    "                    \"input\": test_dataset.inputs[orig_idx] if orig_idx < len(test_dataset.inputs) else \"\",\n",
    "                    \"reference_output\": reference_answers[idx],  # 标准答案\n",
    "                    \"generated_output\": generated_answers[idx]   # 模型生成的答案\n",
    "                }\n",
    "                f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    # 在保存生成结果前添加长度分析\n",
    "    # 分析生成回答的长度分布，帮助判断模型是否生成了合适长度的回答\n",
    "    token_length_analysis = {}\n",
    "    if len(generated_answers) > 0:\n",
    "        token_lengths = [len(tokenizer.encode(ans)) for ans in generated_answers]\n",
    "        breaks = [0, 100, 200, 500, 1000, 2000, 5000]  # 长度区间\n",
    "        for i in range(len(breaks)-1):\n",
    "            count = sum(1 for l in token_lengths if breaks[i] <= l < breaks[i+1])\n",
    "            token_length_analysis[f\"{breaks[i]}-{breaks[i+1]-1}\"] = {\n",
    "                \"count\": count,\n",
    "                \"percentage\": f\"{count/len(token_lengths)*100:.1f}%\"\n",
    "            }\n",
    "        \n",
    "        # 打印分析结果\n",
    "        print(\"\\n生成回答长度分布:\")\n",
    "        for range_name, stats in token_length_analysis.items():\n",
    "            print(f\"  {range_name} tokens: {stats['count']} 个回答 ({stats['percentage']})\")\n",
    "        \n",
    "        # 将分析结果添加到metrics中\n",
    "        metrics[\"token_length_analysis\"] = token_length_analysis\n",
    "    \n",
    "    print(f\"评估结果已保存到 {results_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b6589",
   "metadata": {},
   "source": [
    "```bash\n",
    "python evaluate_model.py --model_path /root/autodl-tmp/目标模型路径 --test_file /root/autodl-tmp/data/sft/deepspeek_sft_dataset_1k.jsonl --batch_size 8 --max_length 1024 --device auto --sample_size 400 --max_new_tokens 1024\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f529adb",
   "metadata": {},
   "source": [
    "一轮调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7365b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528153726407.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bff5f5",
   "metadata": {},
   "source": [
    "三轮微调后模型验证值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276ae6e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528150855309.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e0dad",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528150947847.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61c35c",
   "metadata": {},
   "source": [
    "原生模型检验值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8e082",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528151423710.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0e5b7",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528151552039.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72288e",
   "metadata": {},
   "source": [
    "### 1. **Perplexity (困惑度)**\n",
    "```\n",
    "原生模型: 7.627673149108887\n",
    "微调模型: 2.877972602842383\n",
    "```\n",
    "\n",
    "**含义**：困惑度衡量模型对语言的理解能力\n",
    "- **数值越小越好**\n",
    "- **原理**：困惑度 = exp(平均损失)，表示模型在预测下一个词时的\"困惑程度\"\n",
    "- **改善幅度**：7.63 → 2.88，**下降了62.3%** ✅\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f07fb3",
   "metadata": {},
   "source": [
    "\n",
    "### 2. **ROUGE分数**\n",
    "```json\n",
    "指标          原生模型    微调模型    变化\n",
    "ROUGE-1      0.0999      0.2074     +213%\n",
    "ROUGE-2      0.0014      0.0763     +5350%\n",
    "ROUGE-L      0.0989      0.2044     +225%\n",
    "```\n",
    "\n",
    "**含义**：\n",
    "- **ROUGE-1**：单词重叠度，衡量词汇使用准确性\n",
    "- **ROUGE-2**：双词组重叠度，衡量语法连贯性  \n",
    "- **ROUGE-L**：最长公共子序列，衡量整体结构相似性\n",
    "- **数值越大越好**（范围0-1）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35caa423",
   "metadata": {},
   "source": [
    "\n",
    "## 微调效果评估\n",
    "\n",
    "### **总体评价：显著改善**\n",
    "\n",
    "#### **困惑度分析**\n",
    "```python\n",
    "# 困惑度评级标准\n",
    "def evaluate_perplexity(ppl):\n",
    "    if ppl < 2.0:\n",
    "        return \"优秀\"\n",
    "    elif ppl < 5.0:\n",
    "        return \"良好\"      # 微调模型在这里\n",
    "    elif ppl < 10.0:\n",
    "        return \"一般\"      # 原生模型在这里\n",
    "    else:\n",
    "        return \"较差\"\n",
    "\n",
    "原生模型: 7.63 (一般)\n",
    "微调模型: 2.88 (良好)\n",
    "```\n",
    "\n",
    "#### **ROUGE分数分析**\n",
    "```python\n",
    "# ROUGE评级标准\n",
    "def evaluate_rouge(rouge_score):\n",
    "    if rouge_score > 0.4:\n",
    "        return \"优秀\"\n",
    "    elif rouge_score > 0.2:\n",
    "        return \"良好\"      # 微调模型ROUGE-1和ROUGE-L在这里\n",
    "    elif rouge_score > 0.1:\n",
    "        return \"一般\"\n",
    "    else:\n",
    "        return \"较差\"      # 原生模型在这里\n",
    "\n",
    "# 微调模型评级\n",
    "ROUGE-1: 0.207 (良好)\n",
    "ROUGE-2: 0.076 (一般)  \n",
    "ROUGE-L: 0.204 (良好)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79dfaf",
   "metadata": {},
   "source": [
    "\n",
    "## 判断依据和原理\n",
    "\n",
    "### 1. **困惑度原理**\n",
    "```\n",
    "困惑度 = exp(-1/N * Σ log P(w_i | context))\n",
    "\n",
    "其中：\n",
    "- N: 总词数\n",
    "- P(w_i | context): 模型预测词w_i的概率\n",
    "- 困惑度越低 = 模型预测越准确 = 对语言理解越好\n",
    "```\n",
    "\n",
    "**为什么小点好？**\n",
    "- 困惑度2.88意味着模型在预测时平均有2.88个\"合理选择\"\n",
    "- 困惑度7.63意味着模型平均有7.63个\"合理选择\"\n",
    "- 选择越少 = 预测越精准\n",
    "\n",
    "### 2. **ROUGE分数原理**\n",
    "```python\n",
    "# ROUGE-1计算示例\n",
    "参考答案: \"苹果公司股价上涨了5%\"\n",
    "生成回答: \"苹果股价上涨5%\"\n",
    "\n",
    "重叠词汇: [\"苹果\", \"股价\", \"上涨\", \"5%\"] = 4个\n",
    "参考答案总词数: 7个\n",
    "ROUGE-1 = 4/7 = 0.57\n",
    "```\n",
    "\n",
    "**为什么大点好？**\n",
    "- 分数越高 = 与标准答案重叠越多 = 生成质量越好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1be7c0",
   "metadata": {},
   "source": [
    "## 结论和建议\n",
    "\n",
    "### ✅ **微调成功的证据**\n",
    "1. **困惑度大幅下降**：从7.63降至2.88，语言理解能力显著提升\n",
    "2. **ROUGE全面改善**：所有指标都有数倍提升\n",
    "3. **专业化程度提高**：在金融领域的表现明显优于通用模型\n",
    "\n",
    "### 🎯 **进一步优化建议**\n",
    "1. **增加训练数据**：提高ROUGE-2分数，改善语法连贯性\n",
    "2. **调整训练参数**：可以尝试更长的训练时间或不同的学习率\n",
    "3. **数据质量优化**：确保训练数据的答案质量更高\n",
    "\n",
    "### 📊 **行业对比标准**\n",
    "```json\n",
    "金融领域模型评估标准:\n",
    "- 困惑度 < 3.0: 可用于生产环境\n",
    "- ROUGE-1 > 0.3: 专业级别\n",
    "- ROUGE-L > 0.3: 结构化回答良好\n",
    "\n",
    "当前状态: 困惑度已达标，ROUGE分数接近专业级别\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909b899",
   "metadata": {},
   "source": [
    "# 7.模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a9bee",
   "metadata": {},
   "source": [
    "安装包依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a28f63",
   "metadata": {},
   "source": [
    "pip install psutil aiohttp requests vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a3d4f",
   "metadata": {},
   "source": [
    "### vLLM 介绍\n",
    "vLLM 是一个高性能的大语言模型推理和服务框架，专门为生产环境中的大模型部署而设计。    \n",
    "### vLLM 是什么？\n",
    "vLLM（very Large Language Model）是一个开源的推理引擎，具有以下特点：     \n",
    "**高吞吐量**：通过 PagedAttention 算法优化内存使用，显著提高推理速度    \n",
    "**易于使用**：提供与 OpenAI API 兼容的接口，方便集成    \n",
    "**灵活部署**：支持多种模型格式和硬件配置   \n",
    "**内存高效**：动态管理 GPU 内存，支持更大的批处理    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec00df5",
   "metadata": {},
   "source": [
    "## 7.1 模型启动"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47186c",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "简化版vLLM模型启动器\n",
    "这个脚本的作用是：启动两个大语言模型服务，让它们可以通过网络接口提供文本生成服务\n",
    "\n",
    "什么是vLLM？\n",
    "- vLLM是一个高性能的大语言模型推理引擎\n",
    "- 它可以把训练好的大模型（如Qwen、ChatGLM等）变成可以通过网络访问的API服务\n",
    "- 就像把一个聪明的AI助手部署到服务器上，让其他程序可以调用它\n",
    "\n",
    "这个脚本的作用：\n",
    "1. 读取配置文件，了解要启动哪些模型\n",
    "2. 使用vLLM把模型启动成网络服务\n",
    "3. 管理这些服务（启动、停止、查看状态）\n",
    "4. 让其他应用（如Dify）可以通过HTTP接口使用这些模型\n",
    "\"\"\"\n",
    "\n",
    "import os           # 用于操作系统相关功能，比如设置环境变量\n",
    "import sys          # 用于获取Python解释器路径\n",
    "import time         # 用于时间相关操作，比如等待\n",
    "import json         # 用于处理JSON格式的配置文件\n",
    "import signal       # 用于处理进程信号，比如停止进程\n",
    "import subprocess   # 用于启动和管理子进程\n",
    "import psutil       # 用于检查进程状态\n",
    "from pathlib import Path  # 用于处理文件路径\n",
    "\n",
    "class SimpleVLLMLauncher:\n",
    "    \"\"\"\n",
    "    简化版vLLM启动器类\n",
    "    \n",
    "    这个类就像一个\"模型服务管理员\"，它的主要职责是：\n",
    "    1. 读取配置文件，了解要启动哪些模型\n",
    "    2. 启动模型服务进程（把模型变成可以网络访问的服务）\n",
    "    3. 管理这些进程（启动、停止、查看状态）\n",
    "    4. 持久化保存进程信息，重启后也能管理之前的进程\n",
    "    \n",
    "    比喻：就像一个餐厅经理，负责开启多个厨房（模型服务），\n",
    "    每个厨房都有自己的菜谱（模型）和服务窗口（端口），\n",
    "    顾客（其他应用）可以通过窗口点菜（发送请求）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化启动器\n",
    "        创建一个空的字典来存储正在运行的进程信息\n",
    "        \n",
    "        就像准备一个记录本，用来记录所有正在运行的模型服务\n",
    "        \"\"\"\n",
    "        self.processes = {}  # 存储所有启动的模型进程信息，格式：{模型名: 进程信息}\n",
    "        self.service_info_file = \"running_services.json\"  # 保存运行中服务信息的文件\n",
    "        # 启动时尝试加载之前保存的服务信息\n",
    "        self.load_existing_services()\n",
    "    \n",
    "    def load_existing_services(self):\n",
    "        \"\"\"\n",
    "        加载之前保存的服务信息\n",
    "        这样即使重启启动器，也能管理之前启动的服务\n",
    "        \n",
    "        比喻：就像餐厅经理上班时，先查看昨天的记录，\n",
    "        看看哪些厨房还在营业，哪些已经关门了\n",
    "        \"\"\"\n",
    "        if Path(self.service_info_file).exists():\n",
    "            try:\n",
    "                # 读取之前保存的服务信息文件\n",
    "                with open(self.service_info_file, 'r', encoding='utf-8') as f:\n",
    "                    service_info = json.load(f)\n",
    "                \n",
    "                print(\"检查之前启动的服务...\")\n",
    "                for model_name, info in service_info.items():\n",
    "                    pid = info.get('pid')  # 获取进程ID\n",
    "                    if pid and self.is_process_running(pid):\n",
    "                        # 如果进程还在运行，重新创建进程对象\n",
    "                        try:\n",
    "                            process = psutil.Process(pid)\n",
    "                            self.processes[model_name] = {\n",
    "                                'process': process,\n",
    "                                'config': {\n",
    "                                    'name': model_name,\n",
    "                                    'port': info['port'],\n",
    "                                    'gpu_devices': info.get('gpu_devices', [])\n",
    "                                },\n",
    "                                'log_file': info.get('log_file', f\"{model_name}_service.log\"),\n",
    "                                'pid': pid\n",
    "                            }\n",
    "                            print(f\"✓ 发现运行中的服务: {model_name} (PID: {pid})\")\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"✗ 服务 {model_name} 已停止 (PID: {pid})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"加载服务信息时出错: {e}\")\n",
    "    \n",
    "    def is_process_running(self, pid):\n",
    "        \"\"\"\n",
    "        检查指定PID的进程是否还在运行\n",
    "        \n",
    "        参数说明：\n",
    "        pid: 进程ID（每个运行的程序都有一个唯一的ID号）\n",
    "        \n",
    "        返回值：\n",
    "        True: 进程还在运行\n",
    "        False: 进程已停止\n",
    "        \n",
    "        比喻：就像查看某个员工是否还在工作岗位上\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return psutil.pid_exists(pid)\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def check_gpu_status(self):\n",
    "        \"\"\"\n",
    "        检查GPU状态\n",
    "        这个方法会调用nvidia-smi命令来查看有哪些GPU可用\n",
    "        \n",
    "        GPU（显卡）是运行大模型的重要硬件，就像厨房的炉灶，\n",
    "        需要确认有多少个炉灶可用，每个炉灶的规格如何\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"正在检查GPU状态...\")\n",
    "            # 运行nvidia-smi命令获取GPU信息\n",
    "            # nvidia-smi是NVIDIA显卡的管理工具，可以查看显卡状态\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.total', \n",
    "                                   '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"可用的GPU:\")\n",
    "                for line in result.stdout.strip().split('\\n'):\n",
    "                    if line:\n",
    "                        parts = line.split(', ')\n",
    "                        gpu_id = parts[0]      # GPU编号\n",
    "                        gpu_name = parts[1]    # GPU名称\n",
    "                        gpu_memory = parts[2]  # GPU内存大小\n",
    "                        print(f\"  GPU {gpu_id}: {gpu_name} ({gpu_memory}MB)\")\n",
    "            else:\n",
    "                print(\"无法获取GPU信息，请确保安装了NVIDIA驱动\")\n",
    "        except Exception as e:\n",
    "            print(f\"检查GPU状态时出错: {e}\")\n",
    "    \n",
    "    def check_port_in_use(self, port):\n",
    "        \"\"\"\n",
    "        检查端口是否被占用\n",
    "        \n",
    "        参数说明：\n",
    "        port: 要检查的端口号（端口就像门牌号，每个网络服务都需要一个唯一的端口）\n",
    "        \n",
    "        返回值：\n",
    "        True: 端口被占用（已经有其他服务在使用）\n",
    "        False: 端口可用（可以使用这个端口）\n",
    "        \n",
    "        比喻：就像检查某个房间号是否已经有人入住\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import socket\n",
    "            # 尝试连接到指定端口，如果能连接成功说明端口被占用\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                result = s.connect_ex(('localhost', port))\n",
    "                return result == 0  # 0表示连接成功，即端口被占用\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def start_single_model(self, model_config):\n",
    "        \"\"\"\n",
    "        启动单个模型服务\n",
    "        \n",
    "        参数说明：\n",
    "        model_config: 字典，包含模型的配置信息\n",
    "        - name: 模型名称（比如\"original\"或\"finetuned\"）\n",
    "        - path: 模型文件路径（模型文件存放的位置）\n",
    "        - port: 服务端口号（这个服务将在哪个端口提供服务）\n",
    "        - gpu_devices: 使用的GPU设备列表（使用哪些显卡）\n",
    "        - 其他配置参数\n",
    "        \n",
    "        比喻：就像开启一个新的厨房，需要指定：\n",
    "        - 厨房名称\n",
    "        - 菜谱位置（模型路径）\n",
    "        - 服务窗口号（端口）\n",
    "        - 使用哪些炉灶（GPU）\n",
    "        \"\"\"\n",
    "        # 从配置中提取信息\n",
    "        model_name = model_config['name']           # 模型名称\n",
    "        model_path = model_config['path']           # 模型文件路径\n",
    "        port = model_config['port']                 # 服务端口\n",
    "        gpu_devices = model_config.get('gpu_devices', [0])  # 使用的GPU，默认使用GPU 0\n",
    "        \n",
    "        print(f\"\\n开始启动模型: {model_name}\")\n",
    "        print(f\"模型路径: {model_path}\")\n",
    "        print(f\"服务端口: {port}\")\n",
    "        print(f\"使用GPU: {gpu_devices}\")\n",
    "        \n",
    "        # 检查端口是否已被占用\n",
    "        if self.check_port_in_use(port):\n",
    "            print(f\"警告: 端口 {port} 已被占用，可能该模型已在运行\")\n",
    "            return None\n",
    "        \n",
    "        # 检查模型路径是否存在\n",
    "        if not Path(model_path).exists():\n",
    "            print(f\"错误: 模型路径不存在 - {model_path}\")\n",
    "            return None\n",
    "        \n",
    "        # 设置要使用的GPU\n",
    "        gpu_str = ','.join(map(str, gpu_devices))  # 将GPU列表转换为字符串，比如[0,1]变成\"0,1\"\n",
    "        tensor_parallel_size = len(gpu_devices)    # 并行度等于GPU数量\n",
    "        \n",
    "        # 构建启动命令\n",
    "        # 这个命令会启动vLLM的OpenAI兼容API服务器\n",
    "        # 就像告诉厨师：\"用这个菜谱，在这个窗口，用这些炉灶开始营业\"\n",
    "        cmd = [\n",
    "            sys.executable,                          # Python解释器路径\n",
    "            '-m', 'vllm.entrypoints.openai.api_server',  # vLLM的API服务器模块\n",
    "            '--model', model_path,                   # 指定模型路径\n",
    "            '--port', str(port),                     # 指定服务端口\n",
    "            '--tensor-parallel-size', str(tensor_parallel_size),  # 设置并行度（使用多少个GPU）\n",
    "            '--gpu-memory-utilization', str(model_config.get('gpu_memory_utilization', 0.85)),  # GPU内存使用率\n",
    "            '--max-model-len', str(model_config.get('max_model_len', 4096)),  # 最大序列长度\n",
    "            '--trust-remote-code',                   # 信任模型代码\n",
    "            '--disable-log-requests',                # 禁用请求日志（减少输出）\n",
    "        ]\n",
    "        \n",
    "        # 如果配置中指定了数据类型，添加到命令中\n",
    "        if model_config.get('dtype'):\n",
    "            cmd.extend(['--dtype', model_config['dtype']])\n",
    "        \n",
    "        # 设置环境变量\n",
    "        env = os.environ.copy()  # 复制当前环境变量\n",
    "        env['CUDA_VISIBLE_DEVICES'] = gpu_str  # 设置可见的GPU设备\n",
    "        \n",
    "        # 创建日志文件\n",
    "        log_file = f\"{model_name}_service.log\"\n",
    "        print(f\"日志文件: {log_file}\")\n",
    "        \n",
    "        try:\n",
    "            # 启动进程\n",
    "            # 就像正式开启厨房营业，所有的操作记录都会写入日志文件\n",
    "            with open(log_file, 'w') as f:\n",
    "                process = subprocess.Popen(\n",
    "                    cmd,                    # 要执行的命令\n",
    "                    env=env,               # 环境变量\n",
    "                    stdout=f,              # 标准输出重定向到日志文件\n",
    "                    stderr=subprocess.STDOUT,  # 错误输出也重定向到日志文件\n",
    "                    preexec_fn=os.setsid   # 创建新的进程组（方便后续管理）\n",
    "                )\n",
    "            \n",
    "            # 保存进程信息\n",
    "            self.processes[model_name] = {\n",
    "                'process': process,      # 进程对象\n",
    "                'config': model_config,  # 配置信息\n",
    "                'log_file': log_file,    # 日志文件路径\n",
    "                'pid': process.pid       # 进程ID\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ 模型 {model_name} 启动成功，进程ID: {process.pid}\")\n",
    "            return process\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ 启动模型 {model_name} 失败: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def wait_for_service_ready(self, port, timeout=300):\n",
    "        \"\"\"\n",
    "        等待服务启动完成\n",
    "        \n",
    "        参数说明：\n",
    "        port: 服务端口号\n",
    "        timeout: 超时时间（秒），默认5分钟\n",
    "        \n",
    "        返回值：\n",
    "        True: 服务启动成功\n",
    "        False: 服务启动失败或超时\n",
    "        \n",
    "        比喻：就像等待厨房准备完毕，可以开始接受订单\n",
    "        大模型启动需要时间加载到内存中，这个过程可能需要几分钟\n",
    "        \"\"\"\n",
    "        print(f\"等待端口 {port} 上的服务启动...\")\n",
    "        \n",
    "        # 尝试导入requests库，用于发送HTTP请求\n",
    "        try:\n",
    "            import requests\n",
    "        except ImportError:\n",
    "            print(\"需要安装requests库: pip install requests\")\n",
    "            return False\n",
    "        \n",
    "        start_time = time.time()  # 记录开始时间\n",
    "        \n",
    "        # 在超时时间内不断尝试连接服务\n",
    "        while time.time() - start_time < timeout:\n",
    "            try:\n",
    "                # 尝试访问服务的模型列表接口\n",
    "                # 这是vLLM提供的标准接口，如果能正常访问说明服务已就绪\n",
    "                response = requests.get(f\"http://localhost:{port}/v1/models\", timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"✓ 端口 {port} 上的服务已就绪\")\n",
    "                    return True\n",
    "            except:\n",
    "                # 如果连接失败，继续等待\n",
    "                pass\n",
    "            \n",
    "            # 每5秒检查一次\n",
    "            time.sleep(5)\n",
    "            print(\".\", end=\"\", flush=True)  # 显示等待进度\n",
    "        \n",
    "        print(f\"\\n✗ 端口 {port} 上的服务启动超时\")\n",
    "        return False\n",
    "    \n",
    "    def start_all_models(self, config_file=\"vllm_config.json\"):\n",
    "        \"\"\"\n",
    "        启动所有配置的模型\n",
    "        \n",
    "        参数说明：\n",
    "        config_file: 配置文件路径\n",
    "        \n",
    "        比喻：就像同时开启餐厅的所有厨房，每个厨房都有不同的菜谱和服务窗口\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"开始启动vLLM模型服务\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 检查配置文件是否存在\n",
    "        if not Path(config_file).exists():\n",
    "            print(f\"错误: 配置文件 {config_file} 不存在\")\n",
    "            print(\"请先创建配置文件\")\n",
    "            return False\n",
    "        \n",
    "        # 读取配置文件\n",
    "        try:\n",
    "            with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"错误: 无法读取配置文件 - {e}\")\n",
    "            return False\n",
    "        \n",
    "        # 检查GPU状态\n",
    "        self.check_gpu_status()\n",
    "        \n",
    "        # 获取要启动的模型列表\n",
    "        models = config.get('models', [])\n",
    "        if not models:\n",
    "            print(\"错误: 配置文件中没有找到模型配置\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\n准备启动 {len(models)} 个模型...\")\n",
    "        \n",
    "        # 逐个启动模型\n",
    "        for i, model_config in enumerate(models, 1):\n",
    "            model_name = model_config.get('name', f'model_{i}')\n",
    "            \n",
    "            # 检查模型是否已经在运行\n",
    "            if model_name in self.processes:\n",
    "                print(f\"\\n[{i}/{len(models)}] 模型 {model_name} 已在运行，跳过启动\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n[{i}/{len(models)}] 启动模型...\")\n",
    "            \n",
    "            # 启动单个模型\n",
    "            process = self.start_single_model(model_config)\n",
    "            \n",
    "            if process:\n",
    "                # 如果不是最后一个模型，等待一段时间再启动下一个\n",
    "                # 这样可以避免GPU资源冲突\n",
    "                if i < len(models):\n",
    "                    print(\"等待10秒后启动下一个模型...\")\n",
    "                    time.sleep(10)\n",
    "            else:\n",
    "                print(f\"模型启动失败，跳过\")\n",
    "        \n",
    "        # 等待所有服务启动完成\n",
    "        print(f\"\\n等待所有服务启动完成...\")\n",
    "        all_ready = True\n",
    "        \n",
    "        for model_name, info in self.processes.items():\n",
    "            port = info['config']['port']\n",
    "            if self.wait_for_service_ready(port):\n",
    "                print(f\"✓ {model_name} 服务就绪\")\n",
    "            else:\n",
    "                print(f\"✗ {model_name} 服务启动失败\")\n",
    "                all_ready = False\n",
    "        \n",
    "        # 保存服务信息到文件\n",
    "        self.save_service_info()\n",
    "        \n",
    "        if all_ready:\n",
    "            print(f\"\\n🎉 所有模型服务启动成功！\")\n",
    "            self.show_running_services()\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\n⚠️  部分模型服务启动失败，请检查日志\")\n",
    "            return False\n",
    "    \n",
    "    def stop_all_models(self):\n",
    "        \"\"\"\n",
    "        停止所有运行中的模型服务\n",
    "        \n",
    "        比喻：就像关闭餐厅的所有厨房，让所有厨师下班\n",
    "        \"\"\"\n",
    "        print(\"正在停止所有模型服务...\")\n",
    "        \n",
    "        if not self.processes:\n",
    "            print(\"没有运行中的模型服务\")\n",
    "            return\n",
    "        \n",
    "        # 逐个停止进程\n",
    "        for model_name, info in list(self.processes.items()):\n",
    "            print(f\"停止 {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                pid = info['pid']\n",
    "                \n",
    "                # 使用psutil来停止进程\n",
    "                if self.is_process_running(pid):\n",
    "                    process = psutil.Process(pid)\n",
    "                    # 停止进程及其子进程\n",
    "                    children = process.children(recursive=True)\n",
    "                    for child in children:\n",
    "                        try:\n",
    "                            child.terminate()\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    process.terminate()\n",
    "                    \n",
    "                    # 等待进程结束\n",
    "                    try:\n",
    "                        process.wait(timeout=30)\n",
    "                        print(f\"✓ {model_name} 已停止\")\n",
    "                    except psutil.TimeoutExpired:\n",
    "                        # 如果30秒内没有结束，强制杀死进程\n",
    "                        process.kill()\n",
    "                        print(f\"✓ {model_name} 已强制停止\")\n",
    "                else:\n",
    "                    print(f\"✓ {model_name} 进程已不存在\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ 停止 {model_name} 时出错: {e}\")\n",
    "            \n",
    "            # 从进程列表中移除\n",
    "            del self.processes[model_name]\n",
    "        \n",
    "        # 清空服务信息文件\n",
    "        self.save_service_info()\n",
    "        print(\"所有模型服务已停止\")\n",
    "    \n",
    "    def show_running_services(self):\n",
    "        \"\"\"\n",
    "        显示当前运行中的服务状态\n",
    "        \n",
    "        比喻：就像查看餐厅状态报告，看看哪些厨房在营业，\n",
    "        每个厨房的窗口号是多少，使用了哪些设备\n",
    "        \"\"\"\n",
    "        if not self.processes:\n",
    "            print(\"没有运行中的模型服务\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"运行中的模型服务:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for model_name, info in self.processes.items():\n",
    "            config = info['config']\n",
    "            pid = info['pid']\n",
    "            \n",
    "            # 检查进程是否还在运行\n",
    "            if self.is_process_running(pid):\n",
    "                status = \"🟢 运行中\"\n",
    "            else:\n",
    "                status = \"🔴 已停止\"\n",
    "            \n",
    "            print(f\"\\n模型名称: {model_name}\")\n",
    "            print(f\"  状态: {status}\")\n",
    "            print(f\"  进程ID: {pid}\")\n",
    "            print(f\"  服务端口: {config['port']}\")\n",
    "            print(f\"  使用GPU: {config.get('gpu_devices', [])}\")\n",
    "            print(f\"  API地址: http://localhost:{config['port']}\")\n",
    "            print(f\"  日志文件: {info['log_file']}\")\n",
    "    \n",
    "    def save_service_info(self):\n",
    "        \"\"\"\n",
    "        保存服务信息到文件\n",
    "        这个文件会被对比测试脚本读取，用于知道哪些服务在运行\n",
    "        \n",
    "        比喻：就像把餐厅的营业状态记录在册，\n",
    "        这样其他人（比如外卖平台）就知道哪些厨房在营业，可以接单\n",
    "        \"\"\"\n",
    "        service_info = {}\n",
    "        \n",
    "        for model_name, info in self.processes.items():\n",
    "            service_info[model_name] = {\n",
    "                'port': info['config']['port'],\n",
    "                'pid': info['pid'],\n",
    "                'gpu_devices': info['config'].get('gpu_devices', []),\n",
    "                'log_file': info['log_file']\n",
    "            }\n",
    "        \n",
    "        # 写入文件\n",
    "        with open(self.service_info_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(service_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数 - 程序的入口点\n",
    "    \n",
    "    这是程序开始执行的地方，就像餐厅的总经理，\n",
    "    根据用户的指令决定要执行什么操作\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    # 创建命令行参数解析器\n",
    "    # 这让用户可以通过命令行告诉程序要做什么\n",
    "    parser = argparse.ArgumentParser(description='简化版vLLM模型启动器')\n",
    "    parser.add_argument('action', \n",
    "                       choices=['start', 'stop', 'status'], \n",
    "                       help='要执行的操作: start(启动), stop(停止), status(查看状态)')\n",
    "    parser.add_argument('--config', \n",
    "                       type=str, \n",
    "                       default='vllm_config.json',\n",
    "                       help='配置文件路径 (默认: vllm_config.json)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 创建启动器实例\n",
    "    launcher = SimpleVLLMLauncher()\n",
    "    \n",
    "    # 根据用户指定的操作执行相应功能\n",
    "    if args.action == 'start':\n",
    "        # 启动所有模型\n",
    "        launcher.start_all_models(args.config)\n",
    "        \n",
    "    elif args.action == 'stop':\n",
    "        # 停止所有模型\n",
    "        launcher.stop_all_models()\n",
    "        \n",
    "    elif args.action == 'status':\n",
    "        # 显示服务状态\n",
    "        launcher.show_running_services()\n",
    "\n",
    "# 当直接运行这个脚本时，执行main函数\n",
    "# 这是Python的标准写法，确保只有直接运行脚本时才执行main函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2e17a",
   "metadata": {},
   "source": [
    "启动配置文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fe1e1",
   "metadata": {},
   "source": [
    "vllm_config.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc03acf",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"name\": \"original\",\n",
    "      \"path\": \"/root/autodl-tmp/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "      \"port\": 8000,\n",
    "      \"gpu_devices\": [0, 1],\n",
    "      \"gpu_memory_utilization\": 0.85,\n",
    "      \"max_model_len\": 4096,\n",
    "      \"dtype\": \"bfloat16\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"finetuned\",\n",
    "      \"path\": \"/root/autodl-tmp/best_complete_model_05261653_028\",\n",
    "      \"port\": 8001,\n",
    "      \"gpu_devices\": [2, 3],\n",
    "      \"gpu_memory_utilization\": 0.85,\n",
    "      \"max_model_len\": 4096,\n",
    "      \"dtype\": \"bfloat16\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced473fd",
   "metadata": {},
   "source": [
    "启动命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6101f",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 查看当前状态（现在应该能正确显示运行中的服务）\n",
    "python simple_vllm_launcher.py status\n",
    "\n",
    "# 如果需要重新启动\n",
    "python simple_vllm_launcher.py stop\n",
    "python simple_vllm_launcher.py start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e56250",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528014825107.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27863e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528015039068.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd540e58",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528015056624.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc1490",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528020348540.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67ca68",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528015202591.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904a578",
   "metadata": {},
   "source": [
    "## 7.2 对比输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67af9ed",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "模型对比测试脚本\n",
    "修复了API端点问题，支持多种API格式，并自动获取正确的模型名称\n",
    "\n",
    "这个脚本的作用：\n",
    "就像组织一场\"AI模型考试\"，让不同的模型回答相同的问题，\n",
    "然后对比它们的回答质量，帮助我们了解哪个模型表现更好。\n",
    "\n",
    "比喻：\n",
    "- 这个脚本 = 考试监考老师\n",
    "- 不同的模型 = 不同的学生\n",
    "- 测试数据 = 考试题目\n",
    "- 模型回答 = 学生答案\n",
    "- 对比结果 = 成绩单\n",
    "\n",
    "主要功能：\n",
    "1. 自动发现正在运行的模型服务\n",
    "2. 向所有模型发送相同的问题\n",
    "3. 收集和对比不同模型的回答\n",
    "4. 生成详细的对比报告\n",
    "\"\"\"\n",
    "\n",
    "import json         # 用于处理JSON格式的数据（配置文件、测试数据等）\n",
    "import random       # 用于随机选择测试样本（从大量数据中随机抽取一部分进行测试）\n",
    "import asyncio      # 用于异步编程，可以同时向多个模型发送请求（提高效率）\n",
    "import aiohttp      # 用于发送异步HTTP请求（与模型API通信）\n",
    "import time         # 用于时间相关操作（记录时间戳、控制请求频率）\n",
    "import argparse     # 用于处理命令行参数（让用户可以自定义测试参数）\n",
    "from pathlib import Path  # 用于处理文件路径（跨平台兼容）\n",
    "from typing import List, Dict, Any  # 用于类型提示，让代码更清晰易懂\n",
    "\n",
    "class ModelComparisonTester:\n",
    "    \"\"\"\n",
    "    模型对比测试器类\n",
    "    \n",
    "    这个类就像一个\"智能考试系统\"，它的主要职责是：\n",
    "    1. 管理多个模型服务的连接信息（知道有哪些\"考生\"参加考试）\n",
    "    2. 向模型发送问题并获取回答（组织考试过程）\n",
    "    3. 对比不同模型的回答效果（评分和排名）\n",
    "    4. 支持多种API格式（适配不同\"考生\"的答题方式）\n",
    "    5. 自动获取正确的模型名称（确认\"考生\"身份）\n",
    "    \n",
    "    比喻：就像一个全自动的考试系统，能够：\n",
    "    - 自动识别参加考试的学生\n",
    "    - 同时给所有学生发放相同的试卷\n",
    "    - 收集所有学生的答案\n",
    "    - 对比分析不同学生的表现\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_config_file=\"running_services.json\"):\n",
    "        \"\"\"\n",
    "        初始化对比测试器\n",
    "        \n",
    "        参数说明：\n",
    "        service_config_file: 服务配置文件路径，包含运行中的模型服务信息\n",
    "        \n",
    "        比喻：就像考试系统启动时，先读取\"考生名单\"，\n",
    "        了解有哪些学生要参加考试，他们的\"座位号\"（端口）是多少\n",
    "        \"\"\"\n",
    "        self.services = {}  # 存储模型服务的连接信息，格式：{模型名: 连接信息}\n",
    "        self.load_service_config(service_config_file)  # 加载服务配置\n",
    "    \n",
    "    def load_service_config(self, config_file):\n",
    "        \"\"\"\n",
    "        加载服务配置信息\n",
    "        从文件中读取当前运行的模型服务信息\n",
    "        \n",
    "        参数说明：\n",
    "        config_file: 配置文件路径\n",
    "        \n",
    "        比喻：就像读取\"考生名单\"，了解有哪些学生要参加考试，\n",
    "        每个学生坐在哪个位置（端口号），如何联系他们\n",
    "        \"\"\"\n",
    "        # 检查配置文件是否存在\n",
    "        if not Path(config_file).exists():\n",
    "            print(f\"错误: 服务配置文件 {config_file} 不存在\")\n",
    "            print(\"请先使用启动器启动模型服务\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # 读取配置文件\n",
    "            with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                service_info = json.load(f)\n",
    "            \n",
    "            # 为每个服务创建连接信息\n",
    "            # 就像为每个考生准备不同的联系方式\n",
    "            for model_name, info in service_info.items():\n",
    "                port = info['port']  # 获取服务端口号\n",
    "                self.services[model_name] = {\n",
    "                    'name': model_name,  # 模型名称（考生姓名）\n",
    "                    'base_url': f\"http://localhost:{port}\",  # 基础URL\n",
    "                    'chat_url': f\"http://localhost:{port}/v1/chat/completions\",  # OpenAI Chat API格式\n",
    "                    'completions_url': f\"http://localhost:{port}/v1/completions\",  # OpenAI Completions API格式\n",
    "                    'generate_url': f\"http://localhost:{port}/generate\",  # vLLM原生API格式\n",
    "                    'models_url': f\"http://localhost:{port}/v1/models\",  # 获取模型列表的API\n",
    "                    'port': port,  # 端口号\n",
    "                    'actual_model_id': None  # 将存储从API获取的真实模型ID\n",
    "                }\n",
    "            \n",
    "            print(f\"已加载 {len(self.services)} 个模型服务:\")\n",
    "            for name, info in self.services.items():\n",
    "                print(f\"  - {name}: {info['base_url']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"加载服务配置失败: {e}\")\n",
    "    \n",
    "    async def get_actual_model_id(self, session, service_info):\n",
    "        \"\"\"\n",
    "        从API获取实际的模型ID\n",
    "        \n",
    "        参数说明：\n",
    "        session: HTTP会话对象（用于发送网络请求）\n",
    "        service_info: 模型服务信息\n",
    "        \n",
    "        返回值：\n",
    "        实际的模型ID字符串\n",
    "        \n",
    "        比喻：就像确认考生的真实姓名和学号，\n",
    "        有时候报名时用的是昵称，但考试时需要用真实姓名\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 向模型服务发送请求，获取模型列表\n",
    "            async with session.get(service_info['models_url']) as response:\n",
    "                if response.status == 200:  # 请求成功\n",
    "                    result = await response.json()\n",
    "                    # 从返回的数据中提取模型ID\n",
    "                    if 'data' in result and len(result['data']) > 0:\n",
    "                        model_id = result['data'][0]['id']  # 获取第一个模型的ID\n",
    "                        print(f\"✓ {service_info['name']} 的实际模型ID: {model_id}\")\n",
    "                        return model_id\n",
    "                    else:\n",
    "                        print(f\"✗ {service_info['name']} 没有返回模型信息\")\n",
    "                        return None\n",
    "                else:\n",
    "                    print(f\"✗ {service_info['name']} 获取模型列表失败: HTTP {response.status}\")\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {service_info['name']} 获取模型ID时出错: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def test_api_endpoints(self, session, service_info):\n",
    "        \"\"\"\n",
    "        测试不同的API端点，找到可用的格式\n",
    "        \n",
    "        参数说明：\n",
    "        session: HTTP会话对象\n",
    "        service_info: 模型服务信息\n",
    "        \n",
    "        返回值：\n",
    "        可用的API端点信息\n",
    "        \n",
    "        比喻：就像测试不同的\"沟通方式\"，\n",
    "        有些学生喜欢面对面交流，有些喜欢书面交流，\n",
    "        我们需要找到每个学生最适合的沟通方式\n",
    "        \"\"\"\n",
    "        # 首先获取实际的模型ID\n",
    "        actual_model_id = await self.get_actual_model_id(session, service_info)\n",
    "        if not actual_model_id:\n",
    "            print(f\"✗ {service_info['name']} 无法获取模型ID，跳过测试\")\n",
    "            return None\n",
    "        \n",
    "        # 更新服务信息中的实际模型ID\n",
    "        service_info['actual_model_id'] = actual_model_id\n",
    "        \n",
    "        # 定义要测试的不同API端点\n",
    "        # 就像准备不同格式的\"试卷\"，看看学生更适合哪种格式\n",
    "        endpoints_to_test = [\n",
    "            {\n",
    "                'name': 'chat_completions',  # 对话格式API\n",
    "                'url': service_info['chat_url'],\n",
    "                'test_data': {\n",
    "                    \"model\": actual_model_id,  # 使用实际的模型ID\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],  # 对话格式的测试消息\n",
    "                    \"max_tokens\": 10  # 限制回答长度（这只是测试）\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'completions',  # 补全格式API\n",
    "                'url': service_info['completions_url'],\n",
    "                'test_data': {\n",
    "                    \"model\": actual_model_id,  # 使用实际的模型ID\n",
    "                    \"prompt\": \"Hello\",  # 简单的提示词\n",
    "                    \"max_tokens\": 10  # 限制回答长度\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'generate',  # vLLM原生格式API\n",
    "                'url': service_info['generate_url'],\n",
    "                'test_data': {\n",
    "                    \"prompt\": \"Hello\",  # 提示词\n",
    "                    \"max_tokens\": 10  # 限制回答长度\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 逐个测试每种API格式\n",
    "        for endpoint in endpoints_to_test:\n",
    "            try:\n",
    "                # 发送测试请求\n",
    "                async with session.post(endpoint['url'], json=endpoint['test_data']) as response:\n",
    "                    if response.status == 200:  # 请求成功\n",
    "                        print(f\"✓ {service_info['name']} 支持 {endpoint['name']} API\")\n",
    "                        return endpoint  # 找到可用的API格式，直接返回\n",
    "                    else:\n",
    "                        # 请求失败，记录错误信息\n",
    "                        error_text = await response.text()\n",
    "                        print(f\"✗ {service_info['name']} {endpoint['name']} API 返回 {response.status}: {error_text[:100]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ {service_info['name']} {endpoint['name']} API 测试失败: {e}\")\n",
    "        \n",
    "        return None  # 没有找到可用的API格式\n",
    "    \n",
    "    def format_prompt(self, instruction, input_text=\"\"):\n",
    "        \"\"\"\n",
    "        格式化提示词\n",
    "        将问题转换为模型能理解的格式\n",
    "        \n",
    "        参数说明：\n",
    "        instruction: 指令或问题（比如\"请翻译以下文本\"）\n",
    "        input_text: 额外的输入文本（比如要翻译的具体内容）\n",
    "        \n",
    "        返回值：\n",
    "        格式化后的提示词字符串\n",
    "        \n",
    "        比喻：就像把考试题目整理成标准格式，\n",
    "        确保所有学生都能清楚地理解题目要求\n",
    "        \"\"\"\n",
    "        if input_text and input_text.strip():\n",
    "            # 如果有额外输入，将指令和输入组合\n",
    "            # 格式：<|im_start|>user\\n指令\\n\\n输入内容<|im_end|>\\n<|im_start|>assistant\\n\n",
    "            prompt = f\"<|im_start|>user\\n{instruction}\\n\\n{input_text}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        else:\n",
    "            # 如果只有指令，使用简化格式\n",
    "            # 格式：<|im_start|>user\\n指令<|im_end|>\\n<|im_start|>assistant\\n\n",
    "            prompt = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    async def ask_model_with_endpoint(self, session, service_info, endpoint_info, prompt):\n",
    "        \"\"\"\n",
    "        使用指定的API端点向模型发送问题\n",
    "        \n",
    "        参数说明：\n",
    "        session: HTTP会话对象\n",
    "        service_info: 模型服务信息\n",
    "        endpoint_info: API端点信息（使用哪种格式与模型通信）\n",
    "        prompt: 格式化后的提示词\n",
    "        \n",
    "        返回值：\n",
    "        包含回答结果的字典\n",
    "        \n",
    "        比喻：就像用特定的方式向学生提问，\n",
    "        有些学生适合口头提问，有些适合书面提问\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 根据不同的API格式构建请求数据\n",
    "            # 就像根据不同学生的特点，用不同的方式提问\n",
    "            if endpoint_info['name'] == 'chat_completions':\n",
    "                # 对话格式：适合聊天式的交互\n",
    "                request_data = {\n",
    "                    \"model\": service_info['actual_model_id'],  # 使用实际的模型ID\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],  # 用户消息\n",
    "                    \"max_tokens\": 2048,  # 最大回答长度\n",
    "                    \"temperature\": 0.7,  # 创造性参数（0-1，越高越有创意）\n",
    "                    \"top_p\": 0.9,  # 多样性参数\n",
    "                    \"stop\": [\"<|im_end|>\", \"</s>\"]  # 停止标记\n",
    "                }\n",
    "            elif endpoint_info['name'] == 'completions':\n",
    "                # 补全格式：适合文本续写\n",
    "                request_data = {\n",
    "                    \"model\": service_info['actual_model_id'],  # 使用实际的模型ID\n",
    "                    \"prompt\": prompt,  # 提示词\n",
    "                    \"max_tokens\": 2048,  # 最大回答长度\n",
    "                    \"temperature\": 0.7,  # 创造性参数\n",
    "                    \"top_p\": 0.9,  # 多样性参数\n",
    "                    \"stop\": [\"<|im_end|>\", \"</s>\"]  # 停止标记\n",
    "                }\n",
    "            else:  # generate格式\n",
    "                # vLLM原生格式\n",
    "                request_data = {\n",
    "                    \"prompt\": prompt,  # 提示词\n",
    "                    \"max_tokens\": 2048,  # 最大回答长度\n",
    "                    \"temperature\": 0.7,  # 创造性参数\n",
    "                    \"top_p\": 0.9,  # 多样性参数\n",
    "                    \"stop\": [\"<|im_end|>\", \"</s>\"]  # 停止标记\n",
    "                }\n",
    "            \n",
    "            # 发送请求给模型\n",
    "            async with session.post(endpoint_info['url'], json=request_data) as response:\n",
    "                if response.status == 200:  # 请求成功\n",
    "                    result = await response.json()\n",
    "                    \n",
    "                    # 根据不同的API格式提取生成的文本\n",
    "                    # 就像从不同格式的答卷中提取学生的答案\n",
    "                    if endpoint_info['name'] == 'chat_completions':\n",
    "                        generated_text = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                    elif endpoint_info['name'] == 'completions':\n",
    "                        generated_text = result[\"choices\"][0][\"text\"].strip()\n",
    "                    else:  # generate\n",
    "                        generated_text = result[\"text\"][0].strip()\n",
    "                    \n",
    "                    # 返回成功结果\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"model_name\": service_info['name'],\n",
    "                        \"response\": generated_text,\n",
    "                        \"api_type\": endpoint_info['name'],\n",
    "                        \"error\": None\n",
    "                    }\n",
    "                else:\n",
    "                    # 请求失败\n",
    "                    error_text = await response.text()\n",
    "                    return {\n",
    "                        \"success\": False,\n",
    "                        \"model_name\": service_info['name'],\n",
    "                        \"response\": \"\",\n",
    "                        \"api_type\": endpoint_info['name'],\n",
    "                        \"error\": f\"HTTP {response.status}: {error_text}\"\n",
    "                    }\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # 发生异常\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"model_name\": service_info['name'],\n",
    "                \"response\": \"\",\n",
    "                \"api_type\": endpoint_info['name'] if endpoint_info else \"unknown\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    async def ask_model(self, session, service_info, prompt, endpoint_info=None):\n",
    "        \"\"\"\n",
    "        向单个模型发送问题并获取回答\n",
    "        这是一个异步函数，可以同时向多个模型发送请求\n",
    "        \n",
    "        参数说明：\n",
    "        session: HTTP会话对象\n",
    "        service_info: 模型服务信息\n",
    "        prompt: 格式化后的提示词\n",
    "        endpoint_info: 可选的API端点信息\n",
    "        \n",
    "        返回值：\n",
    "        包含回答结果的字典\n",
    "        \n",
    "        比喻：就像向一个学生提问，\n",
    "        如果不知道这个学生喜欢什么样的提问方式，\n",
    "        就先测试一下，找到最适合的方式再正式提问\n",
    "        \"\"\"\n",
    "        # 如果没有提供端点信息，先测试可用的端点\n",
    "        if endpoint_info is None:\n",
    "            endpoint_info = await self.test_api_endpoints(session, service_info)\n",
    "            if endpoint_info is None:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"model_name\": service_info['name'],\n",
    "                    \"response\": \"\",\n",
    "                    \"api_type\": \"none\",\n",
    "                    \"error\": \"没有找到可用的API端点\"\n",
    "                }\n",
    "        \n",
    "        # 使用找到的端点发送问题\n",
    "        return await self.ask_model_with_endpoint(session, service_info, endpoint_info, prompt)\n",
    "    \n",
    "    def detect_file_format(self, file_path):\n",
    "        \"\"\"\n",
    "        自动检测文件格式\n",
    "        \n",
    "        参数说明：\n",
    "        file_path: 文件路径\n",
    "        \n",
    "        返回值：\n",
    "        'json' 或 'jsonl'\n",
    "        \n",
    "        比喻：就像自动识别试卷是什么格式，\n",
    "        是标准的JSON格式，还是每行一个题目的JSONL格式\n",
    "        \"\"\"\n",
    "        file_path = str(file_path)\n",
    "        # 根据文件扩展名判断\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            return 'jsonl'\n",
    "        elif file_path.endswith('.json'):\n",
    "            return 'json'\n",
    "        else:\n",
    "            # 如果扩展名不明确，尝试读取第一行来判断\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    first_line = f.readline().strip()\n",
    "                    if first_line.startswith('{') and first_line.endswith('}'):\n",
    "                        return 'jsonl'  # 每行一个JSON对象\n",
    "                    elif first_line.startswith('['):\n",
    "                        return 'json'   # 标准JSON数组\n",
    "            except:\n",
    "                pass\n",
    "            return 'json'  # 默认假设是json格式\n",
    "    \n",
    "    def load_test_data(self, file_path):\n",
    "        \"\"\"\n",
    "        加载测试数据\n",
    "        从文件中读取要测试的问题\n",
    "        \n",
    "        参数说明：\n",
    "        file_path: 测试数据文件路径\n",
    "        \n",
    "        返回值：\n",
    "        测试数据列表\n",
    "        \n",
    "        比喻：就像从题库中加载考试题目，\n",
    "        每道题目包含问题、输入数据和标准答案\n",
    "        \"\"\"\n",
    "        # 去除路径中的多余空格\n",
    "        file_path = str(file_path).strip()\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if not Path(file_path).exists():\n",
    "            print(f\"错误: 测试数据文件 {file_path} 不存在\")\n",
    "            print(f\"当前工作目录: {Path.cwd()}\")\n",
    "            \n",
    "            # 尝试查找类似的文件，帮助用户发现可能的文件\n",
    "            parent_dir = Path(file_path).parent\n",
    "            if parent_dir.exists():\n",
    "                print(f\"在目录 {parent_dir} 中找到的文件:\")\n",
    "                for f in parent_dir.glob(\"*.json*\"):\n",
    "                    print(f\"  - {f}\")\n",
    "            \n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 自动检测文件格式\n",
    "            file_format = self.detect_file_format(file_path)\n",
    "            print(f\"检测到文件格式: {file_format}\")\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                if file_format == 'json':\n",
    "                    # JSON格式文件：整个文件是一个JSON数组\n",
    "                    data = json.load(f)\n",
    "                else:\n",
    "                    # JSONL格式文件：每行一个JSON对象\n",
    "                    data = []\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if line:  # 跳过空行\n",
    "                            try:\n",
    "                                data.append(json.loads(line))\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"警告: 第{line_num}行JSON格式错误: {e}\")\n",
    "                                continue\n",
    "            \n",
    "            print(f\"成功加载 {len(data)} 条测试数据\")\n",
    "            \n",
    "            # 显示第一条数据的结构，帮助用户了解数据格式\n",
    "            if data:\n",
    "                first_item = data[0]\n",
    "                print(f\"数据结构预览:\")\n",
    "                for key in first_item.keys():\n",
    "                    # 如果值太长，只显示前100个字符\n",
    "                    value = str(first_item[key])[:100] + \"...\" if len(str(first_item[key])) > 100 else str(first_item[key])\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"加载测试数据失败: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def select_random_samples(self, data, sample_size):\n",
    "        \"\"\"\n",
    "        随机选择测试样本\n",
    "        \n",
    "        参数说明：\n",
    "        data: 完整的测试数据列表\n",
    "        sample_size: 要选择的样本数量\n",
    "        \n",
    "        返回值：\n",
    "        随机选择的样本列表\n",
    "        \n",
    "        比喻：就像从题库中随机抽取一部分题目进行考试，\n",
    "        这样可以节省时间，同时保证测试的代表性\n",
    "        \"\"\"\n",
    "        if sample_size <= 0 or sample_size >= len(data):\n",
    "            print(f\"使用全部 {len(data)} 条数据进行测试\")\n",
    "            return data\n",
    "        \n",
    "        # 设置随机种子，确保结果可重现\n",
    "        # 这样每次运行程序，抽取的题目都是一样的，便于对比\n",
    "        random.seed(42)\n",
    "        selected = random.sample(data, sample_size)\n",
    "        print(f\"随机选择了 {len(selected)} 条数据进行测试\")\n",
    "        return selected\n",
    "    \n",
    "    async def run_comparison_test(self, test_data, sample_size=10):\n",
    "        \"\"\"\n",
    "        运行对比测试\n",
    "        这是主要的测试函数，会向所有模型发送问题并收集回答\n",
    "        \n",
    "        参数说明：\n",
    "        test_data: 测试数据列表\n",
    "        sample_size: 要测试的样本数量\n",
    "        \n",
    "        返回值：\n",
    "        测试结果列表\n",
    "        \n",
    "        比喻：就像组织一场正式考试，\n",
    "        给所有学生发放相同的试卷，收集他们的答案，\n",
    "        然后对比分析每个学生的表现\n",
    "        \"\"\"\n",
    "        # 检查是否有可用的模型服务\n",
    "        if len(self.services) < 1:\n",
    "            print(\"错误: 没有可用的模型服务\")\n",
    "            return []\n",
    "        \n",
    "        if len(self.services) < 2:\n",
    "            print(\"警告: 只有1个模型服务，无法进行对比\")\n",
    "        \n",
    "        # 选择测试样本\n",
    "        samples = self.select_random_samples(test_data, sample_size)\n",
    "        if not samples:\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\n开始对比测试\")\n",
    "        print(f\"参与测试的模型: {list(self.services.keys())}\")\n",
    "        print(f\"测试样本数量: {len(samples)}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 首先测试所有服务的API端点\n",
    "        # 就像考试前先确认每个学生的答题方式\n",
    "        print(\"正在测试API端点...\")\n",
    "        service_endpoints = {}  # 存储每个服务可用的API端点\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for service_name, service_info in self.services.items():\n",
    "                endpoint_info = await self.test_api_endpoints(session, service_info)\n",
    "                if endpoint_info:\n",
    "                    service_endpoints[service_name] = endpoint_info\n",
    "                    print(f\"✓ {service_name} 将使用 {endpoint_info['name']} API\")\n",
    "                else:\n",
    "                    print(f\"✗ {service_name} 没有可用的API端点\")\n",
    "        \n",
    "        if not service_endpoints:\n",
    "            print(\"错误: 没有找到任何可用的API端点\")\n",
    "            return []\n",
    "        \n",
    "        results = []  # 存储所有测试结果\n",
    "        \n",
    "        # 创建HTTP会话，用于发送请求\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # 逐个处理测试样本\n",
    "            for i, sample in enumerate(samples, 1):\n",
    "                print(f\"\\n[{i}/{len(samples)}] 测试样本 {i}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                # 提取问题信息\n",
    "                # 每个样本通常包含：指令、输入、期望输出\n",
    "                instruction = sample.get(\"instruction\", \"\")  # 指令（比如\"请翻译以下文本\"）\n",
    "                input_text = sample.get(\"input\", \"\")         # 输入（比如要翻译的文本）\n",
    "                reference_answer = sample.get(\"output\", \"\") # 参考答案\n",
    "                \n",
    "                # 显示问题信息\n",
    "                print(f\"问题: {instruction}\")\n",
    "                if input_text:\n",
    "                    # 如果输入文本太长，只显示前200个字符\n",
    "                    display_input = input_text[:200] + \"...\" if len(input_text) > 200 else input_text\n",
    "                    print(f\"输入: {display_input}\")\n",
    "                \n",
    "                # 格式化提示词\n",
    "                prompt = self.format_prompt(instruction, input_text)\n",
    "                \n",
    "                # 创建任务列表，同时向所有可用的模型发送请求\n",
    "                # 这是异步编程的核心：同时进行多个操作，提高效率\n",
    "                tasks = []\n",
    "                for service_name, service_info in self.services.items():\n",
    "                    if service_name in service_endpoints:\n",
    "                        endpoint_info = service_endpoints[service_name]\n",
    "                        # 创建异步任务\n",
    "                        task = self.ask_model_with_endpoint(session, service_info, endpoint_info, prompt)\n",
    "                        tasks.append(task)\n",
    "                \n",
    "                # 等待所有模型回答完成\n",
    "                # 就像等待所有学生完成答题\n",
    "                print(\"正在获取模型回答...\")\n",
    "                model_responses = await asyncio.gather(*tasks)\n",
    "                \n",
    "                # 显示各个模型的回答\n",
    "                response_dict = {}\n",
    "                for response in model_responses:\n",
    "                    model_name = response['model_name']\n",
    "                    response_dict[model_name] = response\n",
    "                    \n",
    "                    print(f\"\\n{model_name} 的回答 ({response.get('api_type', 'unknown')} API):\")\n",
    "                    print(\"-\" * 40)\n",
    "                    if response['success']:\n",
    "                        # 如果回答太长，只显示前500个字符\n",
    "                        display_response = response['response'][:500] + \"...\" if len(response['response']) > 500 else response['response']\n",
    "                        print(display_response)\n",
    "                    else:\n",
    "                        print(f\"❌ 生成失败: {response['error']}\")\n",
    "                \n",
    "                # 显示参考答案\n",
    "                print(f\"\\n📚 参考答案:\")\n",
    "                print(\"-\" * 40)\n",
    "                display_reference = reference_answer[:500] + \"...\" if len(reference_answer) > 500 else reference_answer\n",
    "                print(display_reference)\n",
    "                \n",
    "                # 保存这个样本的测试结果\n",
    "                result = {\n",
    "                    \"sample_id\": i,                    # 样本编号\n",
    "                    \"instruction\": instruction,        # 指令\n",
    "                    \"input\": input_text,              # 输入\n",
    "                    \"reference_answer\": reference_answer,  # 参考答案\n",
    "                    \"model_responses\": response_dict,  # 所有模型的回答\n",
    "                    \"timestamp\": time.time()          # 时间戳\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # 在处理下一个样本前稍作等待，避免请求过于频繁\n",
    "                # 就像考试中给学生一点休息时间\n",
    "                if i < len(samples):\n",
    "                    await asyncio.sleep(1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results, output_file):\n",
    "        \"\"\"\n",
    "        保存测试结果到文件\n",
    "        \n",
    "        参数说明：\n",
    "        results: 测试结果列表\n",
    "        output_file: 输出文件路径\n",
    "        \n",
    "        比喻：就像把考试成绩单保存到文件中，\n",
    "        方便后续查看和分析\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                # 使用缩进格式保存，便于阅读\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\n✅ 测试结果已保存到: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存结果失败: {e}\")\n",
    "    \n",
    "    def print_summary(self, results):\n",
    "        \"\"\"\n",
    "        打印测试结果摘要\n",
    "        \n",
    "        参数说明：\n",
    "        results: 测试结果列表\n",
    "        \n",
    "        比喻：就像打印考试成绩汇总表，\n",
    "        显示每个学生的总体表现\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(f\"测试结果摘要\")\n",
    "        print(f\"=\" * 80)\n",
    "        print(f\"总测试样本数: {len(results)}\")\n",
    "        \n",
    "        # 统计每个模型的成功率\n",
    "        model_names = list(self.services.keys())\n",
    "        for model_name in model_names:\n",
    "            successful_count = 0  # 成功回答的题目数\n",
    "            total_count = len(results)  # 总题目数\n",
    "            \n",
    "            # 遍历所有测试结果，统计成功次数\n",
    "            for result in results:\n",
    "                model_response = result['model_responses'].get(model_name, {})\n",
    "                if model_response.get('success', False):\n",
    "                    successful_count += 1\n",
    "            \n",
    "            # 计算成功率\n",
    "            success_rate = (successful_count / total_count) * 100 if total_count > 0 else 0\n",
    "            print(f\"{model_name}: {successful_count}/{total_count} 成功 ({success_rate:.1f}%)\")\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    主函数 - 程序的入口点\n",
    "    \n",
    "    比喻：就像考试系统的总控制台，\n",
    "    负责接收用户的指令，协调整个考试流程\n",
    "    \"\"\"\n",
    "    # 创建命令行参数解析器\n",
    "    # 让用户可以通过命令行自定义测试参数\n",
    "    parser = argparse.ArgumentParser(description='模型对比测试工具')\n",
    "    parser.add_argument('--data_file', \n",
    "                       type=str, \n",
    "                       default='/root/autodl-tmp/data/sft/deepspeek_sft_dataset_1_1k.jsonl',\n",
    "                       help='测试数据文件路径（题库文件）')\n",
    "    parser.add_argument('--sample_size', \n",
    "                       type=int, \n",
    "                       default=10,\n",
    "                       help='随机选择的测试样本数量（抽取多少道题）')\n",
    "    parser.add_argument('--output_file', \n",
    "                       type=str, \n",
    "                       default='comparison_results.json',\n",
    "                       help='结果输出文件路径（成绩单保存位置）')\n",
    "    parser.add_argument('--service_config', \n",
    "                       type=str,\n",
    "                       default='running_services.json',\n",
    "                       help='服务配置文件路径（考生名单文件）')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"🚀 启动模型对比测试\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 创建测试器实例\n",
    "    tester = ModelComparisonTester(args.service_config)\n",
    "    \n",
    "    # 检查是否有可用的模型服务\n",
    "    if not tester.services:\n",
    "        print(\"❌ 没有找到可用的模型服务\")\n",
    "        print(\"请先使用启动器启动模型服务\")\n",
    "        return\n",
    "    \n",
    "    # 加载测试数据\n",
    "    print(f\"📁 加载测试数据: {args.data_file}\")\n",
    "    test_data = tester.load_test_data(args.data_file)\n",
    "    if not test_data:\n",
    "        return\n",
    "    \n",
    "    # 运行对比测试\n",
    "    results = await tester.run_comparison_test(test_data, args.sample_size)\n",
    "    \n",
    "    if results:\n",
    "        # 保存结果\n",
    "        tester.save_results(results, args.output_file)\n",
    "        \n",
    "        # 显示摘要\n",
    "        tester.print_summary(results)\n",
    "        \n",
    "        print(f\"\\n🎉 测试完成！\")\n",
    "        print(f\"详细结果请查看: {args.output_file}\")\n",
    "    else:\n",
    "        print(\"❌ 测试失败\")\n",
    "\n",
    "# 当直接运行这个脚本时，执行main函数\n",
    "# 这是Python的标准写法，确保只有直接运行脚本时才执行main函数\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c9374",
   "metadata": {},
   "source": [
    "python model_comparison_test.py --sample_size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace561d4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528014955951.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d84664",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528020730829.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f78c2",
   "metadata": {},
   "source": [
    "```json\n",
    "\"instruction\": \"以下是一个关于金融领域的问题，请提供详细的分析和见解。\",\n",
    "    \"input\": \"根据公开信息，Reinsurance Group of America (RGA) 2022年和2023年的每股收益Zacks共识预期分别为9.76美元和15.25美元，同比增长763.7%和56.3%。过去30天内，2022年和2023年的预期分别上调了10.6%和3.2%。该公司目前Zacks评级为3（持有），年初至今股价上涨14.6%，而行业整体下跌8.7%。其业务增长主要来自美国、拉丁美洲、加拿大、欧洲中东非洲(EMEA)和亚太地区的保费增长。EMEA业务受益于新业务活动增加、年金业务投资增加以及封闭长寿业务新业务量增长；亚太业务则得益于有利的理赔经验、财务解决方案再保险持续增长、更高的投资收入和相关收益；加拿大业务则因现有条约下业务量增加、可变投资收入提高和投资资产基础扩大而持续获益。截至2022年第一季度末，RGA拥有约10亿美元的过剩资本，维持着8.5亿美元的银团循环信贷额度和总计9.29亿美元的承诺信用证额度。2015-2022年间，该公司股息以10.4%的复合年增长率增长，2021年第一季度通过股息和股票回购向股东返还7400万美元。基于这些信息，请分析：1) RGA未来两年业绩增长的主要驱动因素有哪些？2) 其强劲的资本状况将如何支持业务发展战略？3) 与同行业其他公司（如United Fire Group、RLI Corp.和HCI Group）相比，RGA的投资价值体现在哪些方面？\",\n",
    "    \"reference_answer\": \"<think>\\n1. 问题背景分析：\\nRGA是一家再保险公司，业务遍布全球主要区域。从提供的财务数据来看，公司展现出强劲的增长势头，2022-2023年EPS预期呈现爆发式增长（763.7%和56.3%），且预期持续上调。资本状况稳健（10亿美元过剩资本+17.79亿美元信贷额度），股东回报政策积极（10.4%股息CAGR+股票回购）。\\n\\n2. 数据解读：\\n- 增长质量：虽然2022年763.7%的增长基数效应明显，但2023年仍维持56.3%的高增长，且两个年度预期均被上调（10.6%/3.2%），说明增长具有持续性\\n- 超额收益：YTD股价+14.6% vs 行业-8.7%，显示市场已开始price in其增长潜力\\n- 资本充足率：过剩资本+信贷额度合计达27.79亿美元，相当于2022年预期EPS的28.5倍\\n\\n3. 业绩驱动因素分析：\\n(1) 区域业务协同：\\n- EMEA：新业务+年金投资+长寿业务构成增长三角\\n- 亚太：理赔改善+财务解决方案+投资收入三驾马车\\n- 加拿大：条约业务规模效应+投资组合优化\\n(2) 资本运作优势：\\n- 投资资产基础扩大带来规模效应\\n- 可变投资收入提升盈利弹性\\n\\n4. 资本战略价值：\\n- 业务拓展：支持新条约承接和现有业务扩张\\n- 风险管理：缓冲长寿风险等特殊风险暴露\\n- 股东回报：维持股息增长和回购的财务灵活性\\n\\n5. 同业比较优势：\\n- 增长维度：相比United Fire等区域性保险公司，RGA的全球化布局提供更稳定的增长来源\\n- 盈利质量：封闭长寿业务等特殊险种带来差异化竞争优势\\n- 资本效率：10.4%的股息CAGR显著高于行业平均水平\\n\\n6. 潜在风险考量：\\n- 长寿业务可能面临人口结构变化风险\\n- 投资收入受市场波动影响较大\\n- 高增长预期可能带来估值压力\\n</think>\\n\\n专业分析报告：\\n\\n1. 未来两年业绩增长核心驱动因素：\\n从区域维度看，RGA将形成\\\"三极增长引擎\\\"：①EMEA地区通过长寿再保险等特色业务实现差异化增长；②亚太区依托财务解决方案再保险构建技术壁垒；③加拿大市场发挥规模效应提升资本效率。从业务结构看，封闭型长寿业务和年金再保险将形成\\\"双轮驱动\\\"，其特有的长尾特性可提供稳定的现金流。特别值得注意的是，可变投资收入占比提升将增强盈利弹性，在利率上行周期中可能产生超额收益。\\n\\n2. 资本状况的战略价值体现：\\n公司当前27.79亿美元的可动用资本将重点支撑三方面发展：首先，作为新业务承保的资本缓冲，特别是在长寿再保险等资本密集型领域；其次，为并购活动提供资金储备，尤其是在EMEA和亚太等高增长区域；最后，维持\\\"股息增长+股票回购\\\"的双轨股东回报机制。这种资本配置策略既能保障业务扩张，又能维持约11%的股东回报复合增长率。\\n\\n3. 同业比较下的投资亮点：\\n相较于United Fire等区域性同业，RGA展现出三大核心优势：一是全球化业务布局带来更显著的风险分散效应，2022年各区域保费同步增长即为明证；二是特殊险种的专业化经营形成护城河，长寿再保险等业务具有较高的技术门槛；三是资本管理效率突出，10.4%的股息CAGR配合股票回购，总股东回报率显著优于行业平均。当前14.6%的股价超额收益已部分反映这些优势，但考虑到2023年预期仍在持续上调，估值溢价有望进一步扩大。\",\n",
    "    \"model_responses\": {\n",
    "      \"original\": {\n",
    "        \"success\": true,\n",
    "        \"model_name\": \"original\",\n",
    "        \"response\": \"好的，我现在要分析Reinsurance Group of America (RGA)的未来两年业绩增长主要驱动因素，以及其强劲的资本状况如何支持业务发展战略，同时还要比较其与同行业其他公司的投资价值。\\n\\n首先，业绩增长因素。根据用户提供的信息，RGA在2022年的每股收益为9.76美元，2023年预期为15.25美元，同比增长了763.7%。这说明公司在过去一年的业绩非常强劲。此外，Zacks共识预期在2022年上调了10.6%，2023年上调了3.2%，进一步显示了市场对公司的认可。业务增长主要来自多个地区的保费增长，尤其是北美、拉丁美洲、欧洲中东非洲和亚太地区。其中，EMEA受益于新业务增加和年金投资，亚太得益于理赔和财务解决方案的增长，加拿大则因现有条约和投资收入提升。这些地区的增长点都是重要的驱动因素。\\n\\n接下来，资本状况方面，RGA拥有约10亿美元的过剩资本，还有8.5亿美元的银团循环信贷额度和9.29亿美元的承诺信用证额度。这表明公司在财务上非常稳健，有足够的资本来支持业务扩展和投资。强大的资本实力不仅能够应对市场波动，还能支持公司继续扩大业务，增加新的保险产品，或者进行并购和投资，从而进一步提升业绩和市场地位。\\n\\n然后，比较投资价值。与同行业公司如United Fire Group、RLI Corp.和HCI Group相比，RGA在多个方面更具吸引力。首先，股息增长速度快，10.4%的复合年增长率，2021年通过股息和股票回购向股东返还了7400万美元，这显示公司 returning cash to shareholders 的能力很强，股东回报率高。其次，RGA的业务增长速度快，尤其是在2023年预期同比增长56.3%，这在同行业中表现突出。此外，RGA的资本实力强，过剩资本充足，这在同行业内是比较少见的，能够支持公司持续增长。最后，RGA的业务范围广，覆盖多个地区，风险分散，这在同行业中具有竞争力。\\n\\n总结一下，RGA的未来两年业绩增长主要依赖于多个地区的保费增长，强大的资本实力为业务发展提供了坚实基础，股息增长和股东回报能力也是其投资价值的重要体现。与同行业相比，RGA在业务扩展、资本实力和股东回报方面更具优势，因此其投资价值更高。\\n</think>\\n\\n### 分析RGA的未来两年业绩增长主要驱动因素、资本状况支持业务战略以及与同行业其他公司的投资价值\\n\\n#### 1. RGA未来两年业绩增长的主要驱动因素\\n\\nRGA的未来两年业绩增长将主要受到以下因素的推动：\\n\\n- **多地区业务增长**：RGA的业务增长主要来自北美、拉丁美洲、欧洲中东非洲（EMEA）和亚太地区的保费增长。这些地区的增长点包括：\\n  - **北美**：由于现有条约下业务量增加和可变投资收入提高，北美业务持续获益。\\n  - **拉丁美洲**：受益于有利的理赔经验、财务解决方案再保险持续增长和相关收益。\\n  - **EMEA**：得益于新业务活动增加、年金业务投资增加以及封闭长寿业务新业务量增长。\\n  - **亚太地区**：由于有利的理赔经验、财务解决方案再保险持续增长、更高的投资收入和相关收益。\\n- **强大的业务扩展能力**：RGA拥有超过10亿美元的过剩资本，以及8.5亿美元的银团循环信贷额度和9.29亿美元的承诺信用证额度，这些资本实力将支持其业务扩展和投资。\\n- **持续的股息增长**：RGA的股息以10.4%的复合年增长率增长，显示出公司对股东回报的承诺和能力。\\n\\n#### 2. 强劲的资本状况如何支持业务发展战略\\n\\nRGA的强劲资本状况为其业务发展战略提供了坚实的支持：\\n\\n- **充足过剩资本**：公司拥有约10亿美元的过剩资本，这为其业务扩展提供了充足的资金支持，能够用于投资新的保险产品、市场扩展和技术创新。\\n- **灵活的融资能力**：RGA拥有8.5亿美元的银团循环信贷额度和9.29亿美元的承诺信用证额度，这些融资工具使其能够在需要时迅速调整资本结构，支持业务增长。\\n- **稳定的现金流**：强大的资本实力和业务增长能力确保了公司有足够的现金流，用于投资和应对潜在的财务风险，从而支持其长期增长目标。\\n\\n#### 3. RGA的投资价值与同行业公司的比较\\n\\n与同行业其他公司（如United Fire Group、RLI Corp.和HCI Group）相比，RGA的投资价值体现在以下几个方面：\\n\\n- **股息增长能力**：RGA的股息增长速度快，10.4%的复合年增长率，显示出公司对股东回报的承诺和能力，这在同行业中具有竞争力。\\n- **业务增长速度**：RGA的业务增长速度在同行业中表现突出，2023年的预期同比增长56.3%，远超行业平均水平，显示其业务扩展能力的强健。\\n- **资本实力**：RGA拥有充足的过剩资本和灵活的融资能力，这在同行业中较为少见，能够支持其业务增长和投资。\\n- **业务范围广**：RGA的业务覆盖多个地区，包括北美、拉丁美洲、EMEA和亚太地区，风险分散，市场竞争力强。\\n\\n### 总结\\n\\nRGA未来两年的业绩增长将主要受益于多地区业务的持续增长、强大的资本实力以及持续的股息增长。其强劲的资本状况不仅支持了其业务发展战略，还为其提供了稳定的现金流和融资能力。与同行业其他公司相比，RGA在股息增长、业务增长速度和资本实力方面具有明显优势，因此其投资价值更高。\",\n",
    "        \"api_type\": \"chat_completions\",\n",
    "        \"error\": null\n",
    "      },\n",
    "      \"finetuned\": {\n",
    "        \"success\": true,\n",
    "        \"model_name\": \"finetuned\",\n",
    "        \"response\": \"1. 问题背景分析：\\nRGA作为专业再保险公司，其业务增长主要来源于全球多个地区（美国、拉丁美洲、加拿大、欧洲EMEA和亚太）的保费增长。关键数据点包括：\\n- 2022年和2023年EPS预期：9.76美元（同比+763.7%）和15.25美元（同比+56.3%）\\n- 30天内预期上调：2022年+10.6%，2023年+3.2%\\n- Zacks评级：3（持有）\\n- 股价表现：YTD +14.6% vs 行业-8.7%\\n- 资本状况：10亿美元过剩资本，8.5亿银团信贷额度，9.29亿承诺信用证额度\\n- 股息增长：2015-2022复合增长率10.4%，2021Q1股息+股票回购7400万美元\\n\\n2. 数据解读：\\n- 763.7%的EPS同比增长率异常高，表明可能存在一次性因素或业务结构的重大变化\\n- 56.3%的2023年增长预期仍强劲，显示业务复苏势头良好\\n- 预期上调幅度（10.6%和3.2%）显示分析师持续看好\\n- 股价表现显著跑赢行业，反映市场对其前景的认可\\n- 资本状况保守（约19.29亿美元总可用额度），显示抗风险能力强\\n\\n3. 业绩增长驱动因素分析：\\n- EMEA业务：年金业务投资增加、封闭长寿业务增长\\n- 亚太业务：有利理赔经验、再保险增长、投资收入\\n- 加拿大业务：现有条约下业务量、可变投资收入\\n- 全球整体保费增长推动\\n\\n4. 资本状况分析：\\n- 10亿美元过剩资本提供充足缓冲\\n- 8.5亿银团信贷和9.29亿承诺信用证构成强大融资能力\\n- 可支持业务扩张、股息支付和股东回报\\n\\n5. 同业比较维度：\\n- 股息增长记录（10.4% CAGR）\\n- 资本充足性（19.29亿可用额度）\\n- 业务区域多元化（全球覆盖）\\n- 投资收入能力（高增长点）\\n- 股价表现（显著优于同业）\\n\\n6. 风险考量：\\n- 763.7%的EPS增长可能包含一次性项目\\n- 股价表现已部分反映利好\\n- 需关注可持续性增长\\n</think>\\n\\n1) RGA未来两年业绩增长的主要驱动因素：\\n- 区域性业务增长：北美地区（美国、加拿大）业务量增加，欧洲EMEA和亚太市场的再保险和投资业务持续扩张\\n- 业务结构优化：年金业务投资增加、长寿保险业务增长带来稳定收益\\n- 投资收入提升：高利率环境下的可变投资收益增长\\n- 资本效率提升：现有合约条款优化和资产配置改善\\n\\n2) 资本状况对业务战略的支持：\\n- 资本缓冲：10亿美元过剩资本和19.29亿美元融资能力提供充足资金支持\\n- 资本配置灵活性：可灵活调整业务布局和投资组合\\n- 财务稳健性：确保业务扩张和股东回报的可持续性\\n- 风险管理能力：强大的信贷额度支持应对潜在风险\\n\\n3) 投资价值体现：\\n- 股息增长记录：10.4%的复合年增长率显示稳定的股东回报能力\\n- 资本充足性：19.29亿美元的融资能力提供充足财务灵活性\\n- 业务多元化：全球覆盖降低区域风险\\n- 投资收入潜力：高增长的年金和长寿保险业务\\n- 股价表现：持续跑赢行业的表现显示市场认可\\n- 股息支付能力：通过股票回购和股息返还提升股东价值\\n\\nRGA展现出稳健的业务增长动力和资本实力，其投资价值主要体现在持续的高股息增长、多元化的业务结构和良好的财务灵活性上。当前股价表现可能已部分反映这些优势，但强劲的资本状况为未来业务扩张提供了坚实基础。建议关注其持续的股息增长能力和资本使用效率。\",\n",
    "        \"api_type\": \"chat_completions\",\n",
    "        \"error\": null\n",
    "      } \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c19bcb",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **参考答案（最高可靠性）**\n",
    "- **优势**：\n",
    "  - **结构严谨**：采用“三极增长引擎”“双轮驱动”等专业分析框架，逻辑严密。\n",
    "  - **数据深度**：精确计算资本与EPS的比例（27.79亿美元可动用资本对应28.5倍EPS），体现财务分析专业性。\n",
    "  - **风险全面性**：明确列出长寿业务风险、投资波动风险及估值压力，展现风险收益平衡视角。\n",
    "  - **同业对比维度**：从全球化布局、特殊险种技术壁垒、资本效率三方面对比，突出差异化优势。\n",
    "\n",
    "### 2. **微调答案（次优）**\n",
    "- **优势**：\n",
    "  - **分点清晰**：业务增长、资本支持、同业比较三部分结构明确，数据引用具体（如19.29亿美元可用资本）。\n",
    "  - **风险提示**：指出高增长可能含一次性因素，并提示股价已部分反映利好。\n",
    "  - **实操建议**：建议关注股息增长和资本效率，贴近投资者视角。\n",
    "- **不足**：\n",
    "  - **分析框架较弱**：缺乏参考答案中的系统性模型（如“双轮驱动”）。\n",
    "  - **部分数据遗漏**：未提及资本与EPS的倍数关系，削弱财务联动分析。\n",
    "\n",
    "### 3. **原始答案（可靠性最低）**\n",
    "- **优势**：\n",
    "  - **覆盖全面**：涵盖问题所有要点（增长因素、资本、同业比较）。\n",
    "  - **基础分析**：对区域业务增长和资本状况有基本解释。\n",
    "- **不足**：\n",
    "  - **分析浅显**：仅列举数据，未深入业务结构（如未提封闭长寿业务的技术壁垒）。\n",
    "  - **逻辑松散**：未形成分析框架，各部分关联性弱。\n",
    "  - **风险忽视**：未讨论潜在风险，结论片面。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2559d96",
   "metadata": {},
   "source": [
    "# 8.总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ae9b5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/muyan/image-20250528163241279.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028484b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
